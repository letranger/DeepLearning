<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2021-06-01 Tue 22:17 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Deep Learning with Python</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Yung Chin, Yen" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<style type="text/css"> * { font-family: "Georgia";}h1, h2, h3, h4, h5, h6,h1 span, h2 span, h3 span, h4 span, h5 span, h6 span,h1 a, h2 a, h3 a, h4 a, h5 a, h6 a {    font-family: "Arial";}pre, code {    font-family: Monaco, "Courier New", Courier;}#text-table-of-contents a {    font-family: "Arial";}/* @end *//* @group Baseline */body {	font-size: 14px;	line-height: 1.5em;	padding: 0;	margin: 0;}h1 {	margin: 0;	font-size: 1.6666666666666667em;	line-height: 0.9em;	margin-bottom: 0.9em;}h2 {	margin: 0;	font-size: 1.5em;	line-height: 1em;	margin-bottom: 1em;}h3 {	margin: 0;	font-size: 1.3333333333333333em;	line-height: 1.125em;	margin-bottom: 1.125em;}h4 {	margin: 0;	font-size: 1.1666666666666667em;	line-height: 1.2857142857142858em;	margin-bottom: 1.2857142857142858em;}p, ul, blockquote, pre, td, th, label {	margin: 0;	font-size: 1em;	line-height: 1.5em;	margin-bottom: 1.5em;}p.small, #postamble {	margin: 0;	font-size: 0.8333333333333334em;	line-height: 1.8em;	margin-bottom: 1.8em;}table {	border-collapse: collapse;	margin-bottom: 1.5em;}/* @end *//* @group Layout */#content {	width: 70em;	margin-left: auto;	margin-right: auto;}/* #header {	height: 10em;}*/#table-of-contents {	width: 15em;	float: left;	overflow: auto;}/* #main { */div.outline-2 {        width: 52em;	float: right;	/* The lines below are useful if the "main" div isn't available and	   div.outline-2 has to be used. */	position: relative;}#postamble {	clear: both;	text-align: center;}div.outline-2 pre {    width: 40em;    overflow: auto;}/* @end *//* @group Header */h1.title {	margin-top: 10px;	text-align: center;}h1.title {	font-size: 3em;	font-weight: bold;	margin-bottom: 0.8em;}/* @end *//* @group Org Keywords */.todo {	color: red;}.done {	color: green;}.tag {	color: blue;	text-transform: lowercase;	/* This will be obscured by the surrounding span tag, so blank everything. */	background: #fff;	border: none;	/* position: relative;	text-align: right;	right: 1em; */}.timestamp {}.timestamp-kwd  {	/* keyword associated with a time stamp, like SCHEDULED */}.target {	/* target for links */}/* @end *//* @group Table of Contents */#table-of-contents h2 {	letter-spacing: -0.1em;}#table-of-contents ul,#table-of-contents ol {    padding-left: 1em;}/* @end *//* @group Outline Level 2 */.outline-2 h2 {    background: #ffc;    border-bottom: 1px solid #ccc;}.outline-2 h2, .outline-2 h3 {    letter-spacing: -0.05em;}.outline-2 {	padding: 5px;	/* margin-bottom: 10px; */	/* border-top: 1px solid #ccc; */}/* @end */td {	border: 1px solid #ccc;}h1 span, h2 span, h3 span, h4 span, h5 span, h6 span {    background-color: #eee;    padding: 2px;    border: 1px solid #ccc;}.outline-1, .outline-2, .outline-3, .outline-4, .outline-5, .outline-6 {    margin-left: 2em;}a {    text-decoration: none;    color: #57d; /* TODO: Find a better colour for this. */}a:hover {    border-bottom: 1px dotted #57d;}#postamble p {    margin: 0px;}.footpara { display: inline; }.footdef { margin-bottom: 3em; font-size: 80%; }/*]]>*/--></style><script type="text/javascript">/*@licstart  The following is the entire license notice for theJavaScript code in this tag.Copyright (C) 2012-2013 Free Software Foundation, Inc.The JavaScript code in this tag is free software: you canredistribute it and/or modify it under the terms of the GNUGeneral Public License (GNU GPL) as published by the Free SoftwareFoundation, either version 3 of the License, or (at your option)any later version.  The code is distributed WITHOUT ANY WARRANTY;without even the implied warranty of MERCHANTABILITY or FITNESSFOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.As additional permission under GNU GPL version 3 section 7, youmay distribute non-source (e.g., minimized or compacted) forms ofthat code without the copy of the GNU GPL normally required bysection 4, provided you include this license notice and a URLthrough which recipients can access the Corresponding Source.@licend  The above is the entire license noticefor the JavaScript code in this tag.*/<!--/*--><![CDATA[/*><!--*/ function CodeHighlightOn(elem, id) {   var target = document.getElementById(id);   if(null != target) {     elem.cacheClassElem = elem.className;     elem.cacheClassTarget = target.className;     target.className = "code-highlighted";     elem.className   = "code-highlighted";   } } function CodeHighlightOff(elem, id) {   var target = document.getElementById(id);   if(elem.cacheClassElem)     elem.className = elem.cacheClassElem;   if(elem.cacheClassTarget)     target.className = elem.cacheClassTarget; }</script>
<script type="text/javascript">
// @license magnet:?xt=urn:btih:1f739d935676111cfff4b4693e3816e664797050&amp;dn=gpl-3.0.txt GPL-v3-or-Later
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.cacheClassElem = elem.className;
         elem.cacheClassTarget = target.className;
         target.className = "code-highlighted";
         elem.className   = "code-highlighted";
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(elem.cacheClassElem)
         elem.className = elem.cacheClassElem;
       if(elem.cacheClassTarget)
         target.className = elem.cacheClassTarget;
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Deep Learning with Python</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org3ae050a">1. 監督式學習範例：以感知器解決分類問題</a></li>
<li><a href="#org27ae31b">2. 名詞解釋</a>
<ul>
<li><a href="#org0462128">2.1. Supervised neural networks</a></li>
<li><a href="#orge987c24">2.2. Unsupervised Pre-trained Neural Networks</a></li>
</ul>
</li>
<li><a href="#org5646fb4">3. 類神經網路</a>
<ul>
<li><a href="#org6ba26ff">3.1. Perceptron</a></li>
<li><a href="#org5cb9edc">3.2. 為什麼沒有Activation Function的perceptron為linear model</a></li>
<li><a href="#orgc187313">3.3. Activation Function</a></li>
</ul>
</li>
<li><a href="#org70c9525">4. Story of gate: From perceptron to MLP</a>
<ul>
<li><a href="#org7943bb9">4.1. Example #1</a></li>
<li><a href="#org1b2eb02">4.2. Example #2</a></li>
<li><a href="#org4565e4a">4.3. XOR Problem</a></li>
<li><a href="#orgddb6f82">4.4. MLP (Multilayer perceptron)</a></li>
</ul>
</li>
<li><a href="#org97e3a6c">5. Say Hello to DNN: MNIST</a>
<ul>
<li><a href="#org7565392">5.1. MNIST 資料集</a></li>
<li><a href="#org792d800">5.2. MNIST 資料集:以 DNN Sequential 模型為例</a></li>
</ul>
</li>
<li><a href="#orga320739">6. 深度神經網路 DNN (Deep Neural Network)</a>
<ul>
<li><a href="#org7459220">6.1. 學習與參數:以迴歸問題為例</a></li>
<li><a href="#org00bc2c6">6.2. 如何調整參數</a></li>
<li><a href="#org30a96aa">6.3. 模型的極限</a></li>
<li><a href="#orga697eaf">6.4. 神經網路為什麼要有那麼多層</a></li>
</ul>
</li>
<li><a href="#orge09fea9">7. Propagation</a>
<ul>
<li><a href="#orgf490b60">7.1. 三層神經網路</a></li>
<li><a href="#org0bbfa76">7.2. Forward propagation</a></li>
<li><a href="#org5951c86">7.3. Backward propagation</a></li>
<li><a href="#org0928a3a">7.4. 輸出層的設計：恆等函數與 softmax 函數</a></li>
<li><a href="#org5c47d76">7.5. softmax 函數的特色</a></li>
<li><a href="#orgc088ae1">7.6. 梯度下降法</a></li>
</ul>
</li>
<li><a href="#org29ef03b">8. 神經網路的學習</a>
<ul>
<li><a href="#org39fb59d">8.1. 從資料中學習</a></li>
<li><a href="#org87e5607">8.2. Overfitting v.s. underfitting</a></li>
<li><a href="#org9d17651">8.3. 損失函數</a></li>
<li><a href="#org8dee103">8.4. 數值微分</a></li>
<li><a href="#org85fe303">8.5. 梯度</a></li>
<li><a href="#orgd64fa0d">8.6. 學習演算法</a></li>
</ul>
</li>
<li><a href="#org1c2f8e3">9. 建構良好的訓練集：數據預處理</a>
<ul>
<li><a href="#orgbebc2ba">9.1. 處理數據遺漏</a></li>
<li><a href="#org1eb9197">9.2. 填補遺遺漏值</a></li>
<li><a href="#org4de56f0">9.3. 處理數據中的分類特徵編碼問題</a></li>
<li><a href="#orgdd4ba17">9.4. 訓練集與測試集的數據分割</a></li>
<li><a href="#orga1dde0e">9.5. 縮放特徵值、維持特徵值影響比例：正規化(normalization)</a></li>
<li><a href="#org9c44297">9.6. 選取有意義的特徵</a></li>
<li><a href="#orgf67362c">9.7. 循序特徵選擇法</a></li>
<li><a href="#orgeec69d3">9.8. 以隨機森林評估特徵的重要性</a></li>
</ul>
</li>
<li><a href="#org19d002e">10. 降維來壓縮數據</a>
<ul>
<li><a href="#org5afc65e">10.1. 以主成份分析(PCA)對非監督式數據壓縮</a></li>
<li><a href="#org9dee650">10.2. 利用線性判別分析(LDA)做監督式數據壓縮</a></li>
<li><a href="#org35a9c43">10.3. 利用核主成份分析(KPCA)處理非線性對應</a></li>
<li><a href="#org69ecb2c">10.4. 相關資源</a></li>
</ul>
</li>
<li><a href="#orgcd65d61">11. 以少量資料集實做 CNN</a>
<ul>
<li><a href="#org63e3959">11.1. 深度學習與少量資料的相關性</a></li>
<li><a href="#orgf8208a5">11.2. 實作</a></li>
<li><a href="#org5bf6f16">11.3. 改善#1: 使用資料擴增法(data augmentation)</a></li>
<li><a href="#orgac8c095">11.4. 改善 2: 使用 pretrained network</a></li>
</ul>
</li>
<li><a href="#org097008e">12. 視覺化呈現 CNN 的學習內容</a>
<ul>
<li><a href="#orgf0bd8ff">12.1. 中間層輸出視覺化</a></li>
<li><a href="#org797b1ff">12.2. 視覺化 convnet 的 filter</a></li>
<li><a href="#org5c5f0b5">12.3. 視覺化類別激活熱圖 heatmap of class activation</a></li>
</ul>
</li>
<li><a href="#org615672f">13. MLP 神經網路模型實作：以 Keras 為實作工具</a>
<ul>
<li><a href="#orge12ed1e">13.1. 簡介</a></li>
<li><a href="#orga9b8ded">13.2. Keras 程式設計模式</a></li>
<li><a href="#org692aeea">13.3. 基本流程</a></li>
<li><a href="#org5b87db5">13.4. 以 Keras 實作 MNist 手寫數字辨識資料集</a></li>
<li><a href="#org33a7c3a">13.5. 強化 MLP 辨識 solution #1: 增加隠藏層神經元數</a></li>
<li><a href="#orgff64d1e">13.6. 強化 MLP 辨識 solution #2: 加入 DropOut 以避免 overfitting</a></li>
<li><a href="#org969585d">13.7. 強化 MLP 辨識 solution #3: 增加隱藏層層數</a></li>
</ul>
</li>
<li><a href="#org49420cd">14. 以 Keras 解決分類問題</a>
<ul>
<li><a href="#orgb469cad">14.1. 二元分類：IMDB</a></li>
<li><a href="#orgf858a4a">14.2. 多類別分類：數位新聞</a></li>
</ul>
</li>
<li><a href="#orgd5bf4ba">15. 以 Keras 解決迴歸問題：預測房價: Boston</a>
<ul>
<li><a href="#org8a480a4">15.1. 準備資料</a></li>
<li><a href="#orgac6afe4">15.2. 建立神經網路</a></li>
<li><a href="#orge7458a4">15.3. 驗證</a></li>
<li><a href="#orgf03a632">15.4. 小結</a></li>
</ul>
</li>
<li><a href="#orge9b4325">16. CNN 卷積神經網路: 以 Keras 為實作工具</a>
<ul>
<li><a href="#orgb43fd6e">16.1. CNN v.s. MLP</a></li>
<li><a href="#orgf9c2d87">16.2. CNN 模型架構</a></li>
<li><a href="#orga05e7cf">16.3. 卷積運算</a></li>
<li><a href="#orgf13e09f">16.4. 池化層(Pooling Layer)</a></li>
<li><a href="#org97149cb">16.5. 以 CNN 實作 MNIST</a></li>
<li><a href="#org38e8209">16.6. 以 Keras 實作 Cifar-10</a></li>
</ul>
</li>
<li><a href="#orgbb0aef1">17. Keras GAN</a></li>
<li><a href="#org23cb4e7">18. 深度學習</a>
<ul>
<li><a href="#org40ab2dc">18.1. 深度學習運作原理</a></li>
<li><a href="#org3fe0ce7">18.2. 深度學習應用領域</a></li>
<li><a href="#orgdc187f5">18.3. 深度學習的幾種類型</a></li>
<li><a href="#org0490260">18.4. 深度學習的高速化</a></li>
<li><a href="#org66535fb">18.5. 深度學習的應用範例</a></li>
<li><a href="#org9c28e00">18.6. 梯度遞減問題</a></li>
<li><a href="#orgde553af">18.7. 最佳選擇</a></li>
</ul>
</li>
<li><a href="#org910315f">19. TensorFlow</a>
<ul>
<li><a href="#org4742bff">19.1. 安裝</a></li>
<li><a href="#orge9f393c">19.2. Tenser</a></li>
<li><a href="#org6f13505">19.3. TenserBoard</a></li>
<li><a href="#org4f8769c">19.4. 情緒分析 sentiment analysis</a></li>
<li><a href="#orgdc2cf21">19.5. IMDb-Movie-Review</a></li>
<li><a href="#orgd509e85">19.6. 使用 XLNet(2019)</a></li>
</ul>
</li>
<li><a href="#org23241ef">20. PyTorch</a>
<ul>
<li><a href="#orga095b40">20.1. 簡介</a></li>
<li><a href="#org30b79b3">20.2. PyTorch 基本架構</a></li>
<li><a href="#org51fac67">20.3. PyTorch 基礎運算</a></li>
<li><a href="#org505d180">20.4. PyTorch 簡單案例：線性迴歸</a></li>
<li><a href="#orged720b2">20.5. PyTorch 基本運算</a></li>
<li><a href="#org21a19be">20.6. PyTorch 自動求導(autograd)機制</a></li>
</ul>
</li>
<li><a href="#org2776df8">21. Scikit-learn: 分類問題</a>
<ul>
<li><a href="#org4815b0c">21.1. 簡介</a></li>
<li><a href="#org091ee63">21.2. 常用模組</a></li>
<li><a href="#org63829bb">21.3. 分類：識別某個對象屬於哪個類別</a></li>
<li><a href="#orgf41e2af">21.4. 使用 scikit-learn: 感知器演算法</a></li>
<li><a href="#org67abd7f">21.5. 使用 scikit-learn: 邏輯斯迴歸</a></li>
<li><a href="#orga5d3ee0">21.6. 使用 SVM</a></li>
<li><a href="#org9e07415">21.7. 使用 SVM 解決非線性問題</a></li>
<li><a href="#org6a72c2b">21.8. 使用決策樹</a></li>
<li><a href="#org7200218">21.9. 使用隨機森林結合決策樹</a></li>
<li><a href="#org44ee840">21.10. KNN</a></li>
<li><a href="#org5ad8146">21.11. 有母數模型與無母數模型</a></li>
</ul>
</li>
<li><a href="#org2cf41e9">22. NumPy</a>
<ul>
<li><a href="#orge4d1012">22.1. 簡介</a></li>
<li><a href="#org5180a71">22.2. NumPy 陣列</a></li>
<li><a href="#org7770535">22.3. Numpy 檔案輸出輸入</a></li>
<li><a href="#orgc7e07b5">22.4. Data Visualization</a></li>
</ul>
</li>
<li><a href="#orgcfb423d">23. Pandas</a>
<ul>
<li><a href="#orgeaee77a">23.1. Pandas 簡介</a></li>
<li><a href="#orgddc2121">23.2. Pandas 提供的資料結構</a></li>
<li><a href="#org7c26c8b">23.3. Pandas 實作</a></li>
</ul>
</li>
<li><a href="#org94df761">24. AI v.s. security</a>
<ul>
<li><a href="#org402e0c1">24.1. 釣魚網站偵測實戰</a></li>
<li><a href="#orge122155">24.2. Text Classification</a></li>
<li><a href="#orge1012d0">24.3. AI and Botnet Detection</a></li>
</ul>
</li>
<li><a href="#orgb2b2490">25. Emacs 常用快速鍵</a></li>
</ul>
</div>
</div>

<div id="outline-container-org3ae050a" class="outline-2">
<h2 id="org3ae050a"><span class="section-number-2">1</span> 監督式學習範例：以感知器解決分類問題</h2>
<div class="outline-text-2" id="text-1">
<p>
從「鳶尾花資料集」取出兩類花(Setosa, Versicolor)，藉由不同的兩個屬性（花萼長、花瓣長）對其行分類，如圖<a href="#org8640604">1</a>。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">coding: utf-8</span>
<span class="linenr">  2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  3: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">  4: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">  5: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr">  6: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">&#24863;&#30693;&#22120;&#27169;&#22411;</span>
<span class="linenr">  7: </span>  <span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">Perceptron</span>(<span style="color: #c678dd;">object</span>):
<span class="linenr">  8: </span>      <span style="color: #83898d;">"""Perceptron classifier.</span>
<span class="linenr">  9: </span><span style="color: #83898d;">      &#21443;&#25976;&#65306;</span>
<span class="linenr"> 10: </span><span style="color: #83898d;">      ------------</span>
<span class="linenr"> 11: </span><span style="color: #83898d;">      eta : float: &#23416;&#32722;&#29575; (0.0 ~ 1.0)</span>
<span class="linenr"> 12: </span><span style="color: #83898d;">      n_iter : int: &#35347;&#32244;&#27425;&#25976;</span>
<span class="linenr"> 13: </span><span style="color: #83898d;">        Passes over the training dataset.</span>
<span class="linenr"> 14: </span><span style="color: #83898d;">      random_state : int</span>
<span class="linenr"> 15: </span><span style="color: #83898d;">      &#23660;&#24615;</span>
<span class="linenr"> 16: </span><span style="color: #83898d;">      -----------</span>
<span class="linenr"> 17: </span><span style="color: #83898d;">      w_ : 1d-array: &#35347;&#32244;&#24460;&#30340;&#27402;&#37325;</span>
<span class="linenr"> 18: </span><span style="color: #83898d;">        Weights after fitting.</span>
<span class="linenr"> 19: </span><span style="color: #83898d;">      errors_ : list: &#27599;&#27425;&#35347;&#32244;&#30340;&#37679;&#35492;&#27425;&#25976;</span>
<span class="linenr"> 20: </span>
<span class="linenr"> 21: </span><span style="color: #83898d;">      """</span>
<span class="linenr"> 22: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>, eta=<span style="color: #da8548; font-weight: bold;">0.01</span>, n_iter=<span style="color: #da8548; font-weight: bold;">50</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>):
<span class="linenr"> 23: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">eta</span> = eta
<span class="linenr"> 24: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">n_iter</span> = n_iter
<span class="linenr"> 25: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">random_state</span> = random_state
<span class="linenr"> 26: </span>
<span class="linenr"> 27: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">fit</span>(<span style="color: #51afef;">self</span>, X, y):
<span class="linenr"> 28: </span>          <span style="color: #83898d;">"""Fit training data.</span>
<span class="linenr"> 29: </span><span style="color: #83898d;">          Parameters</span>
<span class="linenr"> 30: </span><span style="color: #83898d;">          ----------</span>
<span class="linenr"> 31: </span><span style="color: #83898d;">          X : {array-like}, shape = [n_samples, n_features]</span>
<span class="linenr"> 32: </span><span style="color: #83898d;">            Training vectors, where n_samples is the number of samples and</span>
<span class="linenr"> 33: </span><span style="color: #83898d;">            n_features is the number of features.</span>
<span class="linenr"> 34: </span><span style="color: #83898d;">          y : array-like, shape = [n_samples]</span>
<span class="linenr"> 35: </span><span style="color: #83898d;">            Target values.</span>
<span class="linenr"> 36: </span>
<span class="linenr"> 37: </span><span style="color: #83898d;">          Returns</span>
<span class="linenr"> 38: </span><span style="color: #83898d;">          -------</span>
<span class="linenr"> 39: </span><span style="color: #83898d;">          self : object</span>
<span class="linenr"> 40: </span>
<span class="linenr"> 41: </span><span style="color: #83898d;">          """</span>
<span class="linenr"> 42: </span>          rgen = np.random.RandomState(<span style="color: #51afef;">self</span>.random_state)
<span class="linenr"> 43: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">w_</span> = rgen.normal(loc=<span style="color: #da8548; font-weight: bold;">0.0</span>, scale=<span style="color: #da8548; font-weight: bold;">0.01</span>, size=<span style="color: #da8548; font-weight: bold;">1</span> + X.shape[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr"> 44: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">errors_</span> = []
<span class="linenr"> 45: </span>
<span class="linenr"> 46: </span>          <span style="color: #51afef;">for</span> _ <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #51afef;">self</span>.n_iter):
<span class="linenr"> 47: </span>              <span style="color: #dcaeea;">errors</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr"> 48: </span>              <span style="color: #51afef;">for</span> xi, target <span style="color: #51afef;">in</span> <span style="color: #c678dd;">zip</span>(X, y):
<span class="linenr"> 49: </span>                  <span style="color: #dcaeea;">update</span> = <span style="color: #51afef;">self</span>.eta * (target - <span style="color: #51afef;">self</span>.predict(xi))
<span class="linenr"> 50: </span>                  <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">w_</span>[<span style="color: #da8548; font-weight: bold;">1</span>:] += update * xi
<span class="linenr"> 51: </span>                  <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">w_</span>[<span style="color: #da8548; font-weight: bold;">0</span>] += update
<span class="linenr"> 52: </span>                  <span style="color: #dcaeea;">errors</span> += <span style="color: #c678dd;">int</span>(update != <span style="color: #da8548; font-weight: bold;">0.0</span>)
<span class="linenr"> 53: </span>              <span style="color: #51afef;">self</span>.errors_.append(errors)
<span class="linenr"> 54: </span>          <span style="color: #51afef;">return</span> <span style="color: #51afef;">self</span>
<span class="linenr"> 55: </span>
<span class="linenr"> 56: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">net_input</span>(<span style="color: #51afef;">self</span>, X):
<span class="linenr"> 57: </span>          <span style="color: #83898d;">"""Calculate net input"""</span>
<span class="linenr"> 58: </span>          <span style="color: #51afef;">return</span> np.dot(X, <span style="color: #51afef;">self</span>.w_[<span style="color: #da8548; font-weight: bold;">1</span>:]) + <span style="color: #51afef;">self</span>.w_[<span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr"> 59: </span>
<span class="linenr"> 60: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">predict</span>(<span style="color: #51afef;">self</span>, X):
<span class="linenr"> 61: </span>          <span style="color: #83898d;">"""Return class label after unit step"""</span>
<span class="linenr"> 62: </span>          <span style="color: #51afef;">return</span> np.where(<span style="color: #51afef;">self</span>.net_input(X) &gt;= <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1</span>, -<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 63: </span>
<span class="linenr"> 64: </span>  <span style="color: #dcaeea;">v1</span> = np.array([<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>])
<span class="linenr"> 65: </span>  <span style="color: #dcaeea;">v2</span> = <span style="color: #da8548; font-weight: bold;">0.5</span> * v1
<span class="linenr"> 66: </span>  np.arccos(v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))
<span class="linenr"> 67: </span>
<span class="linenr"> 68: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#35347;&#32244;&#38598;&#36039;&#26009;</span>
<span class="linenr"> 69: </span>  <span style="color: #dcaeea;">df</span> = pd.read_csv(<span style="color: #98be65;">'https://archive.ics.uci.edu/ml/'</span>
<span class="linenr"> 70: </span>          <span style="color: #98be65;">'machine-learning-databases/iris/iris.data'</span>, header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr"> 71: </span>  <span style="color: #51afef;">print</span>(df.tail())
<span class="linenr"> 72: </span>
<span class="linenr"> 73: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">select setosa and versicolor</span>
<span class="linenr"> 74: </span>  <span style="color: #dcaeea;">y</span> = df.iloc[<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">100</span>, <span style="color: #da8548; font-weight: bold;">4</span>].values
<span class="linenr"> 75: </span>  <span style="color: #dcaeea;">y</span> = np.where(y == <span style="color: #98be65;">'Iris-setosa'</span>, -<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 76: </span>
<span class="linenr"> 77: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">extract sepal length and petal length</span>
<span class="linenr"> 78: </span>  <span style="color: #dcaeea;">X</span> = df.iloc[<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">100</span>, [<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">2</span>]].values
<span class="linenr"> 79: </span>
<span class="linenr"> 80: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot data</span>
<span class="linenr"> 81: </span>  plt.clf()
<span class="linenr"> 82: </span>  plt.scatter(X[:<span style="color: #da8548; font-weight: bold;">50</span>, <span style="color: #da8548; font-weight: bold;">0</span>], X[:<span style="color: #da8548; font-weight: bold;">50</span>, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr"> 83: </span>              color=<span style="color: #98be65;">'red'</span>, marker=<span style="color: #98be65;">'o'</span>, label=<span style="color: #98be65;">'setosa'</span>)
<span class="linenr"> 84: </span>  plt.scatter(X[<span style="color: #da8548; font-weight: bold;">50</span>:<span style="color: #da8548; font-weight: bold;">100</span>, <span style="color: #da8548; font-weight: bold;">0</span>], X[<span style="color: #da8548; font-weight: bold;">50</span>:<span style="color: #da8548; font-weight: bold;">100</span>, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr"> 85: </span>              color=<span style="color: #98be65;">'blue'</span>, marker=<span style="color: #98be65;">'x'</span>, label=<span style="color: #98be65;">'versicolor'</span>)
<span class="linenr"> 86: </span>
<span class="linenr"> 87: </span>  plt.xlabel(<span style="color: #98be65;">'sepal length [cm]'</span>)
<span class="linenr"> 88: </span>  plt.ylabel(<span style="color: #98be65;">'petal length [cm]'</span>)
<span class="linenr"> 89: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr"> 90: </span>
<span class="linenr"> 91: </span>  plt.savefig(<span style="color: #98be65;">'02_06.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr"> 92: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr"> 93: </span>
<span class="linenr"> 94: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;&#24863;&#30693;&#22120;&#27169;&#22411;</span>
<span class="linenr"> 95: </span>  <span style="color: #dcaeea;">ppn</span> = Perceptron(eta=<span style="color: #da8548; font-weight: bold;">0.1</span>, n_iter=<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr"> 96: </span>  ppn.fit(X, y)
<span class="linenr"> 97: </span>  plt.clf()
<span class="linenr"> 98: </span>  plt.plot(<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(ppn.errors_) + <span style="color: #da8548; font-weight: bold;">1</span>), ppn.errors_, marker=<span style="color: #98be65;">'o'</span>)
<span class="linenr"> 99: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">100: </span>  plt.ylabel(<span style="color: #98be65;">'Number of updates'</span>)
<span class="linenr">101: </span>
<span class="linenr">102: </span>  plt.savefig(<span style="color: #98be65;">'02_07.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">103: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">104: </span>
<span class="linenr">105: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">### A function for plotting decision regions</span>
<span class="linenr">106: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr">107: </span>
<span class="linenr">108: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr">109: </span>      <span style="color: #dcaeea;">markers</span> = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr">110: </span>      <span style="color: #dcaeea;">colors</span> = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr">111: </span>      <span style="color: #dcaeea;">cmap</span> = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr">112: </span>
<span class="linenr">113: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr">114: </span>      <span style="color: #dcaeea;">x1_min</span>, <span style="color: #dcaeea;">x1_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">115: </span>      <span style="color: #dcaeea;">x2_min</span>, <span style="color: #dcaeea;">x2_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">116: </span>      <span style="color: #dcaeea;">xx1</span>, <span style="color: #dcaeea;">xx2</span> = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr">117: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr">118: </span>      <span style="color: #dcaeea;">Z</span> = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr">119: </span>      <span style="color: #dcaeea;">Z</span> = Z.reshape(xx1.shape)
<span class="linenr">120: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr">121: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr">122: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr">123: </span>
<span class="linenr">124: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot class samples</span>
<span class="linenr">125: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr">126: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>], 
<span class="linenr">127: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">128: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>, 
<span class="linenr">129: </span>                      c=colors[idx],
<span class="linenr">130: </span>                      marker=markers[idx], 
<span class="linenr">131: </span>                      label=cl, 
<span class="linenr">132: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr">133: </span>  plt.clf()
<span class="linenr">134: </span>  plot_decision_regions(X, y, classifier=ppn)
<span class="linenr">135: </span>  plt.xlabel(<span style="color: #98be65;">'sepal length [cm]'</span>)
<span class="linenr">136: </span>  plt.ylabel(<span style="color: #98be65;">'petal length [cm]'</span>)
<span class="linenr">137: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">138: </span>
<span class="linenr">139: </span>
<span class="linenr">140: </span>  plt.savefig(<span style="color: #98be65;">'02_08.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">141: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">142: </span>
</pre>
</div>

<pre class="example">
       0    1    2    3               4
145  6.7  3.0  5.2  2.3  Iris-virginica
146  6.3  2.5  5.0  1.9  Iris-virginica
147  6.5  3.0  5.2  2.0  Iris-virginica
148  6.2  3.4  5.4  2.3  Iris-virginica
149  5.9  3.0  5.1  1.8  Iris-virginica
</pre>



<div id="org8640604" class="figure">
<p><img src="images/02_06.png" alt="02_06.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 1: </span>待分類的兩種花依其不同屬性之分佈狀況</p>
</div>


<div id="org8397c1e" class="figure">
<p><img src="images/02_07.png" alt="02_07.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 2: </span>感知器訓練過程</p>
</div>


<div id="orgb6e6e01" class="figure">
<p><img src="images/02_08.png" alt="02_08.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 3: </span>訓練後的分類結果</p>
</div>
</div>
</div>

<div id="outline-container-org27ae31b" class="outline-2">
<h2 id="org27ae31b"><span class="section-number-2">2</span> 名詞解釋</h2>
<div class="outline-text-2" id="text-2">
<p>
本節資料來源: <a href="https://kknews.cc/zh-tw/tech/b4zkbom.html">主流的深度學習模型有哪些？</a><br />
</p>
<ul class="org-ul">
<li>perceptron: 單一神經元，可視為一層的neural network<br /></li>
<li>MPL: Multi-layer perceptron: 多層的perceptron，也可視為neural network<br /></li>
<li>Shallow neural netwokrs: 一般來說有1-2個隱藏層的neural network就可稱為多層，準確來說叫淺層<br /></li>
<li>Deep learning: 隨著隱藏層增加，更深的神經網路(一般來說超過5層)就都叫深度學習<br /></li>
<li>「深度」只是一個商業概念，很多時候工業界把3層隱藏層也叫做「深度學習」<br /></li>
<li>在機器學習領域的約定俗成是，名字中有深度(Deep)的網絡僅代表其有超過5-7層的隱藏層。<br /></li>
</ul>
</div>

<div id="outline-container-org0462128" class="outline-3">
<h3 id="org0462128"><span class="section-number-3">2.1</span> Supervised neural networks</h3>
<div class="outline-text-3" id="text-2-1">
</div>
<ol class="org-ol">
<li><a id="org692dcec"></a>ANN (Artificial Neural Network) / DNN (Deep Neural Networks)<br />
<div class="outline-text-4" id="text-2-1-1">
<p>
神經網絡的基礎模型是感知機(Perceptron)，因此神經網絡也可以叫做多層感知機(Multi-layer Perceptron)，簡稱MLP。單層感知機叫做感知機，多層感知機(MLP)≈人工神經網絡(ANN)。<br />
</p>

<p>
那麼多層到底是幾層？一般來說有1-2個隱藏層的神經網絡就可以叫做多層，準確的說是(淺層)神經網絡(Shallow Neural Networks)。隨著隱藏層的增多，更深的神經網絡(一般來說超過5層)就都叫做深度學習(DNN)。<br />
</p>

<p>
然而，「深度」只是一個商業概念，很多時候工業界把3層隱藏層也叫做「深度學習」，所以不要在層數上太較真。在機器學習領域的約定俗成是，名字中有深度(Deep)的網絡僅代表其有超過5-7層的隱藏層。<br />
</p>
</div>
</li>

<li><a id="org2581c9c"></a>RNN: Recurrent Neural Network，Recursive neural networks。<br />
<div class="outline-text-4" id="text-2-1-2">
<p>
雖然很多時候我們把這兩種網絡都叫做RNN，但事實上這兩種網路的結構事實上是不同的。而我們常常把兩個網絡放在一起的原因是：它們都可以處理有序列的問題，比如時間序列等。<br />
  舉個最簡單的例子，我們預測股票走勢用RNN就比普通的DNN效果要好，原因是股票走勢和時間相關，今天的價格和昨天、上周、上個月都有關係。而RNN有「記憶」能力，可以「模擬」數據間的依賴關係(Dependency)。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org641621d"></a>LSTM:<br />
<div class="outline-text-5" id="text-2-1-2-1">
<p>
為了加強這種RNN的「記憶能力」，人們開發各種各樣的變形體，如非常著名的Long Short-term Memory(LSTM)，用於解決「長期及遠距離的依賴關係」。如下圖所示，左邊的小圖是最簡單版本的循環網絡，而右邊是人們為了增強記憶能力而開發的LSTM。<br />
</p>

<div id="org92e4a29" class="figure">
<p><img src="images/3r5o0000r126proo7o8q.jpg" alt="3r5o0000r126proo7o8q.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 4: </span>LSTM</p>
</div>
</div>
</li>
<li><a id="orgc8014b6"></a>Bi-directional RNN:<br />
<div class="outline-text-5" id="text-2-1-2-2">
<p>
另一個循環網絡的變種 - 雙向循環網絡(Bi-directional RNN)也是現階段自然語言處理和語音分析中的重要模型。開發雙向循環網絡的原因是語言/語音的構成取決於上下文，即「現在」依託於「過去」和「未來」。單向的循環網絡僅著重於從「過去」推出「現在」，而無法對「未來」的依賴性有效的建模。<br />
</p>
</div>
</li>
</ol>
</li>

<li><a id="org425dc66"></a>卷積網絡(Convolutional Neural Networks)<br />
<div class="outline-text-4" id="text-2-1-3">
<p>
卷積運算是一種數學計算，和矩陣相乘不同，卷積運算可以實現稀疏相乘和參數共享，可以壓縮輸入端的維度。和普通DNN不同，CNN並不需要為每一個神經元所對應的每一個輸入數據提供單獨的權重。<br />
</p>
<p width="500">
<img src="images/CNN-1.png" alt="CNN-1.png" width="500" /><br />
以上圖為例，卷積、池化的過程將一張圖片的維度進行了壓縮。從圖示上我們不難看出卷積網絡的精髓就是適合處理結構化數據，而該數據在跨區域上依然有關聯。<br />
</p>

<p>
應用場景：雖然我們一般都把CNN和圖片聯繫在一起，但事實上CNN可以處理大部分格狀結構化數據(Grid-like Data)。舉個例子，圖片的像素是二維的格狀數據，時間序列在等時間上抽取相當於一維的的格狀數據，而視頻數據可以理解為對應視頻幀寬度、高度、時間的三維數據。<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-orge987c24" class="outline-3">
<h3 id="orge987c24"><span class="section-number-3">2.2</span> Unsupervised Pre-trained Neural Networks</h3>
<div class="outline-text-3" id="text-2-2">
</div>
<ol class="org-ol">
<li><a id="org5bdc366"></a>Deep Generative Models<br />
<ol class="org-ol">
<li><a id="orga6fbed2"></a>Boltzmann Machines, Restricted Boltzmann Machines<br /></li>
<li><a id="org34d80cf"></a>Deep Belief Neural Networks<br /></li>
<li><a id="org7c2482a"></a>Generative Adversarial Networks<br />
<div class="outline-text-5" id="text-2-2-1-3">
<p>
簡單的說，GAN訓練兩個網絡：1. 生成網絡用於生成圖片使其與訓練數據相似 2. 判別式網絡用於判斷生成網絡中得到的圖片是否是真的是訓練數據還是偽裝的數據。生成網絡一般有逆卷積層(deconvolutional layer)而判別網絡一般就是上文介紹的CNN。下圖左邊是生成網絡，右邊是判別網絡，相愛相殺。<br />
</p>
</div>
</li>
</ol>
</li>
</ol>
</div>
</div>

<div id="outline-container-org5646fb4" class="outline-2">
<h2 id="org5646fb4"><span class="section-number-2">3</span> 類神經網路</h2>
<div class="outline-text-2" id="text-3">
<p>
神經網絡是非常重要的一種機器學習機制，它是一種模仿人類大腦學習機制的一種方法。人類的大腦從外界接受刺激，並處理這些輸入（通過神經元處理），最終產生輸出<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>。<br />
當任務變得複雜的時候，大腦會使用多個神經元來形成一個複雜的網絡，並在神經元之間傳遞信息，人工神經網絡就是模仿這種處理機制的一種算法。如下圖的網絡是一種由多個互聯的神經元組成的網絡<sup><a id="fnr.1.100" class="footref" href="#fn.1">1</a></sup>。<br />
</p>


<div id="org850aa23" class="figure">
<p><img src="images/2021-05-22_23-30-45.jpg" alt="2021-05-22_23-30-45.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 5: </span>DEEP NEURAL NETWORK</p>
</div>

<ul class="org-ul">
<li>圖<a href="#org850aa23">5</a>中的圓圈就是神經元(perceptron)。每一個perceptron都由weight、bias和activation function組成。數據由input layer輸入。然後perceptron基於weight和bias，通過線性變換將輸入數據進行轉換。activation function則是用來完成非線性的轉換。數據從輸入層輸入後，會來到隱藏層。隱藏層將會對數據進行處理並輸送到輸出層。這種數據處理方式就是著名的前向傳播。<br /></li>
<li>當輸出層的結果和我們期望的結果相差很大怎麼辦呢？在神經網絡中，我們可以基於這種錯誤來更新神經元的權重和偏移。這種處理方式被稱為後向傳播算法。一旦所有的數據經歷過這個處理，最終的權重和便宜就可以用來做預測了<sup><a id="fnr.1.100" class="footref" href="#fn.1">1</a></sup>。<br /></li>
</ul>


<p>
A normal neural network<sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup>:<br />
</p>

<div id="org5d41465" class="figure">
<p><img src="images/nn.gif" alt="nn.gif" width="500" /><br />
</p>
<p><span class="figure-number">Figure 6: </span>Neural Network</p>
</div>
</div>

<div id="outline-container-org6ba26ff" class="outline-3">
<h3 id="org6ba26ff"><span class="section-number-3">3.1</span> Perceptron</h3>
<div class="outline-text-3" id="text-3-1">
<p class="verse">
Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks.<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org9168caa"></a>Version #1: 使用weight<br />
<div class="outline-text-4" id="text-3-1-1">
<p>
感知器（perceptron）是 人造神經元（artificial neuron）的一種，也是最基本的一種。它接受一些輸入，產生一個輸出。<br />
</p>
<div class="org-src-container">
<pre class="src src-dot">digraph perceptron {
  rankdir=LR
  sigma [label=<span style="color: #98be65;">"weighted\n sum"</span>]
  output [label=<span style="color: #98be65;">"output"</span> shape=<span style="color: #98be65;">"record"</span>]
  x1 -&gt; sigma [label = w1]
  x2 -&gt; sigma [label = w2]
  sigma -&gt; output
}
</pre>
</div>

<div id="org5f67967" class="figure">
<p><img src="./images/perceptron-1.png" alt="perceptron-1.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 7: </span>Perceptron version 1</p>
</div>
<ul class="org-ul">
<li>這種架構的輸入/輸出關係為線性<br /></li>
<li>神經網路中再多的線性perceptron叠加，仍為線性<br /></li>
<li>無法解決 <b>線性不可分</b> 的問題<br /></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org10c2bf6"></a>線性可分 v.s. 線性不可分<br />
<div class="outline-text-5" id="text-3-1-1-1">
<p>
簡而言之，如果存在一個超平面完全分離H元素和M元素，那麼上面的表達式表示H和M是線性可分的<sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup>。<br />
</p>

<div id="org101e4c3" class="figure">
<p><img src="images/2021-05-23_14-10-30.jpg" alt="2021-05-23_14-10-30.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 8: </span>線性和非線性分類</p>
</div>

<p>
在圖<a href="#org101e4c3">8</a>中，A顯示了一個線性分類問題，B顯示了一個非線性的分類問題。在A中，我們的決策邊界是一個線性的，它將藍色的點和綠色的點完全分開。在這個場景中，可以實現幾個線性分類器。<br />
</p>
</div>
</li>
<li><a id="org025170b"></a>低維映射至高維<br />
<div class="outline-text-5" id="text-3-1-1-2">
<p>
在SVM的解決方案中，可以透過一個非線性的映射將低維空間線性不可分的樣本轉換至高維空間，使其成為線性可分   <sup><a id="fnr.4" class="footref" href="#fn.4">4</a></sup>，例如:<br />
</p>

<div id="org9691c53" class="figure">
<p><img src="images/2021-05-23_14-14-31.jpg" alt="2021-05-23_14-14-31.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 9: </span>Kernal function mapping</p>
</div>
</div>
</li>
</ol>
</li>

<li><a id="org6c0b793"></a>Version #2: 加入bias<br />
<div class="outline-text-4" id="text-3-1-2">
<div class="org-src-container">
<pre class="src src-dot">digraph perceptron {
  rankdir=LR
  sigma [label=<span style="color: #98be65;">"weighted\n sum"</span>]
  output [label=<span style="color: #98be65;">"output"</span> shape=<span style="color: #98be65;">"record"</span>]
  x1 -&gt; sigma [label = w1]
  x2 -&gt; sigma [label = w2]
  1 -&gt; sigma[label = b]
  sigma -&gt; output
}
</pre>
</div>

<div id="orgb47ee75" class="figure">
<p><img src="./images/perceptron-2.png" alt="perceptron-2.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 10: </span>Perceptron version 2</p>
</div>
<ul class="org-ul">
<li>不加 bias 你的分類線(面)就必須過原點，這顯然是不靈活的<br /></li>
<li>透過bias，可以將NN進行左右調整，以適應(fit)更多情況<br /></li>
<li>可以將bias視為一個activate perceptron的threshold<br /></li>
<li>bias也可以視為當輸入均為0時的輸出值<br /></li>
<li>從仿生學的角度，刺激生物神經元使它興奮需要刺激強度超過一定的閾值，同樣神經元模型也仿照這點設置了bias<br /></li>
</ul>
</div>
</li>

<li><a id="orgde38f40"></a>Version #3: 加入activation function<br />
<div class="outline-text-4" id="text-3-1-3">
<p>
加入activation function<br />
</p>
<div class="org-src-container">
<pre class="src src-dot">digraph perceptron {
  rankdir=LR
  sigma [label=<span style="color: #98be65;">"weighted\n sum"</span>]
  av [label=<span style="color: #98be65;">"activation\n function"</span>]
  output [label=<span style="color: #98be65;">"output"</span> shape=<span style="color: #98be65;">"record"</span>]
  x1 -&gt; sigma [label = w1]
  x2 -&gt; sigma [label = w2]
  1 -&gt; sigma[label = b]
  sigma -&gt; av -&gt; output
}
</pre>
</div>

<div id="org0b9d003" class="figure">
<p><img src="./images/perceptron-3.png" alt="perceptron-3.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 11: </span>Perceptron version 2</p>
</div>
</div>
</li>

<li><a id="org0fc9a30"></a>結論: 什麼是perceptron<br />
<div class="outline-text-4" id="text-3-1-4">
<p>
其實Perceptron就只是一個兩層的神經網路，輸入層和輸出層<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org5cb9edc" class="outline-3">
<h3 id="org5cb9edc"><span class="section-number-3">3.2</span> 為什麼沒有Activation Function的perceptron為linear model</h3>
<div class="outline-text-3" id="text-3-2">
<p>
思考下方堆疊兩層感知器的例子(這裡我們捨棄偏權值b，使問題單純化)：<br />
</p>

<div id="org7c4f9a5" class="figure">
<p><img src="images/2perceptrons.png" alt="2perceptrons.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 12: </span>Stacked perceptrons</p>
</div>

<p>
其中<br />
\[
y_1 = w_1^1x_1 + w_2^1x_2 \\
y_2 = w_3^1x_1 + w_4^1x_2 \\
\hat{y} = w_1^2y_1 + w_2^2y_2
\]<br />
若將\(y_1\)與\(y_2\)代入\(\hat{y}\)，可以發現仍然是原始數據的線性組合，代表即使堆疊多層的感知器時，若沒有使用激活函數，本質上可以被單層感知器取代，如下<sup><a id="fnr.5" class="footref" href="#fn.5">5</a></sup>：<br />
\[\hat{y} = w_1^2y_1 + w_2^2y_2  = (w_1^1w_1^2 + w_2^1x_2^2)x_1 + (w_3^1w_1^2 + w_4^1x_2^2)x_2\]<br />
</p>
</div>
</div>

<div id="outline-container-orgc187313" class="outline-3">
<h3 id="orgc187313"><span class="section-number-3">3.3</span> Activation Function</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Activation function是神經網絡中極其重要的概念。它們決定了某個神經元是否被activate，這個神經元接受到的信息是否是有用的，是否該留下或者是該拋棄。<br />
Activation function的形式如下<sup><a id="fnr.1.100" class="footref" href="#fn.1">1</a></sup>：<br />
</p>
<ul class="org-ul">
<li>在神經元裡定義一些權重(weight)，經過計算之後，判斷結果是否超過一個閾值(threshold)，如果超過，神經元輸出為 1；反之，則輸出 0。<br /></li>
<li>如果不用Activation function，每一層output都是上層input的線性函數，無論神經網絡有多少層，輸出都是輸入的線性組合。<br /></li>
<li>加入非線性的function就能改變線性問題<br /></li>
</ul>
</div>

<ol class="org-ol">
<li><a id="org0629af7"></a>Step function<br />
<div class="outline-text-4" id="text-3-3-1">
<p>
Step function，若輸入超過 0，則輸出 1；若輪入小於等於 0，則輸出 0，如公式\eqref{orgfa0721b}所示，其函數圖形則如圖<a href="#orgcc1a8e5">13</a>所示。<br />
</p>
\begin{equation}
\label{orgfa0721b}
h(x) = \begin{cases}
1 & \text{if } x > 0 \\
0 & \text{if } x\leq 0
\end{cases}
\end{equation}
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">step_function</span>(x):
<span id="coderef-why" class="coderef-off"><span class="linenr"> 5: </span>    <span style="color: #51afef;">return</span> np.array(x&gt;<span style="color: #da8548; font-weight: bold;">0</span>, dtype=np.<span style="color: #c678dd;">int</span>)</span>
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">x</span> = np.arange(-<span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>)
<span class="linenr"> 8: </span><span style="color: #dcaeea;">y</span> = step_function(x)
<span class="linenr"> 9: </span>plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">4</span>,<span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr">10: </span>plt.plot(x, y)
<span class="linenr">11: </span>plt.ylim(-<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">1.1</span>)
<span class="linenr">12: </span>plt.savefig(<span style="color: #98be65;">"images/stepFuncPlot.png"</span>)
<span class="linenr">13: </span><span style="color: #51afef;">return</span> <span style="color: #98be65;">"images/stepFuncPlot.png"</span>
</pre>
</div>

<div id="orgcc1a8e5" class="figure">
<p><img src="images/stepFuncPlot.png" alt="stepFuncPlot.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 13: </span>階梯函數圖</p>
</div>
<ul class="org-ul">
<li>為什麼第<a href="#coderef-why" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-why');" onmouseout="CodeHighlightOff(this, 'coderef-why');">5</a>行(np.array(x&gt;0, dtype=np.int))這樣寫會變成0/1<br /></li>
<li>當調整參數時，節點輸出的值在 0 和 1 之間躍遷，對調整參數造成很大不便。<br /></li>
</ul>
</div>
</li>

<li><a id="orgb982f3e"></a>Sigmoid 函數<br />
<div class="outline-text-4" id="text-3-3-2">
<p>
公式\eqref{orgd83c4ed}即為 sigmoid 函數(sigmoid function)，其中的\(exp(-x)\)代表\(e^{-x}\)，\(e\)為納皮爾常數(Napier&rsquo;s constant)2.71828&#x2026;的實數。<br />
</p>
\begin{equation}
\label{orgd83c4ed}
h(x) = \frac{1}{1+exp(-x)} = \frac{1}{1+e^{-x}}
\end{equation}

<p>
sigmoid 函數的 python 實作如下所述，而其圖形結果為平滑曲線(圖<a href="#org1692dc0">14</a>)，針對輸入產生連續性的輸出，但仍與階梯函數相同，以 0 為界線，這種平滑度對於神經網路有相當重要的意義。此外，step function只能回傳 0 或 1，而 sigmoid 函數可以回傳實數。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr"> 3: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">sigmoid</span>(x):
<span class="linenr"> 4: </span>    <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span> / (<span style="color: #da8548; font-weight: bold;">1</span> + np.exp(-x))
<span class="linenr"> 5: </span><span style="color: #dcaeea;">x</span> = np.arange(-<span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>)
<span class="linenr"> 6: </span><span style="color: #dcaeea;">y</span> = sigmoid(x)
<span class="linenr"> 7: </span><span style="color: #51afef;">print</span>(y)
<span class="linenr"> 8: </span>plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">4</span>,<span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 9: </span>plt.plot(x, y)
<span class="linenr">10: </span>plt.ylim(-<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">1.1</span>)
<span class="linenr">11: </span>plt.savefig(<span style="color: #98be65;">"sigmoidplot2.png"</span>)
<span class="linenr">12: </span><span style="color: #51afef;">return</span> <span style="color: #98be65;">"sigmoidplot2.png"</span>
</pre>
</div>

<div id="org1692dc0" class="figure">
<p><img src="sigmoidplot2.png" alt="sigmoidplot2.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 14: </span>sigmoid 函數圖</p>
</div>
<ul class="org-ul">
<li>非線性功能<br /></li>
<li>輸出：（0,1）<br /></li>
<li>處理極端值<br /></li>
<li>常用於隱藏層和輸出層<br /></li>
<li>常用於二元分類<br /></li>
<li>梯度消失問題<br /></li>
</ul>
</div>
</li>

<li><a id="orgfbf16cb"></a>ReLU 函數<br />
<div class="outline-text-4" id="text-3-3-3">
<p>
神經網路感知器使用的活化函數多為非線性函數，如階梯函數與 sigmoid 函數，其主要原因在於，若在神經網路中使用線性函數，則不論加深多少層，這些函數都能合併為一個單一函數。例如，一個以\(h(x)=cx\)為活化函數的三層網路，其執行結果便相當於\(y(x)=h(h(h(x)))\)，亦即，其執行網果就如同一個以\(y(h)=ax\)為活化函數的一層網路，其中\(a=c^{3}\)。<br />
</p>

<p>
雖然 sigmoid 函數很早就應用於神經網路中，但最近較常使用的為 ReLU (Rectified Linear Unit)函數，若輸入超過 0，則直接輸出；若輪入小於 0，則輸出 0，如公式\eqref{org6ba0ebd}所示，其函數圖形則如圖<a href="#org87c8118">15</a>所示。<br />
</p>
\begin{equation}
\label{org6ba0ebd}
h(x) = \begin{cases} x & \text{if } x > 0
\\ 0 & \text{if } x\leq 0 \end{cases}
\end{equation}
<p>
or<br />
\[h(x) = max(0,x)\]<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr"> 3: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">relu</span>(x):
<span class="linenr"> 4: </span>      <span style="color: #51afef;">return</span> np.maximum(<span style="color: #da8548; font-weight: bold;">0</span>, x)
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">x</span> = np.arange(-<span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>)
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">y</span> = relu(x)
<span class="linenr"> 7: </span>  <span style="color: #51afef;">print</span>(y)
<span class="linenr"> 8: </span>  plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">4</span>,<span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 9: </span>  plt.plot(x, y)
<span class="linenr">10: </span>  plt.ylim(-<span style="color: #da8548; font-weight: bold;">0.5</span>, <span style="color: #da8548; font-weight: bold;">5</span>)
<span class="linenr">11: </span>  plt.savefig(<span style="color: #98be65;">"ReLUPlot.png"</span>)
<span class="linenr">12: </span>  <span style="color: #51afef;">return</span> <span style="color: #98be65;">"ReLUPlot.png"</span>
</pre>
</div>

<div id="org87c8118" class="figure">
<p><img src="ReLUPlot.png" alt="ReLUPlot.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 15: </span>ReLU 函數圖</p>
</div>
<ul class="org-ul">
<li>非線性功能<br /></li>
<li>輸出範圍：（0，\(\infty\)）<br /></li>
<li>捨棄一些信息<br /></li>
<li>常用於隱藏層<br /></li>
<li>梯度爆炸問題<br /></li>
</ul>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-org70c9525" class="outline-2">
<h2 id="org70c9525"><span class="section-number-2">4</span> Story of gate: From perceptron to MLP</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org7943bb9" class="outline-3">
<h3 id="org7943bb9"><span class="section-number-3">4.1</span> Example #1</h3>
<div class="outline-text-3" id="text-4-1">
</div>
<ol class="org-ol">
<li><a id="orgda2c9e5"></a>Question<br />
<div class="outline-text-4" id="text-4-1-1">
<p>
有三個A、一個B，如何進行分類?<br />
</p>

<div id="orgfc78d29" class="figure">
<p><img src="images/2021-05-24_00-48-56.jpg" alt="2021-05-24_00-48-56.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 16: </span>分類任務:問題</p>
</div>
</div>
</li>
<li><a id="org05ad2f3"></a>Idea<br />
<div class="outline-text-4" id="text-4-1-2">
<p>
最簡單的分類方式是在A和B中間直接找條直線(\(w_1x_1+w_2x_2+b=0\))就可以將A和B完整切出兩個區塊，然後再搭配階梯函數(step function)將&gt;0與&lt;=0分別設為1與0，用來代表類別0與1。該直線方程式如下：<br />
</p>
\begin{equation}
\label{org2c741c1}
y = \begin{cases}
1, & w_1x_1 + w_2x_2-b>0 \\
0, & w_1x_1 + w_2x_2-b\leq0 \\
\end{cases}
\end{equation}
</div>
</li>
<li><a id="org1a5a412"></a>Solution<br />
<div class="outline-text-4" id="text-4-1-3">
<p>
經過無數的嚐試錯誤，也許我們可以矇到一個如下的方程式<br />
</p>

<div id="orge21cd69" class="figure">
<p><img src="images/2021-05-24_00-50-07.jpg" alt="2021-05-24_00-50-07.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 17: </span>分類任務:Solution</p>
</div>

<p>
如果畫成Perceptron的圖:<br />
</p>
<div class="org-src-container">
<pre class="src src-dot">digraph perceptron {
  rankdir=LR
  sigma [label=<span style="color: #98be65;">"weighted\n sum"</span>]
  output [label=<span style="color: #98be65;">"output"</span> shape=<span style="color: #98be65;">"record"</span>]
  x1 -&gt; sigma [label = 1]
  x2 -&gt; sigma [label = 1]
  1 -&gt; sigma[label = -0.5]
  sigma -&gt; <span style="color: #98be65;">"step\nfunction"</span> -&gt; output
}
</pre>
</div>
<p width="500">
<img src="./images/orGatePerceptron.png" alt="orGatePerceptron.png" width="500" /><br />
如果將圖<a href="#orge21cd69">17</a>的四點點代入y(方程式\eqref{org2c741c1}):<br />
</p>
\begin{align*}
A(0,1) \rightarrow y &= f(0,1) = f(1\times0+1\times1-0.5) = f(0.5) = 1 \\
A(1,0) \rightarrow y &= f(1,0) = f(1\times1+1\times0–0.5) = f(0.5) = 1\\
A(1,1) \rightarrow y &= f(1,1) = f(1\times1+1\times1–0.5) = f(1.5) = 1\\
B(0,0) \rightarrow y &= f(0,0) = f(1\times0+1\times0–0.5) = f(-0.5) = 0\\
\end{align*}
</div>
</li>
<li><a id="orgbc040bf"></a>OR gate<br />
<div class="outline-text-4" id="text-4-1-4">
<p>
有點計概基礎的同學，應該可以發現圖<a href="#orgfc78d29">16</a>與OR邏輯閘一致，其真值表如下<br />
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">A</th>
<th scope="col" class="org-right">B</th>
<th scope="col" class="org-right">A OR B</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>
</div>
</li>
<li><a id="orgb2e1008"></a>Python實作<br />
<div class="outline-text-4" id="text-4-1-5">
<p>
上述 OR gate的python實作如下<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">step_function</span>(x):
<span class="linenr"> 4: </span>    <span style="color: #51afef;">return</span> np.array(x&gt;<span style="color: #da8548; font-weight: bold;">0</span>, dtype=np.<span style="color: #c678dd;">int</span>)
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">OR</span>(x1, x2):
<span class="linenr"> 7: </span>    <span style="color: #dcaeea;">x</span> = np.array([x1, x2])
<span class="linenr"> 8: </span>    <span style="color: #dcaeea;">w</span> = np.array([<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr"> 9: </span>    <span style="color: #dcaeea;">b</span> = -<span style="color: #da8548; font-weight: bold;">0.5</span>
<span class="linenr">10: </span>    <span style="color: #dcaeea;">theta</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">11: </span>    <span style="color: #dcaeea;">y</span> = np.<span style="color: #c678dd;">sum</span>(w*x) + b
<span class="linenr">12: </span>    <span style="color: #51afef;">return</span> step_function(y)
<span class="linenr">13: </span>
<span class="linenr">14: </span><span style="color: #51afef;">print</span>(<span style="color: #98be65;">"0 OR 0 -&gt; "</span>, OR(<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">0</span>))
<span class="linenr">15: </span><span style="color: #51afef;">print</span>(<span style="color: #98be65;">"0 OR 1 -&gt; "</span>, OR(<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr">16: </span><span style="color: #51afef;">print</span>(<span style="color: #98be65;">"1 OR 0 -&gt; "</span>, OR(<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">0</span>))
<span class="linenr">17: </span><span style="color: #51afef;">print</span>(<span style="color: #98be65;">"1 OR 1 -&gt; "</span>, OR(<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">1</span>))
</pre>
</div>

<pre class="example">
0 OR 0 -&gt;  0
0 OR 1 -&gt;  1
1 OR 0 -&gt;  1
1 OR 1 -&gt;  1
</pre>
</div>
</li>
</ol>
</div>

<div id="outline-container-org1b2eb02" class="outline-3">
<h3 id="org1b2eb02"><span class="section-number-3">4.2</span> Example #2</h3>
<div class="outline-text-3" id="text-4-2">
<p>
上述範例中，我們以瞎貓精神找出了一組solution解決了OR gate的分類問題，請比照辦理，解決AND gate，建構出perceptro，實作出python code.<br />
已知AND gate真值表如下<br />
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">A</th>
<th scope="col" class="org-right">B</th>
<th scope="col" class="org-right">A AND B</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-org4565e4a" class="outline-3">
<h3 id="org4565e4a"><span class="section-number-3">4.3</span> XOR Problem</h3>
<div class="outline-text-3" id="text-4-3">
</div>
<ol class="org-ol">
<li><a id="orgee4857f"></a>Question<br />
<div class="outline-text-4" id="text-4-3-1">
<p>
XOR(互斥或)真值表如下:<br />
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">A</th>
<th scope="col" class="org-right">B</th>
<th scope="col" class="org-right">A XOR B</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>
<p>
其輸入/輸出分佈圖為<br />
</p>

<div id="org520dd4a" class="figure">
<p><img src="images/2021-05-24_14-15-53.jpg" alt="2021-05-24_14-15-53.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 18: </span>XOR Gate</p>
</div>
</div>
</li>
<li><a id="orgc197b1c"></a>Idea<br />
<div class="outline-text-4" id="text-4-3-2">
<p>
這個時候一般線性的分類就沒有辦法很完美分割(如下圖)，所以就需要一些變形的方法來達到目的。<br />
</p>

<div id="org6600dd0" class="figure">
<p><img src="images/2021-05-24_14-18-51.jpg" alt="2021-05-24_14-18-51.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 19: </span>XOR Gate Solution ideas</p>
</div>

<p class="verse">
一個便當吃不飽那就吃兩個阿<br />
&#x2013;馬惠帝<br />
</p>
<p>
即便一個人再如何bumbler，仍有可能提出一些明智的話語，就如同星爺告訴我們的<br />
</p>
<p width="500">
<img src="images/2021-05-24_14-25-08.jpg" alt="2021-05-24_14-25-08.jpg" width="500" /><br />
所以，一條線無法分割&#x2013;那就用兩條啊<br />
</p>
</div>
</li>
<li><a id="org912df59"></a>Solution<br />
<div class="outline-text-4" id="text-4-3-3">
<p width="500">
<img src="images/2021-05-24_14-28-04.jpg" alt="2021-05-24_14-28-04.jpg" width="500" /><br />
如前所述，一條線為一個perceptron，這裡會用到兩個<br />
</p>
<ul class="org-ul">
<li>\(h_1(x) = x_1 + x_2 - 0.5\)<br /></li>
<li>\(h_2(x) = x_1 + x_2 - 1.5\)<br /></li>
</ul>

<div id="org0b88e50" class="figure">
<p><img src="images/2021-05-24_14-36-07.jpg" alt="2021-05-24_14-36-07.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 20: </span>XOR Gate Solution 2</p>
</div>

<p>
將圖<a href="#org6600dd0">19</a>的4個點代入\(h_1\):<br />
</p>
\begin{align*}
h_1(0,0) &= f(1\times0+1\times0–0.5) = f(-0.5) = 0\\
h_1(0,1) &= f(1\times0+1\times1-0.5) = f(0.5) = 1\\
h_1(1,0) &= f(1\times1+1\times0–0.5) = f(0.5) = 1\\
h_1(1,1) &= f(1\times1+1\times1–0.5) = f(1.5) = 1\\
\end{align*}

<p>
將圖<a href="#org6600dd0">19</a>的4個點代入\(h_2\):<br />
</p>
\begin{align*}
h_2(0,0) &= f(1\times0+1\times0–1.5) = f(-1.5) = 0\\
h_2(0,1) &= f(1\times0+1\times1-1.5) = f(-0.5) = 0\\
h_2(1,0) &= f(1\times1+1\times0–1.5) = f(-0.5) = 0\\
h_2(1,1) &= f(1\times1+1\times1–1.5) = f(0.5) = 1\\
\end{align*}

<p>
由上可知:<br />
</p>
<ul class="org-ul">
<li>(0, 0)帶入第1個perceptron \(h_1(0,0)\)輸出-0.5、帶入第2個perceptron \(h_2(0,0)\)輸出-1.5；(-0.5, -1.5)再經由step function轉換輸出(0,0)<br /></li>
<li>(0, 1)帶入第1個perceptron \(h_1(0,1)\)輸出0.5、帶入第2個perceptron \(h_2(0,1)\)輸出-0.5；(0.5, -0.5)再經由step function轉換輸出(1,0)<br /></li>
<li>(1, 0)帶入第1個perceptron \(h_1(1,0)\)輸出0.5、帶入第2個perceptron \(h_2(1,0)\)輸出-0.5；(0.5, -0.5)再經由step function轉換輸出(1,0)<br /></li>
<li>(1, 1)帶入第1個perceptron \(h_1(1,1)\)輸出1.5、帶入第2個perceptron \(h_2(1,1)\)輸出0.5；(1.5, 0.5)再經由step function轉換輸出(1,1)<br /></li>
</ul>

<p>
即<br />
</p>
\begin{align*}
data(0,0) &= f(h_1,h_2) = (0,0) \\
data(0,1) &= f(h_1,h_2) = (1,0) \\
data(1,0) &= f(h_1,h_2) = (1,0) \\
data(1,1) &= f(h_1,h_2) = (1,1) \\
\end{align*}

<p>
這相當於透過兩個perceptron將原本的輸入做特徵空間轉換，如圖<a href="#orgb756f27">21</a>:<br />
</p>

<div id="orgb756f27" class="figure">
<p><img src="images/2021-05-24_16-12-36.jpg" alt="2021-05-24_16-12-36.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 21: </span>XOR Gate Solution 3</p>
</div>

<p>
這個時候只要設計一個線性分類器就可以完美分割兩類的資料了阿，如圖<a href="#orgc002b44">22</a>:<br />
</p>

<div id="orgc002b44" class="figure">
<p><img src="images/2021-05-24_16-13-44.jpg" alt="2021-05-24_16-13-44.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 22: </span>XOR Gate Solution 4</p>
</div>

<p>
XOR問題的神經網路結構如下圖:<br />
</p>

<div id="org23b9ab2" class="figure">
<p><img src="images/2021-05-24_16-15-02.jpg" alt="2021-05-24_16-15-02.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 23: </span>XOR Gate Solution 5</p>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgddb6f82" class="outline-3">
<h3 id="orgddb6f82"><span class="section-number-3">4.4</span> MLP (Multilayer perceptron)</h3>
<div class="outline-text-3" id="text-4-4">
<p>
由XOR問題的例子可以知道，第一層兩個Perceptron在做的事情其實是將資料投影到另一個特徵空間去,最後再把h1和h2的結果當作另一個Perceptron的輸入，再做一個下一層的Perceptron就可以完美分類XOR問題。<br />
</p>

<p>
上例其實就是一個Two-Layer Perceptrons，第一層的Perceptron輸出其實就是每個hidden node，所以如果hidden layer再多一層就是Three-Layer Perceptrons，所以很多層的Perceptrons組合起來就是多層感知機 (Multilayer perceptron, MLP)。MLP其實就是可以用多層和多個Perceptron來達到最後目的，有點像用很多個回歸方法/線性分類器一層一層疊加來達到目的。<br />
</p>

<p>
中間一堆的hidden layer其實就是在做資料的特徵擷取，可以降維，也可以增加維度，而這個過程不是經驗法則去設計，而是由資料去學習得來，最後的輸出才是做分類，所以最後一層也可以用SVM來分類。<br />
</p>

<p>
如果層數再多也可以稱為深度神經網路(deep neural network, DNN)，所以現在稱的DNN其實就是人工神經網路的MLP。有一說法是說因為MLP相關的神經網路在之前因為電腦限制所以performance一直都沒有很好的突破，所以相關研究沒有像SVM這麼的被接受，因此後來Deep learning的聲名大噪，MLP也換個較酷炫的名字(deep neural network)來反轉神經網路這個名稱的聲勢。<br />
</p>

<p>
多層感知機是一種前向傳遞類神經網路，至少包含三層結構(輸入層、隱藏層和輸出層)，並且利用到「倒傳遞」的技術達到學習(model learning)的監督式學習，以上是傳統的定義。現在深度學習的發展，其實MLP是深度神經網路(deep neural network, DNN)的一種special case，概念基本上一樣，DNN只是在學習過程中多了一些手法和層數會更多更深<sup><a id="fnr.6" class="footref" href="#fn.6">6</a></sup>。<br />
</p>

<p>
本章參考來源:<a href="https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF-%E5%A4%9A%E5%B1%A4%E6%84%9F%E7%9F%A5%E6%A9%9F-multilayer-perceptron-mlp-%E9%81%8B%E4%BD%9C%E6%96%B9%E5%BC%8F-f0e108e8b9af">機器學習- 神經網路(多層感知機 Multilayer perceptron, MLP)運作方式</a><br />
</p>
</div>
</div>
</div>

<div id="outline-container-org97e3a6c" class="outline-2">
<h2 id="org97e3a6c"><span class="section-number-2">5</span> Say Hello to DNN: MNIST</h2>
<div class="outline-text-2" id="text-5">
<p>
使用神經網路解決問題分為兩個步驟：「學習」與「推論」。<br />
</p>
<ul class="org-ul">
<li>學習指使用訓練資料進行權重參數的學習<br /></li>
<li>推論指使用學習過的參數進行資料分類<br /></li>
</ul>
</div>

<div id="outline-container-org7565392" class="outline-3">
<h3 id="org7565392"><span class="section-number-3">5.1</span> MNIST 資料集</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li>MNIST 是機器學習領域中相當著名的資料集，號稱機器學習領域的「Hello world.」，其重要性不言可喻。<br /></li>
<li>MNIST 資料集由 0~9 的數字影像構成(如圖<a href="#orge9e68c9">24</a>)，共計 60000 張訓練影像、10000 張測試影像。<br /></li>
<li>一般的 MMIST 資料集的用法為：使用訓練影像進行學習，再利用學習後的模型預測能否正確分類測試影像。<br /></li>
</ul>

<div id="orge9e68c9" class="figure">
<p><img src="images/MNIST.jpg" alt="MNIST.jpg" width="400" /><br />
</p>
<p><span class="figure-number">Figure 24: </span>MNIST 資料集內容範例</p>
</div>

<p>
準備資料是訓練模型的第一步，基礎資料可以是網上公開的資料集，也可以是自己的資料集。視覺、語音、語言等各種型別的資料在網上都能找到相應的資料集。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org3a49783"></a>準備 MNIST 資料<br />
<div class="outline-text-4" id="text-5-1-1">
<p>
MNIST 數據集來自美國國家標準與技術研究所, National Institute of Standards and Technology (NIST). 訓練集 (training set) 由來自 250 個不同人手寫的數字構成, 其中 50% 是高中學生, 50% 來自人口普查局 (the Census Bureau) 的工作人員. 測試集(test set) 也是同樣比例的手寫數字數據。MNIST 數據集可在 <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a> 獲取, 它包含了四個部分:<br />
</p>
<ol class="org-ol">
<li>Training set images: train-images-idx3-ubyte.gz (9.9 MB, 解壓後 47 MB, 包含 60,000 個樣本)<br /></li>
<li>Training set labels: train-labels-idx1-ubyte.gz (29 KB, 解壓後 60 KB, 包含 60,000 個標籤)<br /></li>
<li>Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 解壓後 7.8 MB, 包含 10,000 個樣本)<br /></li>
<li>Test set labels: t10k-labels-idx1-ubyte.gz (5KB, 解壓後 10 KB, 包含 10,000 個標籤)<br /></li>
</ol>

<p>
MNIST 資料集是一個適合拿來當作 TensotFlow 的練習素材，在 Tensorflow 的現有套件中，也已經有內建好的 MNIST 資料集，我們只要在安裝好 TensorFlow 的 Python 環境中執行以下程式碼，即可將 MNIST 資料成功讀取進來。.<br />
</p>
<div class="org-src-container">
<pre class="src src-python">  <span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
  <span style="color: #dcaeea;">mnist</span> = tf.keras.datasets.mnist
<span id="coderef-get-keras-mnist" class="coderef-off">  (x_train, y_train), (x_test, y_test) = mnist.load_data()</span>
</pre>
</div>
<p>
在訓練模型之前，需要將樣本資料劃分為訓練集、測試集，有些情況下還會劃分為訓練集、測試集、驗證集。由上述程式第<a href="#coderef-get-keras-mnist" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-get-keras-mnist');" onmouseout="CodeHighlightOff(this, 'coderef-get-keras-mnist');">3</a>行可知，下載後的 MNIST 資料分成訓練資料(training data)與測試資料(testing data)，其中 x 為圖片、y為所對應數字。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr"> 2: </span>  <span style="color: #dcaeea;">mnist</span> = tf.keras.datasets.mnist
<span class="linenr"> 3: </span>  (x_train, y_train), (x_test, y_test) = mnist.load_data()
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=====================================</span>
<span class="linenr"> 5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21028;&#26039;&#36039;&#26009;&#24418;&#29376;</span>
<span class="linenr"> 6: </span>  <span style="color: #51afef;">print</span>(x_train.shape)
<span class="linenr"> 7: </span>  <span style="color: #51afef;">print</span>(x_test.shape)
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31532;&#19968;&#20491;label&#30340;&#20839;&#23481;</span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">print</span>(y_train[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">10: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#24433;&#20687;&#20839;&#23481;</span>
<span class="linenr">11: </span>  <span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr">12: </span>  <span style="color: #dcaeea;">img</span> = x_train[<span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr">13: </span>  plt.imshow(img)
<span class="linenr">14: </span>  plt.savefig(<span style="color: #98be65;">"MNIST-Image.png"</span>)
</pre>
</div>
<pre class="example">
(60000, 28, 28)
(10000, 28, 28)
5
</pre>


<p>
由上述程式輸出結果可以看到載入的 x 為大小為 28*28 的圖片共 60000 張，每一筆 MNIST 資料的照片(x)由 784 個 pixels 組成（28*28），照片內容如圖<a href="#org975c5c5">25</a>，訓練集的標籤(y)則為其對應的數字(0～9)，此例為 5。<br />
</p>

<div id="org975c5c5" class="figure">
<p><img src="images/MNIST-Image.png" alt="MNIST-Image.png" /><br />
</p>
<p><span class="figure-number">Figure 25: </span>MNIST 影像示例</p>
</div>

<p>
x 的影像資料為灰階影像，每個像素的數值介於 0~255 之間，矩陣裡每一項的資料則是代表每個 pixel 顏色深淺的數值，如下圖<a href="#org90d5257">26</a>所示：<br />
</p>

<div id="org90d5257" class="figure">
<p><img src="images/MNIST-Matrix.png" alt="MNIST-Matrix.png" /><br />
</p>
<p><span class="figure-number">Figure 26: </span>MNIST 資料矩陣</p>
</div>

<p>
載入的 y 為所對應的數字 0~9，在這我們要運用 keras 中的 np_utils.to_categorical 將 y 轉成 one-hot 的形式，將他轉為一個 10 維的 vector，例如：我們所拿到的資料為 y=3，經過 np_utils.to_categorical，會轉換為 y=[0,0,0,1,0,0,0,0,0,0]。這部份的轉換程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">mnist</span> = tf.keras.datasets.mnist
<span class="linenr"> 6: </span>  (x_train, y_train), (x_test, y_test) = mnist.load_data()
<span class="linenr"> 7: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=====================================</span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#22294;&#29255;&#36681;&#25563;&#28858;&#19968;&#20491;60000*784&#30340;&#21521;&#37327;&#65292;&#20006;&#19988;&#27161;&#28310;&#21270;</span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">x_train</span> = x_train.reshape(x_train.shape[<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>)
<span class="linenr">10: </span>  <span style="color: #dcaeea;">x_test</span> = x_test.reshape(x_test.shape[<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>)
<span class="linenr">11: </span>  <span style="color: #dcaeea;">x_train</span> = x_train.astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">12: </span>  <span style="color: #dcaeea;">x_test</span> = x_test.astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">13: </span>  <span style="color: #dcaeea;">x_train</span> = x_train/<span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">14: </span>  <span style="color: #dcaeea;">x_test</span> = x_test/<span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;y&#36681;&#25563;&#25104;one-hot encoding</span>
<span class="linenr">16: </span>  <span style="color: #dcaeea;">y_train</span> = np_utils.to_categorical(y_train, <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">17: </span>  <span style="color: #dcaeea;">y_test</span> = np_utils.to_categorical(y_test, <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">18: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22238;&#20659;&#34389;&#29702;&#23436;&#30340;&#36039;&#26009;</span>
<span class="linenr">19: </span>  <span style="color: #51afef;">print</span>(y_train[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">20: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">21: </span>  np.set_printoptions(precision=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">22: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">print(x_train[0])</span>
</pre>
</div>

<pre class="example">
[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
</pre>
</div>
</li>

<li><a id="org776489c"></a>MNIST 的推論處理<br />
<div class="outline-text-4" id="text-5-1-2">
<p>
如圖<a href="#org02124c3">27</a>所示，MNIST 的推論神經網路最前端的輸入層有 784 (\(28*28=784\))個神經元，最後的輸出端有 10 個神經元(\(0~9\)個數字)，至於中間的隠藏層有兩個，第 1 個隱藏層有 50 個神經元，第 2 層有 100 個。此處的 50、100 可以設定為任意數（如，也可以是 128、64）。<br />
</p>

<div id="org02124c3" class="figure">
<p><img src="images/MNIST-CNN.png" alt="MNIST-CNN.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 27: </span>MNIST-NeuralNet</p>
</div>

<p>
為了完成上述推論，此處定義三個函數：get_data()、init_network()、predict()，其中 init_work()直接讀入作者已經訓練好的網絡權重。在以下這段程式碼中，權重與偏權值的參數會儲存成字典型態的變數。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.datasets.mnist <span style="color: #51afef;">import</span> load_data
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> pickle
<span class="linenr"> 4: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">sigmoid</span>(x):
<span class="linenr"> 5: </span>    <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span> / (<span style="color: #da8548; font-weight: bold;">1</span> + np.exp(-x))
<span class="linenr"> 6: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38450;&#27490;&#28322;&#20986;&#22411;</span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">softmax</span>(x):
<span class="linenr"> 8: </span>    <span style="color: #dcaeea;">c</span> = np.<span style="color: #c678dd;">max</span>(x)
<span class="linenr"> 9: </span>    <span style="color: #dcaeea;">exp_x</span> = np.exp(x - c)
<span class="linenr">10: </span>    <span style="color: #dcaeea;">sum_exp_x</span> = np.<span style="color: #c678dd;">sum</span>(exp_x)
<span class="linenr">11: </span>    <span style="color: #51afef;">return</span> exp_x / sum_exp_x
<span class="linenr">12: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">get_data</span>():
<span class="linenr">13: </span>    (X_train, y_train), (X_test, y_test) = load_data()
<span class="linenr">14: </span>    <span style="color: #51afef;">return</span> X_test.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>), y_test
<span class="linenr">15: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">init_network</span>():
<span class="linenr">16: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">https://github.com/Bingyy/deep-learning-from-scratch/blob/master/ch03/sample_weight.pkl</span>
<span class="linenr">17: </span>    <span style="color: #51afef;">with</span> <span style="color: #c678dd;">open</span>(<span style="color: #98be65;">'/Volumes/Vanessa/MNIST/data/mnist/sample_weight.pkl'</span>, <span style="color: #98be65;">'rb'</span>) <span style="color: #51afef;">as</span> <span style="color: #dcaeea;">f</span>:
<span class="linenr">18: </span>      network = pickle.load(f)
<span class="linenr">19: </span>      <span style="color: #51afef;">return</span> network
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23384;&#20786;&#30340;&#26159;&#32178;&#32097;&#21443;&#25976;&#23383;&#20856;</span>
<span class="linenr">21: </span>  <span style="color: #dcaeea;">network</span> = init_network()
<span class="linenr">22: </span>
<span class="linenr">23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32068;&#21512;&#32178;&#32097;&#27969;&#31243;&#65292;&#29992;&#26044;&#38928;&#28204;</span>
<span id="coderef-MNIST-predict" class="coderef-off"><span class="linenr">24: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">predict</span>(network, x):</span>
<span class="linenr">25: </span>    <span style="color: #dcaeea;">W1</span>, <span style="color: #dcaeea;">W2</span>, <span style="color: #dcaeea;">W3</span> = network[<span style="color: #98be65;">'W1'</span>], network[<span style="color: #98be65;">'W2'</span>], network[<span style="color: #98be65;">'W3'</span>]
<span class="linenr">26: </span>    <span style="color: #dcaeea;">b1</span>, <span style="color: #dcaeea;">b2</span>, <span style="color: #dcaeea;">b3</span> = network[<span style="color: #98be65;">'b1'</span>], network[<span style="color: #98be65;">'b2'</span>], network[<span style="color: #98be65;">'b3'</span>]
<span class="linenr">27: </span>    <span style="color: #dcaeea;">a1</span> = np.dot(x,W1) + b1
<span class="linenr">28: </span>    <span style="color: #dcaeea;">z1</span> = sigmoid(a1)
<span class="linenr">29: </span>    <span style="color: #dcaeea;">a2</span> = np.dot(z1, W2) + b2
<span class="linenr">30: </span>    <span style="color: #dcaeea;">z2</span> = sigmoid(a2)
<span class="linenr">31: </span>    <span style="color: #dcaeea;">a3</span> = np.dot(z2, W3) + b3
<span class="linenr">32: </span>    <span style="color: #dcaeea;">y</span> = softmax(a3) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20998;&#39006;&#29992;&#30340;&#26368;&#24460;&#36664;&#20986;&#23652;&#30340;&#28608;&#27963;&#20989;&#25976;</span>
<span class="linenr">33: </span>    <span style="color: #51afef;">return</span> y
<span class="linenr">34: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;&#32178;&#32097;&#38928;&#28204;</span>
<span class="linenr">35: </span>  <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_test</span> = get_data() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24471;&#21040;&#28204;&#35430;&#25976;&#25818;</span>
<span class="linenr">36: </span>  <span style="color: #dcaeea;">network</span> = init_network()
<span class="linenr">37: </span>
<span class="linenr">38: </span>  <span style="color: #dcaeea;">accuracy_cnt</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">39: </span>  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(X_test)):
<span id="coderef-y-predict" class="coderef-off"><span class="linenr">40: </span>    <span style="color: #dcaeea;">y</span> = predict(network, X_test[i])</span>
<span id="coderef-np-argmax" class="coderef-off"><span class="linenr">41: </span>    <span style="color: #dcaeea;">p</span> = np.argmax(y)</span>
<span class="linenr">42: </span>    np.set_printoptions(precision=<span style="color: #da8548; font-weight: bold;">4</span>, suppress=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">43: </span>    <span style="color: #51afef;">if</span> p == <span style="color: #dcaeea;">y_test</span>[i]:
<span class="linenr">44: </span>      accuracy_cnt += <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">45: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#28310;&#30906;&#29575;&#65306;'</span>, <span style="color: #c678dd;">str</span>(<span style="color: #c678dd;">float</span>(accuracy_cnt) / <span style="color: #c678dd;">len</span>(X_test)))
</pre>
</div>

<pre class="example">
準確率： 0.0002
</pre>


<p>
上述程式中，predict 程序(第<a href="#coderef-MNIST-predict" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-MNIST-predict');" onmouseout="CodeHighlightOff(this, 'coderef-MNIST-predict');">24</a>)透過矩陣相乘運算完成神經網路的參數傳遞，最後必須進行準確率的評估，程式碼第<a href="#coderef-y-predict" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-y-predict');" onmouseout="CodeHighlightOff(this, 'coderef-y-predict');">40</a>行為神經網路針對輸入圖片的預測結果，所傳回的值為各猜測值的機率陣列，如：[0.0004 0.0011 0.9859 0.0065 0.     0.0007 0.0051 0.     0.0003 0.    ]；而程式碼第<a href="#coderef-np-argmax" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-np-argmax');" onmouseout="CodeHighlightOff(this, 'coderef-np-argmax');">41</a>則是該圖片的應對標籤，np.argmax(y)會傳回 y 的最大值所在順序，若 y=[0,0,0,1,0,0,0,0,0,0]，則傳回 3，藉此計算預測正確的百分比。<br />
</p>
</div>
</li>

<li><a id="orgb8c3256"></a>Python 與神經網路運算的批次處理<br />
<div class="outline-text-4" id="text-5-1-3">
<p>
前節程式碼中最後以 for 迴圈來逐一處理預測結果與比較，輸入(X)為單一圖片，其處理程序如圖<a href="#org4ffa17b">28</a>所示：<br />
</p>

<div id="org4ffa17b" class="figure">
<p><img src="images/MNIST-single.png" alt="MNIST-single.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 28: </span>MNIST-單一處理架構</p>
</div>

<p>
事實上，在使用批次處理（如一次處理 100 張圖）反而能大幅縮短每張圖片的處理時間，因為多數處理數值運算的函式庫都會針對大型陣列運算進行最佳化，尤其是透過 GPU 來處理時更是如此，這時，傳送單張圖片反而成為效能瓶頸，以批次處理則可減輕匯流排頻寛負擔。若以每次處理 100 張為例，其處理程序則如圖<a href="#orgfdb56b7">29</a>所示。<br />
</p>

<div id="orgfdb56b7" class="figure">
<p><img src="images/MNIST-batch.png" alt="MNIST-batch.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 29: </span>MNIST-批次處理架構</p>
</div>

<p>
至於批次運算的程式碼如下。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.datasets.mnist <span style="color: #51afef;">import</span> load_data
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> pickle
<span class="linenr"> 4: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">sigmoid</span>(x):
<span class="linenr"> 5: </span>    <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span> / (<span style="color: #da8548; font-weight: bold;">1</span> + np.exp(-x))
<span class="linenr"> 6: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38450;&#27490;&#28322;&#20986;&#22411;</span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">softmax</span>(x):
<span class="linenr"> 8: </span>    <span style="color: #dcaeea;">c</span> = np.<span style="color: #c678dd;">max</span>(x)
<span class="linenr"> 9: </span>    <span style="color: #dcaeea;">exp_x</span> = np.exp(x - c)
<span class="linenr">10: </span>    <span style="color: #dcaeea;">sum_exp_x</span> = np.<span style="color: #c678dd;">sum</span>(exp_x)
<span class="linenr">11: </span>    <span style="color: #51afef;">return</span> exp_x / sum_exp_x
<span class="linenr">12: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">get_data</span>():
<span class="linenr">13: </span>    (X_train, y_train), (X_test, y_test) = load_data()
<span class="linenr">14: </span>    <span style="color: #51afef;">return</span> X_test.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>), y_test
<span class="linenr">15: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">init_network</span>():
<span class="linenr">16: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">https://github.com/Bingyy/deep-learning-from-scratch/blob/master/ch03/sample_weight.pkl</span>
<span class="linenr">17: </span>    <span style="color: #51afef;">with</span> <span style="color: #c678dd;">open</span>(<span style="color: #98be65;">'/Volumes/Vanessa/MNIST/sample_weight.pkl'</span>, <span style="color: #98be65;">'rb'</span>) <span style="color: #51afef;">as</span> <span style="color: #dcaeea;">f</span>:
<span class="linenr">18: </span>      network = pickle.load(f)
<span class="linenr">19: </span>      <span style="color: #51afef;">return</span> network
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23384;&#20786;&#30340;&#26159;&#32178;&#32097;&#21443;&#25976;&#23383;&#20856;</span>
<span class="linenr">21: </span>  <span style="color: #dcaeea;">network</span> = init_network()
<span class="linenr">22: </span>
<span class="linenr">23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32068;&#21512;&#32178;&#32097;&#27969;&#31243;&#65292;&#29992;&#26044;&#38928;&#28204;</span>
<span class="linenr">24: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">predict</span>(network, x):
<span class="linenr">25: </span>    <span style="color: #dcaeea;">W1</span>, <span style="color: #dcaeea;">W2</span>, <span style="color: #dcaeea;">W3</span> = network[<span style="color: #98be65;">'W1'</span>], network[<span style="color: #98be65;">'W2'</span>], network[<span style="color: #98be65;">'W3'</span>]
<span class="linenr">26: </span>    <span style="color: #dcaeea;">b1</span>, <span style="color: #dcaeea;">b2</span>, <span style="color: #dcaeea;">b3</span> = network[<span style="color: #98be65;">'b1'</span>], network[<span style="color: #98be65;">'b2'</span>], network[<span style="color: #98be65;">'b3'</span>]
<span class="linenr">27: </span>    <span style="color: #dcaeea;">a1</span> = np.dot(x,W1) + b1
<span class="linenr">28: </span>    <span style="color: #dcaeea;">z1</span> = sigmoid(a1)
<span class="linenr">29: </span>    <span style="color: #dcaeea;">a2</span> = np.dot(z1, W2) + b2
<span class="linenr">30: </span>    <span style="color: #dcaeea;">z2</span> = sigmoid(a2)
<span class="linenr">31: </span>    <span style="color: #dcaeea;">a3</span> = np.dot(z2, W3) + b3
<span class="linenr">32: </span>    <span style="color: #dcaeea;">y</span> = softmax(a3) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20998;&#39006;&#29992;&#30340;&#26368;&#24460;&#36664;&#20986;&#23652;&#30340;&#28608;&#27963;&#20989;&#25976;</span>
<span class="linenr">33: </span>    <span style="color: #51afef;">return</span> y
<span class="linenr">34: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;&#32178;&#32097;&#38928;&#28204;</span>
<span class="linenr">35: </span>  <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_test</span> = get_data() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24471;&#21040;&#28204;&#35430;&#25976;&#25818;</span>
<span class="linenr">36: </span>  <span style="color: #dcaeea;">network</span> = init_network()
<span class="linenr">37: </span>
<span class="linenr">38: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25209;&#27425;&#34389;&#29702;&#26550;&#27083;</span>
<span class="linenr">39: </span>  <span style="color: #dcaeea;">batch_size</span> = <span style="color: #da8548; font-weight: bold;">100</span>
<span class="linenr">40: </span>  <span style="color: #dcaeea;">accuracy_cnt</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">41: </span>  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #c678dd;">len</span>(X_test), batch_size):
<span id="coderef-b-mnist-x" class="coderef-off"><span class="linenr">42: </span>    <span style="color: #dcaeea;">x_batch</span> = X_test[i:i+batch_size]</span>
<span class="linenr">43: </span>    <span style="color: #dcaeea;">y_batch</span> = predict(network, x_batch)
<span id="coderef-b-mnist-p" class="coderef-off"><span class="linenr">44: </span>    <span style="color: #dcaeea;">p</span> = np.argmax(y_batch, axis=<span style="color: #da8548; font-weight: bold;">1</span>)</span>
<span class="linenr">45: </span>    <span style="color: #dcaeea;">accuracy_cnt</span> += np.<span style="color: #c678dd;">sum</span>(p == y_test[i:i+batch_size])
<span class="linenr">46: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#28310;&#30906;&#29575;&#65306;'</span>, <span style="color: #c678dd;">str</span>(<span style="color: #c678dd;">float</span>(accuracy_cnt) / <span style="color: #c678dd;">len</span>(X_test)))
</pre>
</div>

<pre class="example">
準確率： 0.9207
</pre>


<p>
上述程式中，第<a href="#coderef-b-mnist-x" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-b-mnist-x');" onmouseout="CodeHighlightOff(this, 'coderef-b-mnist-x');">42</a>行每次取出 100 張圖形檔(X 陣列),第<a href="#coderef-b-mnist-p" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-b-mnist-p');" onmouseout="CodeHighlightOff(this, 'coderef-b-mnist-p');">44</a>行則取得這 100 筆資料中各筆資料最大值索引值，若以每次 4 筆資料為例，所得的估計值 p 可能為[7 2 1 0]，相對應的正確標籤值則儲存於 y_test[0:4]中，以此進行準確率的計算。<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org792d800" class="outline-3">
<h3 id="org792d800"><span class="section-number-3">5.2</span> MNIST 資料集:以 DNN Sequential 模型為例</h3>
<div class="outline-text-3" id="text-5-2">
<p>
此處以最簡單的 DNN (deep neural network) 作為範例。以 Keras 的核心為模型，應用最常使用 Sequential 模型。藉由.add()我們可以一層一層的將神經網路疊起。在每一層之中我們只需要簡單的設定每層的大小(units)與激活函數(activation function)。需要特別記得的是：第一層要記得寫輸入的向量大小、最後一層的 units 要等於輸出的向量大小。在這邊我們最後一層使用的激活函數(activation function)為 softmax。<br />
相對應程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36617;&#20837;&#36039;&#26009;</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">load_data</span>():
<span class="linenr"> 6: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36617;&#20837;minst&#30340;&#36039;&#26009;</span>
<span class="linenr"> 7: </span>    (x_train, y_train), (x_test, y_test) = mnist.load_data()
<span class="linenr"> 8: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#22294;&#29255;&#36681;&#25563;&#28858;&#19968;&#20491;60000*784&#30340;&#21521;&#37327;&#65292;&#20006;&#19988;&#27161;&#28310;&#21270;</span>
<span class="linenr"> 9: </span>    <span style="color: #dcaeea;">x_train</span> = x_train.reshape(x_train.shape[<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>)
<span class="linenr">10: </span>    <span style="color: #dcaeea;">x_test</span> = x_test.reshape(x_test.shape[<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>)
<span class="linenr">11: </span>    <span style="color: #dcaeea;">x_train</span> = x_train.astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">12: </span>    <span style="color: #dcaeea;">x_test</span> = x_test.astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">13: </span>    <span style="color: #dcaeea;">x_train</span> = x_train/<span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">14: </span>    <span style="color: #dcaeea;">x_test</span> = x_test/<span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">15: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;y&#36681;&#25563;&#25104;one-hot encoding</span>
<span class="linenr">16: </span>    <span style="color: #dcaeea;">y_train</span> = np_utils.to_categorical(y_train, <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">17: </span>    <span style="color: #dcaeea;">y_test</span> = np_utils.to_categorical(y_test, <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">18: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22238;&#20659;&#34389;&#29702;&#23436;&#30340;&#36039;&#26009;</span>
<span class="linenr">19: </span>    <span style="color: #51afef;">return</span> (x_train, y_train), (x_test, y_test)
<span class="linenr">20: </span>
<span class="linenr">21: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">22: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">23: </span>  <span style="color: #51afef;">from</span> keras.layers.core <span style="color: #51afef;">import</span> Dense,Activation
<span class="linenr">24: </span>  <span style="color: #51afef;">from</span> keras.optimizers <span style="color: #51afef;">import</span>  Adam
<span class="linenr">25: </span>
<span class="linenr">26: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">build_model</span>():<span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#24314;&#31435;&#27169;&#22411;</span>
<span class="linenr">27: </span>    <span style="color: #dcaeea;">model</span> = Sequential()
<span class="linenr">28: </span>    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23559;&#27169;&#22411;&#30090;&#36215;</span>
<span class="linenr">29: </span>    model.add(Dense(input_dim=<span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>,units=<span style="color: #da8548; font-weight: bold;">500</span>,activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">30: </span>    model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">500</span>,activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">31: </span>    model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">500</span>,activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">32: </span>    model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>,activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">33: </span>    model.summary()
<span class="linenr">34: </span>    <span style="color: #51afef;">return</span> model
<span class="linenr">35: </span>
<span class="linenr">36: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38283;&#22987;&#35347;&#32244;&#27169;&#22411;&#65292;&#27492;&#34389;&#20351;&#29992;&#20102;Adam&#20570;&#28858;&#25105;&#20497;&#30340;&#20778;&#21270;&#22120;&#65292;loss function&#36984;&#29992;&#20102;categorical_crossentropy&#12290;</span>
<span class="linenr">37: </span>  (x_train,y_train),(x_test,y_test)=load_data()
<span class="linenr">38: </span>  <span style="color: #dcaeea;">model</span> = build_model()
<span class="linenr">39: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38283;&#22987;&#35347;&#32244;&#27169;&#22411;</span>
<span class="linenr">40: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,optimizer=<span style="color: #98be65;">"adam"</span>,metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">41: </span>  model.fit(x_train,y_train,batch_size=<span style="color: #da8548; font-weight: bold;">100</span>,epochs=<span style="color: #da8548; font-weight: bold;">20</span>)
<span class="linenr">42: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#39023;&#31034;&#35347;&#32244;&#32080;&#26524;</span>
<span class="linenr">43: </span>  <span style="color: #dcaeea;">score</span> = model.evaluate(x_train,y_train)
<span class="linenr">44: </span>  <span style="color: #51afef;">print</span> (<span style="color: #98be65;">'\nTrain Acc:'</span>, score[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">45: </span>  score = model.evaluate(x_test,y_test)
<span class="linenr">46: </span>  <span style="color: #51afef;">print</span> (<span style="color: #98be65;">'\nTest Acc:'</span>, score[<span style="color: #da8548; font-weight: bold;">1</span>])
</pre>
</div>

<pre class="example">
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_1 (Dense)              (None, 500)               392500
_________________________________________________________________
dense_2 (Dense)              (None, 500)               250500
_________________________________________________________________
dense_3 (Dense)              (None, 500)               250500
_________________________________________________________________
dense_4 (Dense)              (None, 10)                5010
=================================================================
Total params: 898,510
Trainable params: 898,510
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20

  100/60000 [..............................] - ETA: 2:55 - loss: 2.2917 - acc: 0.1300
  800/60000 [..............................] - ETA: 25s - loss: 1.6424 - ACM: 0.5362
.......
16300/60000 [=======&gt;......................] - ETA: 4s - loss: 0.3752 - acc: 0.8898
17000/60000 [=======&gt;......................] - ETA: 4s - loss: 0.3681 - acc: 0.8916
.......
50600/60000 [========================&gt;.....] - ETA: 0s - loss: 0.2232 - acc: 0.9335
51300/60000 [========================&gt;.....] - ETA: 0s - loss: 0.2220 - acc: 0.9338
.......
59700/60000 [============================&gt;.] - ETA: 0s - loss: 0.2078 - acc: 0.9377
60000/60000 [==============================] - 5s 81us/step - loss: 0.2074 - acc: 0.9379
Epoch 2/20

  100/60000 [..............................] - ETA: 5s - loss: 0.0702 - acc: 0.9800
......
60000/60000 [==============================] - 5s 77us/step - loss: 0.0832 - acc: 0.9740
Epoch 3/20
......
Epoch 29/20

   32/60000 [..............................] - ETA: 1:10
 1440/60000 [..............................] - ETA: 3s
......
58496/60000 [============================&gt;.] - ETA: 0s
60000/60000 [==============================] - 2s 34us/step

Train Acc: 0.9981666666666666

   32/10000 [..............................] - ETA: 0s
 1568/10000 [===&gt;..........................] - ETA: 0s
 3104/10000 [========&gt;.....................] - ETA: 0s
 4640/10000 [============&gt;.................] - ETA: 0s
 6176/10000 [=================&gt;............] - ETA: 0s
 7680/10000 [======================&gt;.......] - ETA: 0s
 9184/10000 [==========================&gt;...] - ETA: 0s
10000/10000 [==============================] - 0s 33us/step

Test Acc: 0.9823
</pre>
</div>
</div>
</div>



<div id="outline-container-orga320739" class="outline-2">
<h2 id="orga320739"><span class="section-number-2">6</span> 深度神經網路 DNN (Deep Neural Network)</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-org7459220" class="outline-3">
<h3 id="org7459220"><span class="section-number-3">6.1</span> 學習與參數:以迴歸問題為例</h3>
<div class="outline-text-3" id="text-6-1">
<p>
想像一個國中的數學問題：在平面上畫出\(y=2x+3\)的直線，如圖<a href="#org418f05d">30</a>左的直線(\(y=ax+b\))，決定這條直線的因素有二：斜率\(a\)與截距\(b\)，這兩項因素即可視為該直線的參數，像這種由已知參數去畫出對映直線的問題稱之為順向問題；反之，如果目前只知道平面上有幾個點，希望能畫出最符合的這些點的線，這種問題就稱為逆向問題。像圖<a href="#org418f05d">30</a>右圖的紅線明顯就不是一條最符合的線，而解決這個問題就變成透過「尋找最佳參數」來畫出最理想的迴歸線，神經網路便是希望能藉由網路模型的不斷學習來找出最佳參數。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr"> 3: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">f</span>(x):
<span class="linenr"> 4: </span>      <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">3</span>*x+<span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">x</span> = np.arange(-<span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>)
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">y</span> = f(x)
<span class="linenr"> 7: </span>  plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">10</span>, <span style="color: #da8548; font-weight: bold;">5</span>))
<span class="linenr"> 8: </span>  plt.subplot(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">1</span>) <span style="color: #5B6268;">## </span><span style="color: #5B6268;">&#24038;&#22294;</span>
<span class="linenr"> 9: </span>  plt.plot(x, y)
<span class="linenr">10: </span>  plt.ylim(-<span style="color: #da8548; font-weight: bold;">30</span>, <span style="color: #da8548; font-weight: bold;">30</span>)
<span class="linenr">11: </span>  plt.axvline(<span style="color: #da8548; font-weight: bold;">0</span>, color=<span style="color: #98be65;">'k'</span>, linestyle=<span style="color: #98be65;">'--'</span>)
<span class="linenr">12: </span>  plt.axhline(<span style="color: #da8548; font-weight: bold;">0</span>, color=<span style="color: #98be65;">'k'</span>, linestyle=<span style="color: #98be65;">'--'</span>)
<span class="linenr">13: </span>  plt.xlabel(<span style="color: #98be65;">'x'</span>)
<span class="linenr">14: </span>  plt.ylabel(<span style="color: #98be65;">'y'</span>)
<span class="linenr">15: </span>  plt.subplot(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>) <span style="color: #5B6268;">## </span><span style="color: #5B6268;">&#21491;&#22294;</span>
<span class="linenr">16: </span>  plt.axvline(<span style="color: #da8548; font-weight: bold;">0</span>, color=<span style="color: #98be65;">'k'</span>, linestyle=<span style="color: #98be65;">'--'</span>)
<span class="linenr">17: </span>  plt.axhline(<span style="color: #da8548; font-weight: bold;">0</span>, color=<span style="color: #98be65;">'k'</span>, linestyle=<span style="color: #98be65;">'--'</span>)
<span class="linenr">18: </span>  plt.ylim(-<span style="color: #da8548; font-weight: bold;">30</span>, <span style="color: #da8548; font-weight: bold;">30</span>)
<span class="linenr">19: </span>  plt.plot(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">8</span>, <span style="color: #98be65;">"o"</span>)
<span class="linenr">20: </span>  plt.plot(-<span style="color: #da8548; font-weight: bold;">2</span>, -<span style="color: #da8548; font-weight: bold;">7</span>, <span style="color: #98be65;">"o"</span>)
<span class="linenr">21: </span>  plt.plot(<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">25</span>, <span style="color: #98be65;">"o"</span>)
<span class="linenr">22: </span>  plt.plot(-<span style="color: #da8548; font-weight: bold;">4</span>, -<span style="color: #da8548; font-weight: bold;">24</span>, <span style="color: #98be65;">"o"</span>)
<span class="linenr">23: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">f1</span>(x):
<span class="linenr">24: </span>      <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">12</span>*x+<span style="color: #da8548; font-weight: bold;">29</span>
<span class="linenr">25: </span>  <span style="color: #dcaeea;">y1</span> = f1(x)
<span class="linenr">26: </span>  plt.plot(x, y1, color=<span style="color: #98be65;">'r'</span>)
<span class="linenr">27: </span>  plt.xlabel(<span style="color: #98be65;">'x'</span>)
<span class="linenr">28: </span>  plt.ylabel(<span style="color: #98be65;">'y'</span>)
<span class="linenr">29: </span>  plt.tight_layout()
<span class="linenr">30: </span>  plt.savefig(<span style="color: #98be65;">"simplefx-1.png"</span>)
<span class="linenr">31: </span>  <span style="color: #51afef;">return</span> <span style="color: #98be65;">"simplefx-1.png"</span>
</pre>
</div>

<div id="org418f05d" class="figure">
<p><img src="images/simplefx-1.png" alt="simplefx-1.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 30: </span>由已知函數畫出直線與由已知點找未知函數</p>
</div>

<p>
同理，如果我們將解題目標改變為「預測學生學測總級分」，那麼，我們得先了解有那些因素會影響學生的學測成績，初步估計也許包括以下因素：<br />
</p>
<ol class="org-ol">
<li>上課狀況<br /></li>
<li>是否認真寫作業<br /></li>
<li>歷次段考成績<br /></li>
<li>校內模考成績<br /></li>
<li>回家後是否努力讀書<br /></li>
<li>是否沉迷網路遊戲或手機遊戲<br /></li>
<li>是否有男/女朋友<br /></li>
</ol>

<p>
此時，我們的預測模型就如圖<a href="#orge14ebd6">31</a>所示<br />
</p>
<div class="org-src-container">
<pre class="src src-dot">  digraph exam{
    rankdir=LR;
    nodesep=.05;
    graph[ranksep=<span style="color: #98be65;">"3"</span>]
    &#19978;&#35506;&#29376;&#27841;        -&gt; &#23416;&#28204;&#25104;&#32318;
    &#26159;&#21542;&#35469;&#30495;&#23531;&#20316;&#26989;       -&gt; &#23416;&#28204;&#25104;&#32318;
    &#27511;&#27425;&#27573;&#32771;&#25104;&#32318;        -&gt; &#23416;&#28204;&#25104;&#32318;
    &#26657;&#20839;&#27169;&#32771;&#25104;&#32318;        -&gt; &#23416;&#28204;&#25104;&#32318;
    &#25918;&#23416;&#24460;&#26159;&#21542;&#21162;&#21147;&#35712;&#26360;     -&gt; &#23416;&#28204;&#25104;&#32318;
    &#26159;&#21542;&#27785;&#36855;&#32178;&#36335;&#36938;&#25138;&#25110;&#25163;&#27231;&#36938;&#25138; -&gt; &#23416;&#28204;&#25104;&#32318;
    &#26159;&#21542;&#33457;&#22826;&#22810;&#26178;&#38291;&#20132;&#30064;&#24615;&#26379;&#21451;  -&gt; &#23416;&#28204;&#25104;&#32318;
  }
</pre>
</div>

<div id="orge14ebd6" class="figure">
<p><img src="images/exam-Network1.png" alt="exam-Network1.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 31: </span>學測成績預測模型#1</p>
</div>

<p>
然而，上述因素只是一般性的文字描述，畢竟過於模糊而無法對之進行精確計算，所以，我們有必要再對其進行更精確的描述，此處的參數（即影響因素及相對權重）又稱為特徵值。此外，每個因素影響學測結果的程度理應會有所差異，因此也有必要對各因素賦予「加權」（也稱為權重），詳細考慮後的因素及加權列表如下。<br />
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> 學測預測模型因素列表</caption>

<colgroup>
<col  class="org-right" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">no</th>
<th scope="col" class="org-left">因素編號</th>
<th scope="col" class="org-left">模糊描述</th>
<th scope="col" class="org-left">精確描述</th>
<th scope="col" class="org-left">權重</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-left">\(x_1\)</td>
<td class="org-left">上課狀況</td>
<td class="org-left">平均每次上課時認真聽講的時間百分比</td>
<td class="org-left">\(w_1\)</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-left">\(x_2\)</td>
<td class="org-left">是否認真寫作業</td>
<td class="org-left">作業平均成績</td>
<td class="org-left">\(w_2\)</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-left">\(x_3\)</td>
<td class="org-left">歷次段考成績</td>
<td class="org-left">各科段考平均成績</td>
<td class="org-left">\(w_3\)</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-left">\(x_4\)</td>
<td class="org-left">校內模考成績</td>
<td class="org-left">歷次模考平均成績</td>
<td class="org-left">\(w_4\)</td>
</tr>

<tr>
<td class="org-right">5</td>
<td class="org-left">\(x_5\)</td>
<td class="org-left">放學後是否努力讀書</td>
<td class="org-left">放學後花在課業上的時間</td>
<td class="org-left">\(w_5\)</td>
</tr>

<tr>
<td class="org-right">6</td>
<td class="org-left">\(x_6\)</td>
<td class="org-left">是否沉迷網路遊戲或手機遊戲</td>
<td class="org-left">每天平均花在遊戲的時間</td>
<td class="org-left">\(w_6\)</td>
</tr>

<tr>
<td class="org-right">7</td>
<td class="org-left">\(x_7\)</td>
<td class="org-left">是否花太多時間交異性朋友</td>
<td class="org-left">有/無男女朋友</td>
<td class="org-left">\(w_7\)</td>
</tr>
</tbody>
</table>

<p>
此時，我們的預測模型就如圖<a href="#org0010971">32</a>所示，換言之，是在解一個\(f(x)=x_1*w_1+x_2*w_2+x_3*w_3+...+x_7*w_7\)的函式問題。我們可以先針對這些特徵值對學生進行問卷調查，並追踪學生的學測成績，最後將取得的大量的特徵值輸入到到我們的函數模型（圖<a href="#org0010971">32</a>）中，觀察計算結果與實際資料的吻合程度，藉由不斷的調整參數（權重）來控制函數，讓輸出的計算結果與實際答案完全吻合，以便求得最準確的函數。<br />
</p>
<div class="org-src-container">
<pre class="src src-dot">  digraph exam2{
    rankdir=LR;
    nodesep=.05;
    graph[ranksep=<span style="color: #98be65;">"3"</span>]
    x1:&#19978;&#35506;&#35469;&#30495;&#32893;&#35611;&#26178;&#38291;&#30334;&#20998;&#27604; -&gt; &#23416;&#28204;&#25104;&#32318;[label = <span style="color: #98be65;">"w1"</span>];
    x2:&#20316;&#26989;&#24179;&#22343;&#25104;&#32318;           -&gt; &#23416;&#28204;&#25104;&#32318;[label = <span style="color: #98be65;">"w2"</span>];
    x3:&#21508;&#31185;&#27573;&#32771;&#24179;&#22343;&#25104;&#32318;       -&gt; &#23416;&#28204;&#25104;&#32318;[label = <span style="color: #98be65;">"w3"</span>];
    x4:&#27511;&#27425;&#27169;&#32771;&#24179;&#22343;&#25104;&#32318;       -&gt; &#23416;&#28204;&#25104;&#32318;[label = <span style="color: #98be65;">"w4"</span>];
    x5:&#25918;&#23416;&#24460;&#35712;&#26360;&#30340;&#26178;&#38291;       -&gt; &#23416;&#28204;&#25104;&#32318;[label = <span style="color: #98be65;">"w5"</span>];
    x6:&#27599;&#22825;&#33457;&#22312;&#36938;&#25138;&#30340;&#26178;&#38291;     -&gt; &#23416;&#28204;&#25104;&#32318;[label = <span style="color: #98be65;">"w6"</span>];
    x7:&#26377;&#28961;&#30007;&#22899;&#26379;&#21451;          -&gt; &#23416;&#28204;&#25104;&#32318;[label = <span style="color: #98be65;">"w7"</span>];
  }
</pre>
</div>

<div id="org0010971" class="figure">
<p><img src="./exam-network2.png" alt="exam-network2.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 32: </span>學測成績預測模型#2</p>
</div>

<p>
然而，無論我們事前研究分析調查的再如何嚴謹，實際的計算結果與真實分數總會存在誤差，如表<a href="#org10e404c">2</a>，分別觀察這些誤差值並不容易看出吻合程度，但如果將個別的誤差平方後加總，則可以得到一個明確的誤差函數=\(3^2+(-3)^2+(-2)^2+(-2)^2...\)，至此，解題的任務便轉為：找出能讓誤差函數最小化的一組參數。<br />
</p>
<table id="org10e404c" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 2:</span> 誤差的計算</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">學生 A</th>
<th scope="col" class="org-right">學生 B</th>
<th scope="col" class="org-right">學生 C</th>
<th scope="col" class="org-right">學生 D</th>
<th scope="col" class="org-left">&#x2026;</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">資料</td>
<td class="org-right">70</td>
<td class="org-right">65</td>
<td class="org-right">68</td>
<td class="org-right">50</td>
<td class="org-left">&#x2026;</td>
</tr>

<tr>
<td class="org-left">模型</td>
<td class="org-right">67</td>
<td class="org-right">68</td>
<td class="org-right">70</td>
<td class="org-right">52</td>
<td class="org-left">&#x2026;</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">誤差</td>
<td class="org-right">3</td>
<td class="org-right">-3</td>
<td class="org-right">-2</td>
<td class="org-right">-2</td>
<td class="org-left">&#x2026;</td>
</tr>
</tbody>
</table>

<p>
像這種透過比對現有資料不斷調整參數以便將誤差函數減至最小的學習過程稱為「監督式學習」，而至於「非監督式學習」則是藉由找出不同學生在這些特徵值上的相似程度，將學生分群，而同一群組的學生共通之處或許便會直接與學測成績相關，學童在成長過程中的學習就是非監督式學習，透過觀週遭的一切事物，也許有些事情仍不了解，但多少都會從中整理、歸納出一些重要的知識；待到後開始上學，則較接近監督式學習。<br />
</p>
</div>
</div>

<div id="outline-container-org00bc2c6" class="outline-3">
<h3 id="org00bc2c6"><span class="section-number-3">6.2</span> 如何調整參數</h3>
<div class="outline-text-3" id="text-6-2">
<p>
前節提及，為了能找到最理想的預測函數，我們可以不斷調整權重來把誤差函數降到最低。實務上，我們可以每次以最細微的調幅逐一調整（增加或減少）權重值來試圖減小損失函數，直到其無法再減少為止，此種方式稱為「坡度法」，而這種每次稍微調整一點點再觀察結果變化的手段稱為微分；若是同時微幅調整所有權重以將損失函數降到最低，這種方式則稱為「梯度下降法」。然而類似梯度法並不保證能找到將損失函數降到最小的權重組合，如圖<a href="#orgd2fe7de">33</a>所示，以梯度法可能只會找到 A 點這個局部最小值，然而全體的最小值其實發生在 B 點。<br />
</p>

<div id="orgd2fe7de" class="figure">
<p><img src="images/dotAB.png" alt="dotAB.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 33: </span>極小值與最小值的差異</p>
</div>
</div>
</div>

<div id="outline-container-org30a96aa" class="outline-3">
<h3 id="org30a96aa"><span class="section-number-3">6.3</span> 模型的極限</h3>
<div class="outline-text-3" id="text-6-3">
<p>
在我們透過問卷取得大量的數據後，想像一下我們以這些數據來做為調整模型參數的依據，最後，我們如何評估這個模型的效能呢？一般來說，我們會把數據分成兩部份：一部份做為調整或學習參數的依據，稱做訓練資料；另一部份用來測試或檢驗模型的效能。之所以用不同的數據進行訓練與測試，是為了避免「過擬合」的狀況，即，因為測試資料與訓練資料一致，導致測試結果十分完美，然而，一旦把模型拿來應用到新的數據上（或是實際應用模型到真實世界中）時，反而效果會不如預期。<br />
</p>

<p>
過擬合就好比學生在學習時只死記課本的習題，對於其他題型完全不予理會，如果考試也考課本的習題，考試成績自然優異，然而如果考試時題型略做變化，則考試結果就可能十分悲慘。<br />
</p>

<p>
實際進行測試時，可以將資料分成數組，將其中一組當成測試資料。例如，分為 A、B、C、D4 組，然後輪流拿這 4 組資料中的一組做為測試資料進行 4 次相同的測試，目的在於提升模型的「泛化能力」，也就是減少其過擬合的可能性。<br />
</p>
</div>
</div>

<div id="outline-container-orga697eaf" class="outline-3">
<h3 id="orga697eaf"><span class="section-number-3">6.4</span> 神經網路為什麼要有那麼多層</h3>
<div class="outline-text-3" id="text-6-4">
<p>
前節提及，我們的預測模型就如圖<a href="#org0010971">32</a>所示，換言之，是在解一個\(f(x)=x_1*w_1+x_2*w_2+x_3*w_3+...+x_7*w_7\)的函式問題，那麼，為了能找到最理想的預測函數，可否把函數變的更加複雜，例如，將函數變為二次函數或更複雜的函數以提升預測的精準度？實則，這種社會科學的問題並不如自然科學的物理現象可以用明確的公式來解決，神經網路採用的是以組合的方式來將函數複雜化，例如，把圖<a href="#org0010971">32</a>變為<a href="#org5c27a21">34</a>的樣式，如此藉由改變各因素以及權值的組合，等於建立了許多新的特徵值，也增加了模型的複雜度。<br />
</p>

<div class="org-src-container">
<pre class="src src-dot">digraph exam3 {
    rankdir=LR;
    nodesep=.05;
    graph[ranksep=<span style="color: #98be65;">"3"</span>]
    x4[label = <span style="color: #98be65;">"..."</span>];
    x1[label = <span style="color: #98be65;">"&#19978;&#35506;&#35469;&#30495;&#32893;&#35611;&#26178;&#38291;&#30334;&#20998;&#27604;"</span>];
    x2[label = <span style="color: #98be65;">"&#20316;&#26989;&#24179;&#22343;&#25104;&#32318;"</span>];
    x3[label = <span style="color: #98be65;">"&#21508;&#31185;&#27573;&#32771;&#24179;&#22343;&#25104;&#32318;"</span>];
    o1[label = <span style="color: #98be65;">"&#26032;&#30340;&#29305;&#24501;&#20540;&#20043;2"</span>];
    o2[label = <span style="color: #98be65;">"&#26032;&#30340;&#29305;&#24501;&#20540;&#20043;3"</span>];
    o4[label = <span style="color: #98be65;">"....."</span>];
    o3[label = <span style="color: #98be65;">"&#26032;&#30340;&#29305;&#24501;&#20540;&#20043;1"</span>];
    x1 -&gt; o3[label = w11]; x1-&gt; o1[label = w12]; x1 -&gt; o2[label = w13]; x1 -&gt; o4[label = w14];
    x2 -&gt; o3[label = w21]; x2-&gt; o1[label = w22]; x2 -&gt; o2[label = w23]; x2 -&gt; o4[label = w24];
    x3 -&gt; o3[label = w31]; x3-&gt; o1[label = w32]; x3 -&gt; o2[label = w33]; x3 -&gt; o4[label = w34];
    x4 -&gt; o3[label = w41]; x4-&gt; o1[label = w42]; x4 -&gt; o2[label = w43]; x4 -&gt; o4[label = w44];
}
</pre>
</div>

<div id="org5c27a21" class="figure">
<p><img src="./exam-network3.png" alt="exam-network3.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 34: </span>學測成績預測模型#3</p>
</div>

<p>
然而，如果只是以這種「新增特徵值組合與權重」進而產生新特徵值的方式來改變模型，那麼，再多的層數也能合併為一層，因為這些運算方式均屬於線性轉換，為了有效讓模型更加複雜，此處可以在模型中加入非線性轉換，如圖<a href="#org02124c3">27</a>中的 ReLU 函數。在將類似圖<a href="#org87c8118">15</a>這類非線性轉換函數加入模型後，其結果如圖<a href="#org18058af">35</a>所示。<br />
</p>
<div class="org-src-container">
<pre class="src src-dot">  digraph exam{
    rankdir=LR;
    nodesep=.05;
    graph[ranksep=<span style="color: #98be65;">"3"</span>]
    subgraph cluster_0{
             color=orange;
             o2[label=<span style="color: #98be65;">""</span>]; o3[label=<span style="color: #98be65;">""</span>]; o4[label=<span style="color: #98be65;">""</span>]; o1[label=<span style="color: #98be65;">""</span>];
             label = <span style="color: #98be65;">"&#38750;&#32218;&#24615;&#36681;&#25563;"</span>
    }
    z1[label = <span style="color: #98be65;">"&#36664;&#20986;"</span>];
  x3[label = <span style="color: #98be65;">"&#21508;&#31185;&#27573;&#32771;&#24179;&#22343;&#25104;&#32318;"</span>];

    x4[label = <span style="color: #98be65;">"..."</span>];
    x1[label = <span style="color: #98be65;">"&#19978;&#35506;&#35469;&#30495;&#32893;&#35611;&#26178;&#38291;&#30334;&#20998;&#27604;"</span>];
    x2[label = <span style="color: #98be65;">"&#20316;&#26989;&#24179;&#22343;&#25104;&#32318;"</span>];


    o4[label = <span style="color: #98be65;">"..."</span>];

    x1 -&gt; o1[label = w11]; x1-&gt; o2[label = w12]; x1 -&gt; o3[label = w13]; x1 -&gt; o4[label = w14];
    x2 -&gt; o1[label = w21]; x2-&gt; o2[label = w22]; x2 -&gt; o3[label = w23]; x2 -&gt; o4[label = w24];
    x3 -&gt; o1[label = w31]; x3-&gt; o2[label = w32]; x3 -&gt; o3[label = w33]; x3 -&gt; o4[label = w34];
    x4 -&gt; o1[label = w41]; x4-&gt; o2[label = w42]; x4 -&gt; o3[label = w43]; x4 -&gt; o4[label = w44];
    o1 -&gt; z1; o2 -&gt; z1; o3 -&gt; z1; o4 -&gt; z1;
  }
</pre>
</div>


<div id="org18058af" class="figure">
<p><img src="./images/exam-Network4.png" alt="exam-Network4.png" width="700" /><br />
</p>
<p><span class="figure-number">Figure 35: </span>學測成績預測模型#4</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orge09fea9" class="outline-2">
<h2 id="orge09fea9"><span class="section-number-2">7</span> Propagation</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-orgf490b60" class="outline-3">
<h3 id="orgf490b60"><span class="section-number-3">7.1</span> 三層神經網路</h3>
<div class="outline-text-3" id="text-7-1">
<p>
圖<a href="#org59aa006">36</a>為典型的三層神經網路，輸入層(layer 0)有 2 個神經元，兩個隱藏層(layer 1, 2)各有 3 個及 2 個神經元，輸出層(layer 3)則有兩個神經元。神經網路中各層的神經元的訊息傳遞則是透過矩陣相乘來進行。<br />
</p>

<div class="org-src-container">
<pre class="src src-dot">  digraph threeLayerNetwork{
  rankdir=LR
  splines=line
        nodesep=.05;
        node [label=<span style="color: #98be65;">""</span>];
        subgraph cluster_0 {
                 color=white;
                 node [style=solid,color=blue4, shape=circle];
                 x3[label = x1];
                 x1[label = x2];
                 node [style=filled,color=gray, shape=circle];
                 x2[label = 1];
                 label = <span style="color: #98be65;">"layer 0"</span>;
        }
        subgraph cluster_1 {
                 color=white;
                 node [style=filled,color=gray, shape=circle];
                 a12[label = 1];
                 node [style=solid,color=red2, shape=circle];
                 a22[label = a2];
                 a32[label = a3];
                 a42[label = a1];
                 label = <span style="color: #98be65;">"layer 1"</span>;
        }
        subgraph cluster_2 {
                 color=white;
                 node [style=solid,color=red2, shape=circle];
                 a13[label = b2];
                 a23[label = b1];
                 node [style=filled,color=gray, shape=circle];
                 a33[label = 1];
                 label = <span style="color: #98be65;">"layer 2"</span>;
        }
        subgraph cluster_3 {
                 color=white;
                 node [style=solid,color=seagreen2, shape=circle];
                 O1[label = y1];
                 O2[label = y2];
                 label=<span style="color: #98be65;">"layer 3"</span>;
        }

        x1 -&gt; a22;
        x1 -&gt; a32;
        x1 -&gt; a42;

        x2 -&gt; a22;
        x2 -&gt; a32;
        x2 -&gt; a42;

        x3 -&gt; a22;
        x3 -&gt; a32;
        x3 -&gt; a42;

        a12 -&gt; a13;
        a22 -&gt; a13;
        a32 -&gt; a13;
        a42 -&gt; a13;

        a12 -&gt; a23;
        a22 -&gt; a23;
        a32 -&gt; a23;
        a42 -&gt; a23;

        a13 -&gt; O1
        a23 -&gt; O1
        a33 -&gt; O1

        a13 -&gt; O2
        a23 -&gt; O2
        a33 -&gt; O2

  }
</pre>
</div>

<div id="org59aa006" class="figure">
<p><img src="./3LayerNetwork.png" alt="3LayerNetwork.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 36: </span>三層神經網路</p>
</div>

<p>
神網網路間的訊息傳遞(propagation)可分為以下兩類:<br />
</p>
<ul class="org-ul">
<li>前向傳遞(Forward propagation): 較簡單 (只有線性合成，和非線性轉換)<br /></li>
<li>反向傳遞 (Backward propagation): 較複雜 (因為多微分方程)<br /></li>
</ul>
</div>
</div>

<div id="outline-container-org0bbfa76" class="outline-3">
<h3 id="org0bbfa76"><span class="section-number-3">7.2</span> Forward propagation</h3>
<div class="outline-text-3" id="text-7-2">
</div>
<ol class="org-ol">
<li><a id="org662523d"></a>各層間的訊息傳遞<br />
<div class="outline-text-4" id="text-7-2-1">
<p>
以輸入層傳遞訊息至 layer 1 的神經元 a1 為例，a1 的訊息可由公式\eqref{orge5bd2bd}計算出來。<br />
</p>
\begin{equation}
\label{orge5bd2bd}
a_1^{(1)}=w_{11}^{(1)}x_1+w_{12}^{(1)}x_2+b_1^{(1)}
\end{equation}
<p>
其中<br />
</p>
<ul class="org-ul">
<li>\(w_{ab}^{(c)}\)中的\(a\)指的是下一層的第\(a\)個神經元<br /></li>
<li>\(w_{ab}^{(c)}\)中\(b\)指的是上一層的第\(b\)個神經<br /></li>
<li>\(w_{ab}^{(c)}\)中\(c\)指的則是第\(c\)層的權重。<br /></li>
</ul>

<p>
而透過矩陣乘積，可由公式\eqref{orgae0d1a7}顯示 layer 1 的「加權總和」。<br />
</p>
\begin{equation}
\label{orgae0d1a7}
A_1^{(1)}=XW^{(1)}+B^{(1)}
\end{equation}

<p>
其中，<br />
\[
X = \begin{pmatrix} x_1 & x_2 \end{pmatrix},
A^{(1)} = \begin{pmatrix}
          a_1^{(1)} & a_2^{(1)} & a_3^{(1)}
          \end{pmatrix},
B^{(1)} = \begin{pmatrix}
          b_1^{(1)} & b_2^{(1)} & b_3^{(1)}
          \end{pmatrix}, \\
W^{(1)} = \begin{pmatrix}
  w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\
  w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)}
\end{pmatrix}
\]<br />
</p>

<p>
圖<a href="#orge86e808">37</a>以\(a_i\)代表隱藏層 layer 1 中各節點的加權總和，以\(z\)代表以活化函數轉換後的訊號，至於\(h()\)則表示活化函數 sigmoid。<br />
</p>
<div class="org-src-container">
<pre class="src src-dot">  digraph threeLayerNetworkv1{
        rankdir=LR
        graph [pad=<span style="color: #98be65;">"0.2"</span>, nodesep=<span style="color: #98be65;">"0.1"</span>, ranksep=<span style="color: #98be65;">"2"</span>];
        subgraph cluster_0 {
                 color=white;
                 node [style=filled,color=gray];
                 x0[label = 1];
                 node [style=empty,color=blue4];
                 x1[label = x1];
                 x2[label = x2];
                 label = <span style="color: #98be65;">"layer 0"</span>;
        }
        subgraph cluster_1 {
                 color=white;
                 node [style=filled,color=gray];
                 a12[label = 1];
                 node [style=solid,color=red2];
                 a22[label = <span style="color: #98be65;">"z2=h(a2)"</span>];
                 a32[label = <span style="color: #98be65;">"z3=h(a3)"</span>];
                 a42[label = <span style="color: #98be65;">"z1=h(a1)"</span>];
                 label = <span style="color: #98be65;">"layer 1"</span>;
        }

        x0 -&gt; a22[label = b2];
        x0 -&gt; a32[label = b3];
        x0 -&gt; a42[label = b1];

        x1 -&gt; a22[label = w12];
        x1 -&gt; a32[label = w13];
        x1 -&gt; a42[label = w11];

        x2 -&gt; a22[label = w22];
        x2 -&gt; a32[label = w23];
        x2 -&gt; a42[label = w21];
  }
</pre>
</div>

<div id="orge86e808" class="figure">
<p><img src="./3LayerNetworkv1.png" alt="3LayerNetworkv1.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 37: </span>三層神經網路中 layer 0 至 layer 1 間的訊息傳遞</p>
</div>


<p>
以下列 python 執行此次轉換的結果如下。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">python code for message from layer 0 to node a1</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">sigmoid</span>(x):
<span class="linenr"> 4: </span>    <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span> / (<span style="color: #da8548; font-weight: bold;">1</span> + np.exp(-x))
<span class="linenr"> 5: </span><span style="color: #dcaeea;">X</span> = np.array([<span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>])
<span class="linenr"> 6: </span><span style="color: #dcaeea;">W1</span> = np.array([[<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.3</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>], [<span style="color: #da8548; font-weight: bold;">0.2</span>, <span style="color: #da8548; font-weight: bold;">0.4</span>, <span style="color: #da8548; font-weight: bold;">0.6</span>]])
<span class="linenr"> 7: </span><span style="color: #dcaeea;">B1</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.2</span>, <span style="color: #da8548; font-weight: bold;">0.3</span>])
<span class="linenr"> 8: </span><span style="color: #dcaeea;">A1</span> = np.dot(X, W1) + B1
<span class="linenr"> 9: </span><span style="color: #dcaeea;">Z1</span> = sigmoid(A1)
<span class="linenr">10: </span><span style="color: #51afef;">print</span>(X)
<span class="linenr">11: </span><span style="color: #51afef;">print</span>(Z1)
</pre>
</div>

<pre class="example">
[1.  0.5]
[0.57444252 0.66818777 0.75026011]
</pre>


<p>
各層間的訊息傳遞大致如上述模式，唯一稍有不同的地方在於當隱藏層 layer 2 將訊習傳遞給輸出層 layer 4 時，所應用的活化函數為 identitiy function，其實作方式如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">python code for identity function</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">identity_fun</span>(x):
<span class="linenr"> 4: </span>      <span style="color: #51afef;">return</span> x
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">W3</span> = np.array([[<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.3</span>], [<span style="color: #da8548; font-weight: bold;">0.2</span>, <span style="color: #da8548; font-weight: bold;">0.4</span>]])
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">B3</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.2</span>])
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">Z2</span> = [<span style="color: #da8548; font-weight: bold;">0.62624937</span>, <span style="color: #da8548; font-weight: bold;">0.7710107</span>]
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">A3</span> = np.dot(Z2, W3) + B3
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">Y</span> = identity_fun(A3)
<span class="linenr">10: </span>  <span style="color: #51afef;">print</span>(Y)
</pre>
</div>

<pre class="example">
[0.31682708 0.69627909]
</pre>
</div>
</li>
<li><a id="org6cf78bb"></a>Python 實作: 多層神經網路的訊息傳遞<br />
<div class="outline-text-4" id="text-7-2-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">python code for complete multi-layer network message trasmission</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">identity_fun</span>(x):
<span class="linenr"> 4: </span>      <span style="color: #51afef;">return</span> x
<span class="linenr"> 5: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">sigmoid</span>(x):
<span class="linenr"> 6: </span>      <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span> / (<span style="color: #da8548; font-weight: bold;">1</span> + np.exp(-x))
<span class="linenr"> 7: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">init_network</span>():
<span class="linenr"> 8: </span>      <span style="color: #dcaeea;">network</span> = {}
<span class="linenr"> 9: </span>      <span style="color: #dcaeea;">network</span>[<span style="color: #98be65;">'W1'</span>] = np.array([[<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.3</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>], [<span style="color: #da8548; font-weight: bold;">0.2</span>, <span style="color: #da8548; font-weight: bold;">0.4</span>, <span style="color: #da8548; font-weight: bold;">0.6</span>]])
<span class="linenr">10: </span>      <span style="color: #dcaeea;">network</span>[<span style="color: #98be65;">'b1'</span>] = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.2</span>, <span style="color: #da8548; font-weight: bold;">0.3</span>])
<span class="linenr">11: </span>      <span style="color: #dcaeea;">network</span>[<span style="color: #98be65;">'W2'</span>] = np.array([[<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.4</span>], [<span style="color: #da8548; font-weight: bold;">0.2</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>], [<span style="color: #da8548; font-weight: bold;">0.3</span>, <span style="color: #da8548; font-weight: bold;">0.6</span>]])
<span class="linenr">12: </span>      <span style="color: #dcaeea;">network</span>[<span style="color: #98be65;">'b2'</span>] = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.2</span>])
<span class="linenr">13: </span>      <span style="color: #dcaeea;">network</span>[<span style="color: #98be65;">'W3'</span>] = np.array([[<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.3</span>], [<span style="color: #da8548; font-weight: bold;">0.2</span>, <span style="color: #da8548; font-weight: bold;">0.4</span>]])
<span class="linenr">14: </span>      <span style="color: #dcaeea;">network</span>[<span style="color: #98be65;">'b3'</span>] = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.2</span>])
<span class="linenr">15: </span>      <span style="color: #51afef;">return</span> network
<span class="linenr">16: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">forword</span>(network, x):
<span class="linenr">17: </span>      <span style="color: #dcaeea;">W1</span>, <span style="color: #dcaeea;">W2</span>, <span style="color: #dcaeea;">W3</span> = network[<span style="color: #98be65;">'W1'</span>], network[<span style="color: #98be65;">'W2'</span>], network[<span style="color: #98be65;">'W3'</span>]
<span class="linenr">18: </span>      <span style="color: #dcaeea;">b1</span>, <span style="color: #dcaeea;">b2</span>, <span style="color: #dcaeea;">b3</span> = network[<span style="color: #98be65;">'b1'</span>], network[<span style="color: #98be65;">'b2'</span>], network[<span style="color: #98be65;">'b3'</span>]
<span class="linenr">19: </span>      <span style="color: #dcaeea;">a1</span> = np.dot(x, W1) + b1
<span class="linenr">20: </span>      <span style="color: #dcaeea;">z1</span> = sigmoid(a1)
<span class="linenr">21: </span>      <span style="color: #dcaeea;">a2</span> = np.dot(z1, W2) + b2
<span class="linenr">22: </span>      <span style="color: #dcaeea;">z2</span> = sigmoid(a2)
<span class="linenr">23: </span>      <span style="color: #dcaeea;">a3</span> = np.dot(z2, W3) + b3
<span class="linenr">24: </span>      <span style="color: #dcaeea;">z3</span> = sigmoid(a3)
<span class="linenr">25: </span>      <span style="color: #dcaeea;">y</span> = identity_fun(z3)
<span class="linenr">26: </span>      <span style="color: #51afef;">return</span> y
<span class="linenr">27: </span>  <span style="color: #dcaeea;">network</span> = init_network()
<span class="linenr">28: </span>  <span style="color: #dcaeea;">x</span> = np.array([<span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>])
<span class="linenr">29: </span>  <span style="color: #dcaeea;">y</span> = forword(network, x)
<span class="linenr">30: </span>
<span class="linenr">31: </span>  <span style="color: #51afef;">print</span>(y)
</pre>
</div>

<pre class="example">
[0.57855079 0.66736228]
</pre>
</div>
</li>
</ol>
</div>

<div id="outline-container-org5951c86" class="outline-3">
<h3 id="org5951c86"><span class="section-number-3">7.3</span> Backward propagation</h3>
<div class="outline-text-3" id="text-7-3">
<p>
反向傳遞的目的就是利用最後的目標函數(loss/cost function)來進行參數的更新，一般來說都是用誤差均方和(mean square error)當作目標函數。如果誤差值越大，代表參數學得不好，所以需要繼續學習，直到參數或是誤差值收斂<sup><a id="fnr.6.100" class="footref" href="#fn.6">6</a></sup>。<br />
回到圖<a href="#org59aa006">36</a><br />
\(x^{(i)}\)為第i筆輸入，其輸出值為<br />
\[ y^{(i)}=\begin{bmatrix}
y_1^{(1)} \\
y_2^{(1)} \\
\end{bmatrix}\]<br />
而輸出值與目標的誤差為<br />
\[
E^{(i)}=\frac{1}{2}\sum_{j=0}^m(\hat{y}_j^{(i)}-y_j^{(i)})^2
\]<br />
目標函數為所有樣本的誤差和<br />
\[
E=\sum_{i=0}^nE^{(i)}
\]<br />
最佳化的目的就是讓「所有樣本的誤差均方和」越小越好，所以目標是將最小化(微分方程式等於0找解)。<br />
</p>
</div>
</div>

<div id="outline-container-org0928a3a" class="outline-3">
<h3 id="org0928a3a"><span class="section-number-3">7.4</span> 輸出層的設計：恆等函數與 softmax 函數</h3>
<div class="outline-text-3" id="text-7-4">
<p>
神經網路可以用來解決分類問題與迴歸問題，端視輸出層所使用的活化函數，解決迴歸問題時使用恆等函數，而分類問題則使用 <i>softmax 函數</i> 。恆等函數對於輸入的內容完全不做任何處理，直接輸出，其神經網路的結構如圖<a href="#org6499628">38</a>所示。<br />
</p>
<div class="org-src-container">
<pre class="src src-dot">  digraph IDNT{
    rankdir=LR;
    a3 -&gt; y3[label = <span style="color: #98be65;">"delta()"</span>];
    a2 -&gt; y2[label = <span style="color: #98be65;">"delta()"</span>];
    a1 -&gt; y1[label = <span style="color: #98be65;">"delta()"</span>];
  }
</pre>
</div>

<div id="org6499628" class="figure">
<p><img src="./identity-network.png" alt="identity-network.png" width="300" /><br />
</p>
<p><span class="figure-number">Figure 38: </span>恆等函數神經網路圖</p>
</div>

<p>
而分類問題使用的 softmax 函數則如公式\eqref{org0a73c3c}所示，\(exp(x)\)為代表\(e^x\)的指標函數，輸出層有 n 個節點，而每個節點收到的訊息\(y_k\)來自前一層以箭頭連接的所有訊號輸入，由公式\eqref{org0a73c3c}的分母也可以看出，輸出的各個神經元會依以「依各節點訊號量比例」的模式影響下一層的輸入。<br />
</p>

\begin{equation}
\label{org0a73c3c}
y_k = \frac{exp(a_k)}{\sum_{i=1}^{n}exp(a_i)}
\end{equation}

<div class="org-src-container">
<pre class="src src-dot">  digraph SMNT{
    rankdir=LR;
    nodesep=.05;
    graph[ranksep=<span style="color: #98be65;">"3"</span>]
    a2 -&gt; y1[label = <span style="color: #98be65;">"softmax()"</span>];
    a2 -&gt; y3[label = <span style="color: #98be65;">"softmax()"</span>];
    a2 -&gt; y2[label = <span style="color: #98be65;">"softmax()"</span>];
    a1 -&gt; y1[label = <span style="color: #98be65;">"softmax()"</span>];
    a1 -&gt; y2[label = <span style="color: #98be65;">"softmax()"</span>];
    a1 -&gt; y3[label = <span style="color: #98be65;">"softmax()"</span>];
    a3 -&gt; y1[label = <span style="color: #98be65;">"softmax()"</span>];
    a3 -&gt; y2[label = <span style="color: #98be65;">"softmax()"</span>];
    a3 -&gt; y3[label = <span style="color: #98be65;">"softmax()"</span>];
  }
</pre>
</div>

<div id="org9428d5c" class="figure">
<p><img src="images/softmax-network.png" alt="softmax-network.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 39: </span>softmax 函數神經網路圖</p>
</div>

<p>
至於 softmax 的 python 實作則如下程式碼所示，為了避免因矩陣 a 的值過大而導至指數函數運算出現溢位，程式碼第<a href="#coderef-softmax-overflow1" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-softmax-overflow1');" onmouseout="CodeHighlightOff(this, 'coderef-softmax-overflow1');">4</a>行的內容也可以改由第<a href="#coderef-softmax-overflow2" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-softmax-overflow2');" onmouseout="CodeHighlightOff(this, 'coderef-softmax-overflow2');">5</a>行替代。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">python code for softmax funtion</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">softmax</span>(a):
<span id="coderef-softmax-overflow1" class="coderef-off"><span class="linenr"> 4: </span>      <span style="color: #dcaeea;">exp_a</span> = np.exp(a)</span>
<span id="coderef-softmax-overflow2" class="coderef-off"><span class="linenr"> 5: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">exp_a = np.exp(a - np.max(a))</span></span>
<span class="linenr"> 6: </span>      <span style="color: #dcaeea;">sum_exp_a</span> = np.<span style="color: #c678dd;">sum</span>(exp_a)
<span class="linenr"> 7: </span>      <span style="color: #dcaeea;">y</span> = exp_a / sum_exp_a
<span class="linenr"> 8: </span>      <span style="color: #51afef;">return</span>(y)
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">a</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.3</span>, <span style="color: #da8548; font-weight: bold;">2.9</span>, <span style="color: #da8548; font-weight: bold;">4.0</span>])
<span class="linenr">10: </span>  <span style="color: #51afef;">print</span>(softmax(a))
</pre>
</div>
<pre class="example">
[0.01821127 0.24519181 0.73659691]
</pre>
</div>
</div>

<div id="outline-container-org5c47d76" class="outline-3">
<h3 id="org5c47d76"><span class="section-number-3">7.5</span> softmax 函數的特色</h3>
<div class="outline-text-3" id="text-7-5">
<p>
softmaxe 的輸出為介於 0 到 1 間的實數，此外，其輸出總和為 1，這個性質使的 softmax 函數的輸出也可解釋為「機率」。例如，前節程式碼的輸出結果為[0.01821127 0.24519181 0.73659691]，從以機率的角度我們可以說：分類的結果有 1.8%的機率為第 0 類；有 24.52%的機率為第 1 類；有 73.66%的機率為第 2 類。換言之，使用 softmax 函數可以針對問題提出對應的機率。<br />
softmax 函數的另一個特色是其輸出結果仍保持與輸入訊息一致的大小關係，這是因為惷數函數\(y=exp(x)\)為單週函數。一般而言，神經網路會把輸出最大神經元的類別當作辨識結果，然而，因為 softmax 不影響大小順序，所以一般會省略 softmax 函數。<br />
</p>

<p>
輸出層的節點數量取決於要解決的問題，例如，如果要解決的問題為「判斷一個手寫數字的結果」，則輸出層會有 10 個節點(分別代表 0~9)，而輸出訊息最大的結點則為最有可能的答案類別。<br />
</p>
</div>
</div>

<div id="outline-container-orgc088ae1" class="outline-3">
<h3 id="orgc088ae1"><span class="section-number-3">7.6</span> 梯度下降法</h3>
<div class="outline-text-3" id="text-7-6">
<p>
<a href="https://www.youtube.com/watch?v=reOgXrNxwV8">https://www.youtube.com/watch?v=reOgXrNxwV8</a><br />
<a href="https://www.youtube.com/watch?v=FPD2fsryGYk">https://www.youtube.com/watch?v=FPD2fsryGYk</a><br />
<a href="https://github.com/Qinbf">https://github.com/Qinbf</a><br />
<a href="https://www.youtube.com/channel/UCHbGAYq21PNz9REWNwe8Z-Q/videos">https://www.youtube.com/channel/UCHbGAYq21PNz9REWNwe8Z-Q/videos</a><br />
</p>
</div>
</div>
</div>



<div id="outline-container-org29ef03b" class="outline-2">
<h2 id="org29ef03b"><span class="section-number-2">8</span> 神經網路的學習</h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-org39fb59d" class="outline-3">
<h3 id="org39fb59d"><span class="section-number-3">8.1</span> 從資料中學習</h3>
<div class="outline-text-3" id="text-8-1">
<p>
神經網路的特色是可以從資料中學習，此處的學習指「自動決定權重參數」，以人工來指定權重參數值這個工作太過鉅大以至於可行性不高，因為有些神經網路的權重參數可能高達數千億個。機器學習與神經網路最大的差異也在於此。以辨識手寫字（如圖<a href="#org975c5c5">25</a>）為例，機器學習的作法是先以人工找出該字圖形的特徵量（可以透過 SIFT、SURF、HOG 等視覺領域方法），將該特徵量轉換為向量，再利用機器學習的辨識器（如 SVM、KNN）來學習這些轉換過的特徵向量；而神經網路的作法則是直接將原始圖形當成輸入，至於特徵量本身也是透過神經網路自行學習取得，再透過這些行習得的特徵量進行學習辨識。<br />
</p>

<p>
在機器學習的過程中，一般會把資料分成訓練資料與測試資料兩類，其中訓練資料用來進行學習、尋找最佳參數；而測試資料則是用來評估訓練後的模型成效。若只使用同一種資料集（如同一個人的字跡）來進行訓練與評估，則可能導致模型只對此人的字跡有辨識成效，這㮔問題稱為過度擬合（overfitting）。<br />
</p>
</div>
</div>

<div id="outline-container-org87e5607" class="outline-3">
<h3 id="org87e5607"><span class="section-number-3">8.2</span> Overfitting v.s. underfitting</h3>
<div class="outline-text-3" id="text-8-2">
<p>
機器學習的本質即在於最佳化與普遍性之間的拉扯，最佳化(optimization)是指調整模型使得模型能在訓練資料上獲得最佳表現的過程；而普遍性(generalization)是指已訓練過的模型對從未見過的資料的預測能力。在訓練之初，最佳化和普遍性是高度相關的，訓練資料的損失越低、測試資料的損失也越低。此時，模型仍處於低度擬合狀態，仍有進步空間；在訓練資料經過一定回合(epoch)的訓練後，普遍程度的改善幅逐漸停止，驗證指標隨之停滯然後開始變差，於是模型開始 overfitting。也就是說，模型已經學習了一些訓練集特有的模式，但這些特有模式根本和新資料不相關、甚或會誤導對新資料的預測。<br />
</p>

<p>
為了防止模型在訓練資料中學到錯誤或不相關的模式，最好的解決方案是取得更多訓練資料，如果無法做到這一點，次佳的解決方案是調配模型儲存的訊訊量或者限制儲存資訊的類型或數值，如果一個神經網路只能記住少量的模式，那麼優化過程將迫使它專注於最顯著的那些模式，而不會去記住那些不相關的資訊，如此才能適應從未見過的新資料。<br />
</p>

<p>
這種週配或限制模型資訊(即參數)以對抗 overfitting 的方式稱為常規化(regularization)，幾個常見的常規化技術如下：<br />
</p>
</div>

<ol class="org-ol">
<li><a id="orgbf5ec0e"></a>縮減神經網路的大小<br />
<div class="outline-text-4" id="text-8-2-1">
<p>
即減少模型可用來學習的參數數量(包含網路的層數和每層的神經元個數)，在深度學習中，模型中可學習參數的數量(權重數量)通常被稱為容量(capacity)。直覺上，有更多參數的模型具有更多的記憶能力(memorization capacity)，因此可以很容易在訓練樣本與其目標之間做出完美的對應，但卻沒有任何適應學習的能力。例如，我們可以建立具有 500000 個參數的模型來輕鬆學習(記憶)MNIST 訓練集中的每個數字所屬類別，但這種模型對於預測新的數字圖案毫無用處。<br />
</p>

<p>
此外，如果神經網路的記憶資源有限，則無法輕易地直接在訓練樣本與目標之間做出對應，因此，要讓損失最小化，神經網路必須採用萃取過的資料表示法，以建立對目標的預測能力；另一方面，我們仍須讓神經網路擁有足夠的參數，以免模型 underfitting。也就是說，我們必須在容量過大(too much capacity)和容量不足(not enough capacity)之間取得平衡。<br />
</p>

<p>
然而，到目前為止還沒有有效的公式來確定網路最佳的層數和神經元數，我們必須實際在神經網路進行多次評估(在驗證集上，而非測試集)，以找到正確的模型大小。我們通常會從較少的層數和 units 數開始，再逐漸增加層的 units 數或增加新層數，直到驗證損失不再進步為止。<br />
</p>

<p>
以 imdb 的例子來看，原本的網路模型如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">2: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">3: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">4: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
</pre>
</div>

<p>
如果我們將每一層的神經元數量縮小：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">2: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">4</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">3: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">4</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">4: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
</pre>
</div>

<p>
則兩個版本的驗證損失比較如下圖，黑點為縮小網路 capacity 的驗證損失值，可以看出較原始版本的表現更佳，這個版本的神經網路在第 6 個 epoch 之後才開始 overfitting（原始的模型在第 4 個 epoch）就開始 overfitting。<br />
</p>


<div id="orgef62f70" class="figure">
<p><img src="images/img-191120103824.jpg" alt="img-191120103824.jpg" /><br />
</p>
<p><span class="figure-number">Figure 40: </span>模型容量對驗證損失分數的影響-較小的模型</p>
</div>

<p>
相反的，如果我們刻意將神經網路的 capacity 擴大，如下，每一層的神經元數量由原來的 16 增加到 512，則結果如圖<a href="#orgf415ac6">41</a>，擴大版的神經網路幾乎在訓練之初就開始 overfitting，而且越來越嚴重，其驗證損失的表現也較原始版本差。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">2: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">512</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">3: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">512</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">4: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
</pre>
</div>


<div id="orgf415ac6" class="figure">
<p><img src="images/img-191120103825.jpg" alt="img-191120103825.jpg" /><br />
</p>
<p><span class="figure-number">Figure 41: </span>模型容量對驗證損失分數的影響-較大的容量</p>
</div>

<p>
較大模型的神經網路雖然導致驗證損失的效能下降，但在訓練損失上的表現則否，由圖<a href="#org0621994">42</a>的黑點可明顯看出大網路模型在訓練期間的損失極低，然而這種效能在面對新資料（驗證資烞十）時確無法表現出來。<br />
</p>


<div id="org0621994" class="figure">
<p><img src="images/img-191120103826.jpg" alt="img-191120103826.jpg" /><br />
</p>
<p><span class="figure-number">Figure 42: </span>模型容量對訓練損失分數的影響-較大的容量</p>
</div>
</div>
</li>

<li><a id="org7c52d35"></a>加入權重常規化 (weight regularization)<br />
<div class="outline-text-4" id="text-8-2-2">
<p>
在設計網路模型時，一個原則為：如果兩種模型有同樣的效能表現，則較簡單的模型通常是更好的設計，也更不容易導致 overfitting。這裡所謂的簡單指的是參數值分佈的熵比較小(entropy of distribution of parameter values las less entropy)的模型，或是使用較少參數的模型。因此，降低 overfitting 就是想辦法採用較小的權重值以限制神經網路的複雜性，這會讓權重的分佈更為常規化(regularized)。權重值常規化(weight regularization)在作法上就是對損失函數中較大的權重加上代價(cost)項目，通常有兩種方式：<br />
</p>

<ul class="org-ul">
<li>L1 regularization: 在損失函數多加上一項 cost，這一項和權重係數的絕對值成正比。<br /></li>
<li>L2 regularization: 在損失函數多加上一項 cost，這一項和權重係數的平方成正比。L2 也稱為 weight decay。<br /></li>
</ul>

<p>
在 Keras 中，常規化的實作只要利用 model.add()並指名參數把權重常規化物件傳入神經網路層即可。以下為 L2 regularization 的做法：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> regularizers
<span class="linenr">2: </span>
<span class="linenr">3: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">4: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, kernel_regularizer=regularizers.l2(<span style="color: #da8548; font-weight: bold;">0.001</span>),
<span class="linenr">5: </span>                         activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">6: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, kernel_regularizer=regularizers.l2(<span style="color: #da8548; font-weight: bold;">0.001</span>),
<span class="linenr">7: </span>                         activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">8: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
</pre>
</div>

<p>
上述程式中 l2(0.001)的意思是表示該層權重矩陣中的每個係數都會加上(0.001*權重係數值)到神經網路的總損失函數上，由圖<a href="#orgd0a75eb">43</a>的結果可看，加入 L2 regularization 的模組，其驗證損失的表現較佳。<br />
</p>


<div id="orgd0a75eb" class="figure">
<p><img src="images/img-19112010382A.jpg" alt="img-19112010382A.jpg" /><br />
</p>
<p><span class="figure-number">Figure 43: </span>權重正規化對訓練損失分數的影響</p>
</div>
</div>
</li>

<li><a id="org6bbe1fc"></a>丟棄法 (dropout)<br />
<div class="outline-text-4" id="text-8-2-3">
<p>
由 Ceoff Hinton 教授和他在多倫多大學的學生所開發出來的常規化技術之一，主要是在訓練期間隨機丟棄(dropping out，即把 feature 值歸零)layer 的一些輸出特徵，假設某層在訓練期間的某一狀態下其正常輸出向量為[0.2, 0.5, 1.3, 0.8, 1.1]，在 dropout 後，某幾個輸輸向量的值會被歸零，如[0, 0.5, 1.3, 0, 1.1]。丟棄率則是指被歸零的特徵值個數佔特徵值總數的比例，以此例而言丟棄率為 2/5=0.4。<br />
</p>

<p>
丟棄率通常介於 0.2 到 0.5 之間，而在測試時，其實沒有任何特徵質會被丟棄，取而代之的是層的輸出值將依照丟棄率的比例縮小，以平䚘訓練時的輸出被歸零的影響。隨機歸零的核心想法是在 layer 的輸出值中加入&ldquo;雜訊&rdquo;，這樣可以打破不重要的偶然模式，如果沒有雜訊，神經網路就會開始&ldquo;死記&rdquo;。在 Keras 中，我們可以很簡單的透過 add()來加入 Dropout：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> regularizers
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr"> 4: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, kernel_regularizer=regularizers.l2(<span style="color: #da8548; font-weight: bold;">0.001</span>),
<span class="linenr"> 5: </span>                         activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr"> 6: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr"> 7: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, kernel_regularizer=regularizers.l2(<span style="color: #da8548; font-weight: bold;">0.001</span>),
<span class="linenr"> 8: </span>                         activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 9: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr">10: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
</pre>
</div>

<p>
在加入兩個 Dropout 層後對於降低 overfitting 的效果如圖<a href="#org7149bec">44</a>所示。<br />
</p>


<div id="org7149bec" class="figure">
<p><img src="images/img-19112010382B.jpg" alt="img-19112010382B.jpg" /><br />
</p>
<p><span class="figure-number">Figure 44: </span>權重正規化對訓練損失分數的影響</p>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-org9d17651" class="outline-3">
<h3 id="org9d17651"><span class="section-number-3">8.3</span> 損失函數</h3>
<div class="outline-text-3" id="text-8-3">
<p>
至於神經網路模型的成效則可由損失函數（loss function）來判斷，常見的損失函數有均方誤差與交叉熵誤差兩種。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org9779c84"></a>均方誤差<br />
<div class="outline-text-4" id="text-8-3-1">
<p>
均方誤差（mean squared error）可以公式\eqref{org2e33520}表示：<br />
</p>
\begin{equation}
\label{org2e33520}
E = \frac{1}{2} \sum_{k}(y_k-t_k)^2
\end{equation}
<p>
公式\eqref{org2e33520}中的\(y_k\)為神經網路的輸出，\(t_k\)為訓練資料，\(k\)為資料的維度，其實際運算的資料內容與結果可由如下程式碼，當 y 的預測機率越接近正確答案時，均方誤差的值越小。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">mean_squared_error</span>(y, t):
<span class="linenr"> 3: </span>      <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">0.5</span> * np.<span style="color: #c678dd;">sum</span>((y-t)**<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">case 1: &#27491;&#30906;&#31572;&#26696;&#28858;2, y[2]&#30340;&#27231;&#29575;&#26368;&#39640;&#26178;</span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">y</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.05</span>, <span style="color: #da8548; font-weight: bold;">0.6</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.05</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>])
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">t</span> = np.array([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 7: </span>  <span style="color: #51afef;">print</span>(mean_squared_error(y,t))
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">case 2: &#27491;&#30906;&#31572;&#26696;&#28858;2, y[7]&#30340;&#27231;&#29575;&#26368;&#39640;&#26178;</span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">y</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.05</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.05</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.6</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>])
<span class="linenr">11: </span>  <span style="color: #51afef;">print</span>(mean_squared_error(y,t))
</pre>
</div>

<pre class="example">
0.09750000000000003
0.5975
</pre>
</div>
</li>

<li><a id="orga2419e4"></a>交叉熵誤差<br />
<div class="outline-text-4" id="text-8-3-2">
<p>
交叉熵誤差（cross entropy error）可以公式\eqref{org3fd8a8a}表示：<br />
</p>
\begin{equation}
\label{org3fd8a8a}
E = -\sum_{k}t_k\log y_k
\end{equation}
<p>
公式\eqref{org3fd8a8a}中的\(y_k\)為神經網路的輸出，\(t_k\)為訓練資料（即正確答案標籤，為 one-hot 形式，只有正確案為 1，其餘為 0），\(log\)為以\(e\)為底的自然對數，其輸出圖形如圖<a href="#orgb8dc0ed">45</a>所示。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">2: </span>  <span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr">3: </span>  <span style="color: #dcaeea;">x</span> = np.arange(-<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">0.0001</span>)
<span class="linenr">4: </span>  <span style="color: #dcaeea;">y</span> = np.log(x)
<span class="linenr">5: </span>  plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">4</span>,<span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr">6: </span>  plt.plot(x, y)
<span class="linenr">7: </span>  plt.ylim(-<span style="color: #da8548; font-weight: bold;">5.1</span>, <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">8: </span>  plt.savefig(<span style="color: #98be65;">"log.png"</span>)
<span class="linenr">9: </span>  <span style="color: #51afef;">return</span> <span style="color: #98be65;">"log.png"</span>
</pre>
</div>

<div id="orgb8dc0ed" class="figure">
<p><img src="images/log.png" alt="log.png" /><br />
</p>
<p><span class="figure-number">Figure 45: </span>自然對數\(y=\log x\)曲線圖</p>
</div>

<p>
以前述範例進行交叉熵的計算，其運算結果如下所示。進行\(log\)運算前先加上一極小值 delta 的原因在於避免因 log(0)產生無限大的-inf。執行結果與前例大致相同，越接近正確答案，誤差值越小。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">cross_entropy_error</span>(y, t):
<span class="linenr"> 3: </span>      <span style="color: #dcaeea;">delta</span> = 1e-<span style="color: #da8548; font-weight: bold;">7</span>
<span class="linenr"> 4: </span>      <span style="color: #51afef;">return</span> -np.<span style="color: #c678dd;">sum</span>(t * np.log(y + delta))
<span class="linenr"> 5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">case 1: &#27491;&#30906;&#31572;&#26696;&#28858;2, y[2]&#30340;&#27231;&#29575;&#26368;&#39640;&#26178;</span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">y</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.05</span>, <span style="color: #da8548; font-weight: bold;">0.6</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.05</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>])
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">t</span> = np.array([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 8: </span>  <span style="color: #51afef;">print</span>(cross_entropy_error(y,t))
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">case 2: &#27491;&#30906;&#31572;&#26696;&#28858;2, y[7]&#30340;&#27231;&#29575;&#26368;&#39640;&#26178;</span>
<span class="linenr">11: </span>  <span style="color: #dcaeea;">y</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.05</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.05</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.6</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>])
<span class="linenr">12: </span>  <span style="color: #51afef;">print</span>(cross_entropy_error(y,t))
</pre>
</div>

<pre class="example">
0.510825457099338
2.302584092994546
</pre>
</div>
</li>

<li><a id="org6f4e4d3"></a>批次學習<br />
<div class="outline-text-4" id="text-8-3-3">
<p>
機器學習的目的在於使用訓練資料找能能儘量縮小損失函數的參數，因此，計算損失函數自然不是逐一計算，而是以縮小所有訓練資料的損失函數總和為最終目標。<br />
因此，前節中的交叉熵誤差（公式\eqref{org3fd8a8a}）就要修改為以下算式（\eqref{org5d33cbe}）：<br />
</p>
\begin{equation}
\label{org5d33cbe}
E = -\frac{1}{N}\sum_{n}\sum_{k}t_{nk}\log y_{nk}
\end{equation}
<p>
公式\eqref{org5d33cbe}的 N 為資料筆數，\(y_{nk}\)代表神經網路第 n 筆資料的第 k 個輸出、\(t){nk}\)則為第 n 筆訓練資料的第 k 個值，最後除以 N、進行正規化，所得即為這 N 筆資料的「平均損失函數」。然而，實際執行時並無法取出所有訓練資料來運算，以 MNIST 資料集為例，訓練資料就有 60000 筆，若是其他大數劇則資料筆數可能達數千萬，較務實的作法，是隨機自資料集中抽取 N 筆資料（稱之為 mini batch）來計算平均損失函數，以代表整體。<br />
那麼，這個隨機抽取的資料筆數要設定為多少才適合？可以從以下兩個觀點來看：<br />
</p>
<ul class="org-ul">
<li>在合理範圍內，增大 Batch_Size 有何好處？<br />
<ul class="org-ul">
<li>內存利用率提高了，大矩陣乘法的並行化效率提高。<br /></li>
<li>跑完一次 epoch（全數據集）所需的迭代次數減少，對於相同數據量的處理速度進一步加快。<br /></li>
<li>在一定範圍內，一般來說 Batch_Size 越大，其確定的下降方向越准，引起訓練震蕩越小。<br /></li>
</ul></li>
<li>盲目增大 Batch_Size 有何壞處？<br />
<ul class="org-ul">
<li>內存利用率提高了，但是內存容量可能撐不住了。<br /></li>
<li>跑完一次 epoch（全數據集）所需的迭代次數減少，要想達到相同的精度，其所花費的時間大大增加了，從而對參數的修正也就顯得更加緩慢。<br /></li>
<li>Batch_Size 增大到一定程度，其確定的下降方向已經基本不再變化。<br /></li>
</ul></li>
</ul>
</div>
</li>

<li><a id="orgfe9778d"></a>設定損失函數做為效能指標的原因<br />
<div class="outline-text-4" id="text-8-3-4">
<p>
進行神經網路的學習，目的在於找出辨識準確度高的參數，那麼，為什麼不以「辨識準確率」做為指標，而是要導入損失函數？<br />
神經網路的學習過程在於尋找最佳參數（即偏移值與權重），藉以找出能盡量縮小損失函數的參數，為達此目的，我們要對某特定參數所得的損失函數進行微分（即找出其斜度），亦即，找出「如果稍微改變這些參數，損失函數將會有何種變化？」的答案，假如微分後損失函數的值變為負值，則我們就將參數往正向變化以減少損失函數；假如微分後損失函數的值變為正值，我們就讓參數往負值變化，以減少損失函數；直到損失函數微分後的值變為 0 為止。<br />
無法將辨識準確率作為指標的原因即在於，不論我們如何調整參數，辨識準確率的微分值都為 0。<br />
例如，有 100 張訓練資料，如果能成功辨識出 32 張影像，其準確率為 32%，如果調整參數後只得到極小的變化，辨識準確度也只能在 33%、34%這些不連續的變化，而不會有 32.0124%的連續性變化。然而，如果以損失函數為指標，則能顯示出 0.92345&#x2026;.的結果；而略為改變參數值，即能得到 0.24884 的連續變化。<br />
辨識準確率幾乎不會反應改變參數所帶來的微小變化，即使有反應也是不連續的變化，這個道理就好像把階梯函數拿來當成活化函數，對階梯函數的任何點（除 0 以外）做切線，其斜率均為 0。<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org8dee103" class="outline-3">
<h3 id="org8dee103"><span class="section-number-3">8.4</span> 數值微分</h3>
<div class="outline-text-3" id="text-8-4">
<p>
梯度法使用梯度的資料來決定學習（或找出最佳參數）的方向，此處會應用到數學的微分。微分指的是某個瞬間的變化量（如行進中的車輛瞬間速度的變化量），可以公式\eqref{org7783483}來定義：<br />
</p>
\begin{equation}
\label{org7783483}
\frac{df(x)}{dx}=\lim_{h\to0}\frac{f(x+h)-f(x)}{h}
\end{equation}
<p>
上述公式中，等號左側\(\frac{df(x)}{dx}\)代表\(f(x)\)中對\(x\)微分，亦即，找出相對 x 之 f(x)的變化，也就是希望能找出：「隨著\(x\)的細微改變，函數\(f(x)\)會出現何種變化？」的答案，此處的\(h\)趨近於 0。<br />
以公式\eqref{org7783483}實際進行 python 運算會因為 h 值太小而導致四捨五入無法計算出真實結果，如下列程式碼，若\(x=10e-50\)，其計算結果為 0。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_diff</span>(f, x):
<span class="linenr">2: </span>      <span style="color: #dcaeea;">h</span> = 10e - <span style="color: #da8548; font-weight: bold;">50</span>
<span class="linenr">3: </span>      <span style="color: #51afef;">return</span> (f(x+h) - f(x)) /h
</pre>
</div>
<p>
原因是因為\(h\)值過小而被四捨五入為 0，改善方式為將\(h\)設定為\(10^-4\)。<br />
另一個要處理的問題是：我們想求得的是函數\(f(x)\)在點\(x\)的斜率，然而上述程式碼所計算出的是函數\(f(x)\)在\(x\)與\(x+h\)區間的斜率，改善方式是改為計算\(f(x+h)\)與\(f(x-h)\)區間的斜率，即進行數值微分（數值梯度），其程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_diff</span>(f, x):
<span class="linenr">2: </span>      <span style="color: #dcaeea;">h</span> = 1e-<span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr">3: </span>      <span style="color: #51afef;">return</span> (f(x+h) - f(x-h)) / (<span style="color: #da8548; font-weight: bold;">2</span>*h)
</pre>
</div>
<p>
若以上述方式，分別在\(x=5\)及\(x=10\)時對計算算式\(y=0.01x^2+0.1x\)的微分，其結果如下，橘色與綠色直線分別為\(x=5\)與\(x=10\)的切線。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr"> 3: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_diff</span>(f, x):
<span class="linenr"> 4: </span>      <span style="color: #dcaeea;">h</span> = 1e-<span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr"> 5: </span>      <span style="color: #51afef;">return</span> (f(x+h) - f(x-h)) / (<span style="color: #da8548; font-weight: bold;">2</span>*h)
<span class="linenr"> 6: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">function_1</span>(x):
<span class="linenr"> 7: </span>      <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">0.01</span>*x**<span style="color: #da8548; font-weight: bold;">2</span> + <span style="color: #da8548; font-weight: bold;">0.1</span>*x
<span class="linenr"> 8: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">tangent_line</span>(f, x):
<span class="linenr"> 9: </span>      <span style="color: #dcaeea;">d</span> = numerical_diff(f, x)
<span class="linenr">10: </span>      <span style="color: #dcaeea;">y</span> = f(x) - d*x
<span class="linenr">11: </span>      <span style="color: #51afef;">return</span> <span style="color: #51afef;">lambda</span> t: d*t + y
<span class="linenr">12: </span>  <span style="color: #dcaeea;">x</span> = np.arange(<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">20.0</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>)
<span class="linenr">13: </span>  <span style="color: #dcaeea;">y</span> = function_1(x)
<span class="linenr">14: </span>  <span style="color: #dcaeea;">tf</span> = tangent_line(function_1, <span style="color: #da8548; font-weight: bold;">5</span>)
<span class="linenr">15: </span>  <span style="color: #dcaeea;">y2</span> = tf(x)
<span class="linenr">16: </span>  <span style="color: #dcaeea;">tf2</span> = tangent_line(function_1, <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">17: </span>  <span style="color: #dcaeea;">y3</span> = tf2(x)
<span class="linenr">18: </span>  plt.xlabel(<span style="color: #98be65;">"x"</span>)
<span class="linenr">19: </span>  plt.ylabel(<span style="color: #98be65;">"f(x)"</span>)
<span class="linenr">20: </span>  plt.plot(x, y)
<span class="linenr">21: </span>  plt.plot(x, y2)
<span class="linenr">22: </span>  plt.plot(x, y3)
<span class="linenr">23: </span>  plt.savefig(<span style="color: #98be65;">"function_1.png"</span>)
<span class="linenr">24: </span>  <span style="color: #51afef;">print</span>(numerical_diff(function_1,<span style="color: #da8548; font-weight: bold;">5</span>))
<span class="linenr">25: </span>  <span style="color: #51afef;">print</span>(numerical_diff(function_1,<span style="color: #da8548; font-weight: bold;">10</span>))
<span class="linenr">26: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">return "function_1.png"</span>
</pre>
</div>

<pre class="example">
0.1999999999990898
0.2999999999986347
</pre>



<div id="org4ee2c2a" class="figure">
<p><img src="images/function_1.png" alt="function_1.png" /><br />
</p>
<p><span class="figure-number">Figure 46: </span>\(f(x)=0.01x^2+0.1x\)圖表</p>
</div>

<p>
此處計算的結果與數學解相比：\(\frac{df(x)}{dx}=0.02x+0.1\)，\(x=5\)、\(x=10\)的實際微分值應為 0.2、0.3，嚴格來說並不一致，但因誤差太小，可以視為相同。<br />
</p>

<p>
若再進一步考慮兩個變數時的微分，如算式\eqref{orgb453f1e}，這種由多個變數形成的微分，稱作偏微分，，該公式對應的 python 程式碼如下：<br />
</p>
\begin{equation}
\label{orgb453f1e}
\(f(x_0,x_1)=x_0^2+x_1^2\)
\end{equation}

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">function_2</span>(x):
<span class="linenr">2: </span>      <span style="color: #51afef;">return</span> x[<span style="color: #da8548; font-weight: bold;">0</span>]**<span style="color: #da8548; font-weight: bold;">2</span> + x[<span style="color: #da8548; font-weight: bold;">1</span>]**<span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr">3: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25110; return np.sum(x**2)</span>
</pre>
</div>
<p>
公式\(fx_0,x_1)=x_0^2+x_1^2\)的圖表如圖<a href="#orgcfb506f">47</a>所示。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">function_2</span>(x):
<span class="linenr"> 2: </span>    <span style="color: #51afef;">return</span> x[<span style="color: #da8548; font-weight: bold;">0</span>]**<span style="color: #da8548; font-weight: bold;">2</span> + x[<span style="color: #da8548; font-weight: bold;">1</span>]**<span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">3D&#12464;&#12521;&#12501;&#12434;&#25551;&#30011;</span>
<span class="linenr"> 5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">cat multivariate_func_save.py</span>
<span class="linenr"> 6: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">coding: utf-8</span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 8: </span>  <span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr"> 9: </span>  plt.switch_backend(<span style="color: #98be65;">'agg'</span>)
<span class="linenr">10: </span>  <span style="color: #51afef;">from</span> mpl_toolkits.mplot3d <span style="color: #51afef;">import</span> Axes3D
<span class="linenr">11: </span>
<span class="linenr">12: </span>  <span style="color: #dcaeea;">x</span> = np.meshgrid(np.arange(-<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>), np.arange(-<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>))
<span class="linenr">13: </span>  <span style="color: #dcaeea;">z</span> = x[<span style="color: #da8548; font-weight: bold;">0</span>]**<span style="color: #da8548; font-weight: bold;">2</span> + x[<span style="color: #da8548; font-weight: bold;">1</span>]**<span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr">14: </span>
<span class="linenr">15: </span>  <span style="color: #dcaeea;">fig</span> = plt.figure()
<span class="linenr">16: </span>  <span style="color: #dcaeea;">ax</span> = Axes3D(fig)
<span class="linenr">17: </span>  ax.plot_wireframe(x[<span style="color: #da8548; font-weight: bold;">0</span>], x[<span style="color: #da8548; font-weight: bold;">1</span>], z)
<span class="linenr">18: </span>
<span class="linenr">19: </span>  plt.xlim(-<span style="color: #da8548; font-weight: bold;">3.5</span>, <span style="color: #da8548; font-weight: bold;">3.5</span>)
<span class="linenr">20: </span>  plt.ylim(-<span style="color: #da8548; font-weight: bold;">4.5</span>, <span style="color: #da8548; font-weight: bold;">4.5</span>)
<span class="linenr">21: </span>  plt.xlabel(<span style="color: #98be65;">"x0"</span>)
<span class="linenr">22: </span>  plt.ylabel(<span style="color: #98be65;">"x1"</span>)
<span class="linenr">23: </span>  plt.savefig(<span style="color: #98be65;">'multivariate_func.png'</span>)
</pre>
</div>


<div id="orgcfb506f" class="figure">
<p><img src="images/multivariate_func.png" alt="multivariate_func.png" /><br />
</p>
<p><span class="figure-number">Figure 47: </span>\(f(x_0,x_1)=x_0^2+x_1^2\)圖表</p>
</div>


<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30070;x0=3, x1=4&#26178;&#65292;&#23565;x0&#24494;&#20998;</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_diff</span>(f, x):
<span class="linenr"> 3: </span>      <span style="color: #dcaeea;">h</span> = 1e-<span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr"> 4: </span>      <span style="color: #51afef;">return</span> (f(x+h) - f(x-h)) / (<span style="color: #da8548; font-weight: bold;">2</span>*h)
<span class="linenr"> 5: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">function_mult1</span>(x0):
<span class="linenr"> 6: </span>      <span style="color: #51afef;">return</span> x0*x0 + <span style="color: #da8548; font-weight: bold;">4.0</span>**<span style="color: #da8548; font-weight: bold;">2.0</span>
<span class="linenr"> 7: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30070;x0=3, x1=4&#26178;&#65292;&#23565;x1&#24494;&#20998;</span>
<span class="linenr"> 8: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">function_mult2</span>(x1):
<span class="linenr"> 9: </span>      <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">3.0</span>**<span style="color: #da8548; font-weight: bold;">2.0</span> + x1*x1
<span class="linenr">10: </span>  <span style="color: #51afef;">print</span>(numerical_diff(function_mult1, <span style="color: #da8548; font-weight: bold;">3.0</span>))
<span class="linenr">11: </span>  <span style="color: #51afef;">print</span>(numerical_diff(function_mult2, <span style="color: #da8548; font-weight: bold;">4.0</span>))
</pre>
</div>

<pre class="example">
6.00000000000378
7.999999999999119
</pre>


<p>
上述程式碼中，function_mult1 是先將計算「將\(x_1\)固定為常數 4.0 的新函數」，對\(x_0\)進行微分；function_multi2 則是先將\(x_0\)固定為常數 3，再對\(x_1\)求出微分。<br />
</p>
</div>
</div>

<div id="outline-container-org85fe303" class="outline-3">
<h3 id="org85fe303"><span class="section-number-3">8.5</span> 梯度</h3>
<div class="outline-text-3" id="text-8-5">
<p>
若要一次計算出\(x_0\)與\(x_1\)的偏微分，可以計算\((\frac{\partial{f}}{\partial{x_0}},\frac{\partial{f}}{\partial{x_1}})\)，這種方式也叫梯度（gradient），對應之計算程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x0, x1) = x0**2 + x1**2&#12398;&#21246;&#37197;&#22259;&#12434;&#25551;&#12367;&#12469;&#12531;&#12503;&#12523;&#12467;&#12540;&#12489;&#23455;&#34892;</span>
<span class="linenr"> 2: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">cat gradient_2d_save.py</span>
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">coding: utf-8</span>
<span class="linenr"> 5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">cf.http://d.hatena.ne.jp/white_wheels/20100327/p3</span>
<span class="linenr"> 6: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 7: </span>  <span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr"> 8: </span>  plt.switch_backend(<span style="color: #98be65;">'agg'</span>)
<span class="linenr"> 9: </span>  <span style="color: #51afef;">from</span> mpl_toolkits.mplot3d <span style="color: #51afef;">import</span> Axes3D
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">_numerical_gradient_no_batch</span>(f, x):
<span class="linenr">12: </span>      <span style="color: #dcaeea;">h</span> = 1e-<span style="color: #da8548; font-weight: bold;">4</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">0.0001</span>
<span class="linenr">13: </span>      <span style="color: #dcaeea;">grad</span> = np.zeros_like(x) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29986;&#29983;&#21644;x&#30456;&#21516;&#24418;&#29376;&#12289;&#20839;&#23481;&#22343;&#28858;0&#30340;&#38499;&#21015; </span>
<span class="linenr">14: </span>
<span class="linenr">15: </span>      <span style="color: #51afef;">for</span> idx <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(x.size):
<span class="linenr">16: </span>          <span style="color: #dcaeea;">tmp_val</span> = x[idx]
<span class="linenr">17: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;f(x+h)</span>
<span class="linenr">18: </span>          <span style="color: #dcaeea;">x</span>[idx] = <span style="color: #c678dd;">float</span>(tmp_val) + h
<span class="linenr">19: </span>          <span style="color: #dcaeea;">fxh1</span> = f(x) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x+h)</span>
<span class="linenr">20: </span>          <span style="color: #dcaeea;">x</span>[idx] = tmp_val - h
<span class="linenr">21: </span>          <span style="color: #dcaeea;">fxh2</span> = f(x) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x-h)</span>
<span class="linenr">22: </span>          <span style="color: #dcaeea;">grad</span>[idx] = (fxh1 - fxh2) / (<span style="color: #da8548; font-weight: bold;">2</span>*h)
<span class="linenr">23: </span>
<span class="linenr">24: </span>          <span style="color: #dcaeea;">x</span>[idx] = tmp_val <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24674;&#24489;&#21407;&#20540;</span>
<span class="linenr">25: </span>      <span style="color: #51afef;">return</span> grad
<span class="linenr">26: </span>
<span id="coderef-NumerGrad" class="coderef-off"><span class="linenr">27: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_gradient</span>(f, X):</span>
<span class="linenr">28: </span>      <span style="color: #51afef;">if</span> X.ndim == <span style="color: #da8548; font-weight: bold;">1</span>:
<span class="linenr">29: </span>          <span style="color: #51afef;">return</span> _numerical_gradient_no_batch(f, X)
<span class="linenr">30: </span>      <span style="color: #51afef;">else</span>:
<span class="linenr">31: </span>          grad = np.zeros_like(X)
<span class="linenr">32: </span>          <span style="color: #51afef;">for</span> idx, x <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(X):
<span class="linenr">33: </span>              <span style="color: #dcaeea;">grad</span>[idx] = _numerical_gradient_no_batch(f, x)
<span class="linenr">34: </span>          <span style="color: #51afef;">return</span> grad
<span class="linenr">35: </span>
<span class="linenr">36: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">function_2</span>(x):
<span class="linenr">37: </span>      <span style="color: #51afef;">if</span> x.ndim == <span style="color: #da8548; font-weight: bold;">1</span>:
<span class="linenr">38: </span>          <span style="color: #51afef;">return</span> np.<span style="color: #c678dd;">sum</span>(x**<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">39: </span>      <span style="color: #51afef;">else</span>:
<span class="linenr">40: </span>          <span style="color: #51afef;">return</span> np.<span style="color: #c678dd;">sum</span>(x**<span style="color: #da8548; font-weight: bold;">2</span>, axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">41: </span>
<span class="linenr">42: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">tangent_line</span>(f, x):
<span class="linenr">43: </span>      <span style="color: #dcaeea;">d</span> = numerical_gradient(f, x)
<span class="linenr">44: </span>      <span style="color: #51afef;">print</span>(d)
<span class="linenr">45: </span>      <span style="color: #dcaeea;">y</span> = f(x) - d*x
<span class="linenr">46: </span>      <span style="color: #51afef;">return</span> <span style="color: #51afef;">lambda</span> t: d*t + y
<span class="linenr">47: </span>
<span class="linenr">48: </span>  <span style="color: #dcaeea;">x0</span> = np.arange(-<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2.5</span>, <span style="color: #da8548; font-weight: bold;">0.25</span>)
<span class="linenr">49: </span>  <span style="color: #dcaeea;">x1</span> = np.arange(-<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2.5</span>, <span style="color: #da8548; font-weight: bold;">0.25</span>)
<span class="linenr">50: </span>  <span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">Y</span> = np.meshgrid(x0, x1)
<span class="linenr">51: </span>  <span style="color: #dcaeea;">X</span> = X.flatten()
<span class="linenr">52: </span>  <span style="color: #dcaeea;">Y</span> = Y.flatten()
<span class="linenr">53: </span>  <span style="color: #dcaeea;">grad</span> = numerical_gradient(function_2, np.array([X, Y]) )
<span class="linenr">54: </span>  plt.figure()
<span class="linenr">55: </span>  plt.quiver(X, Y, -grad[<span style="color: #da8548; font-weight: bold;">0</span>], -grad[<span style="color: #da8548; font-weight: bold;">1</span>],  angles=<span style="color: #98be65;">"xy"</span>,color=<span style="color: #98be65;">"#666666"</span>)
<span class="linenr">56: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">,headwidth=10,scale=40,color="#444444")</span>
<span class="linenr">57: </span>  plt.xlim([-<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>])
<span class="linenr">58: </span>  plt.ylim([-<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>])
<span class="linenr">59: </span>  plt.xlabel(<span style="color: #98be65;">'x0'</span>)
<span class="linenr">60: </span>  plt.ylabel(<span style="color: #98be65;">'x1'</span>)
<span class="linenr">61: </span>  plt.grid()
<span class="linenr">62: </span>  plt.legend()
<span class="linenr">63: </span>  plt.draw()
<span class="linenr">64: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">65: </span>  plt.savefig(<span style="color: #98be65;">'gradient_2d.png'</span>)
<span class="linenr">66: </span>  <span style="color: #51afef;">print</span>(numerical_gradient(function_2, np.array([<span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">4.0</span>])))
<span class="linenr">67: </span>  <span style="color: #51afef;">print</span>(numerical_gradient(function_2, np.array([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">2.0</span>])))
<span class="linenr">68: </span>  <span style="color: #51afef;">print</span>(numerical_gradient(function_2, np.array([<span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>])))
</pre>
</div>

<p>
上述程式碼中，函數 numerical\textunderscore{}gradient 以函數 function\textunderscore{}2 以及陣列 x 為參數（程式碼第<a href="#coderef-NumerGrad" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-NumerGrad');" onmouseout="CodeHighlightOff(this, 'coderef-NumerGrad');">27</a>行），針對陣列 x 的各元素計算數值微分，計算(3, 4)、(0, 2)、(3, 0)各點的梯度結果如下：<br />
</p>
<pre class="example">
[6. 8.]
[0. 4.]
[6. 0.]
</pre>

<p>
如圖<a href="#org0213376">48</a>所示，所謂梯度，指的是函數\(f(x0,x1)\)的「最低位置（最小值）」，這裡的箭頭就如同羅盤，離「最低位置」越遠，箭頭越大。雖然圖<a href="#org0213376">48</a>中箭頭所指為最低位置，但實際上不一定如此，梯度所指其實為各點之最低方向，其數學意義為：函數值減少最多的方向。<br />
</p>

<div id="org0213376" class="figure">
<p><img src="images/gradient_2d.png" alt="gradient_2d.png" /><br />
</p>
<p><span class="figure-number">Figure 48: </span>\(f(x_0,x_1)=x_0^2+x_1^2\)之梯度圖形</p>
</div>
</div>


<ol class="org-ol">
<li><a id="org4038d57"></a>梯度法<br />
<div class="outline-text-4" id="text-8-5-1">
<p>
神經網路的學習過程在於尋找最佳參數，這裡的意義也可說成：尋找可以讓損失函數為最小值的參數，以梯度來達成此目的的方法即為梯度法（gradient method），而最小值稱為鞍點（saddle point），鞍點是相對意義的存在，即，由某些方向看來是極大，但由其他方向看來為極小值，梯度法目的在找出梯度為 0 的位置，但不見的是極小值，當函數形成複雜扭曲形狀時，可能進入幾乎平坦而無法繼繼續學習的狀態，稱為「停滯期」。<br />
由此觀之，梯度法的精神即在於「朝著正確的方向前進一段距離、重新計算出正確方向、再前進一段距離&#x2026;.」。同樣的方法可以用來找出極小值（此時為梯度下降法，gradient descent method)，也能用來找出極大值（梯度上升法，gradient ascent method）。<br />
若以公式來顯示梯度法，其結果如公式\eqref{orgc1229e5}所示：<br />
</p>
\begin{equation}
\label{orgc1229e5}
\begin{split}
&x_0=x_0-\eta\frac{\partial{f}}{\partial{x_0}}\\
&x_1=x_1-\eta\frac{\partial{f}}{\partial{x_1}}
\end{split}
\end{equation}
<p>
公式\eqref{orgc1229e5}中的\(\eta\)代表更新的量，在神經網路中稱為學習率（learning rate），即，在一次學習中，要學習多少，要更新多少參數。公式\eqref{orgc1229e5}為一次更新的內容，或者，我們也可以看成「每次朝著正確方向前進的距離」。以 Python 實作的梯度下降法如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_gradient</span>(f, x): <span style="color: #5B6268;">#</span><span style="color: #5B6268;">f &#28858;&#35201;&#36914;&#34892;&#26368;&#20339;&#21270;&#30340;&#30446;&#27161;&#20989;&#25976;</span>
<span class="linenr"> 3: </span>      <span style="color: #dcaeea;">h</span> = 1e-<span style="color: #da8548; font-weight: bold;">4</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">0.0001</span>
<span class="linenr"> 4: </span>      <span style="color: #dcaeea;">grad</span> = np.zeros_like(x)
<span class="linenr"> 5: </span>      <span style="color: #51afef;">for</span> idx <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(x.size):
<span class="linenr"> 6: </span>          <span style="color: #dcaeea;">tmp_val</span> = x[idx]
<span class="linenr"> 7: </span>          <span style="color: #dcaeea;">x</span>[idx] = tmp_val + h <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">&#35336;&#31639;f(x+h)</span>
<span class="linenr"> 8: </span>          <span style="color: #dcaeea;">fxh1</span> = f(x)
<span class="linenr"> 9: </span>          <span style="color: #dcaeea;">x</span>[idx] = tmp_val - h <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;(x-h)</span>
<span class="linenr">10: </span>          <span style="color: #dcaeea;">fxh2</span> = f(x)
<span class="linenr">11: </span>          <span style="color: #dcaeea;">grad</span>[idx] = (fxh1 - fxh2) / (<span style="color: #da8548; font-weight: bold;">2</span>*h)
<span class="linenr">12: </span>          <span style="color: #dcaeea;">x</span>[idx] = tmp_val <span style="color: #5B6268;"># </span><span style="color: #5B6268;">restore original value</span>
<span class="linenr">13: </span>      <span style="color: #51afef;">return</span> grad
<span class="linenr">14: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">gradient_descent</span>(f, init_x, lr=<span style="color: #da8548; font-weight: bold;">0.01</span>, step_num=<span style="color: #da8548; font-weight: bold;">100</span>):
<span class="linenr">15: </span>      <span style="color: #dcaeea;">x</span> = init_x
<span class="linenr">16: </span>      <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(step_num):
<span class="linenr">17: </span>          <span style="color: #dcaeea;">grad</span> = numerical_gradient(f, x)
<span class="linenr">18: </span>          <span style="color: #dcaeea;">x</span> -= lr * grad
<span class="linenr">19: </span>      <span style="color: #51afef;">return</span> x
<span class="linenr">20: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">function_2</span>(x):
<span class="linenr">21: </span>      <span style="color: #51afef;">return</span> x[<span style="color: #da8548; font-weight: bold;">0</span>]**<span style="color: #da8548; font-weight: bold;">2</span> + x[<span style="color: #da8548; font-weight: bold;">1</span>]**<span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr">22: </span>  <span style="color: #dcaeea;">init_x</span> = np.array([-<span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">4.0</span>])
<span class="linenr">23: </span>  <span style="color: #51afef;">print</span>(gradient_descent(function_2, init_x=init_x, lr=<span style="color: #da8548; font-weight: bold;">0.1</span>, step_num=<span style="color: #da8548; font-weight: bold;">100</span>))
<span class="linenr">24: </span>  <span style="color: #dcaeea;">init_x</span> = np.array([<span style="color: #da8548; font-weight: bold;">20.0</span>, -<span style="color: #da8548; font-weight: bold;">30.0</span>])
<span class="linenr">25: </span>  <span style="color: #51afef;">print</span>(gradient_descent(function_2, init_x=init_x, lr=<span style="color: #da8548; font-weight: bold;">0.1</span>, step_num=<span style="color: #da8548; font-weight: bold;">100</span>))
<span class="linenr">26: </span>  <span style="color: #dcaeea;">init_x</span> = np.array([<span style="color: #da8548; font-weight: bold;">220.0</span>, -<span style="color: #da8548; font-weight: bold;">330.0</span>])
<span class="linenr">27: </span>  <span style="color: #51afef;">print</span>(gradient_descent(function_2, init_x=init_x, lr=<span style="color: #da8548; font-weight: bold;">0.1</span>, step_num=<span style="color: #da8548; font-weight: bold;">100</span>))
<span class="linenr">28: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#23416;&#32722;&#29575;&#35373;&#28858;10.0"</span>)
<span class="linenr">29: </span>  <span style="color: #dcaeea;">init_x</span> = np.array([-<span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">4.0</span>])
<span class="linenr">30: </span>  <span style="color: #51afef;">print</span>(gradient_descent(function_2, init_x=init_x, lr=<span style="color: #da8548; font-weight: bold;">10.0</span>, step_num=<span style="color: #da8548; font-weight: bold;">100</span>))
<span class="linenr">31: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#23416;&#32722;&#29575;&#35373;&#28858;1e-10"</span>)
<span class="linenr">32: </span>  <span style="color: #dcaeea;">init_x</span> = np.array([-<span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">4.0</span>])
<span class="linenr">33: </span>  <span style="color: #51afef;">print</span>(gradient_descent(function_2, init_x=init_x, lr=1e-<span style="color: #da8548; font-weight: bold;">10</span>, step_num=<span style="color: #da8548; font-weight: bold;">100</span>))
</pre>
</div>

<pre class="example">
[-6.11110793e-10  8.14814391e-10]
[ 4.07407195e-09 -6.11110793e-09]
[ 4.48147915e-08 -6.72221872e-08]
學習率設為 10.0
[-2.58983747e+13 -1.29524862e+12]
學習率設為 1e-10
[-2.99999994  3.99999992]
</pre>


<p>
上述程式中，init_x 為預設值、lr 代表 learning rate、step\textunderscore{}num 為重複次數，梯度由 numerical\textunderscore{}gradient(f,x)計算，程式執行結果為函數\(f(x_0,x_1)=x_0^2+x_1^2\)的最小值，預設值為\(x_0=-3,x_1=4\)，以梯度法求最小值，最後結果為(-6.1e-10,-8.1-10)，趨近於真實答案(0,0)，即使以其他預設值做為起點，所找到的最小值仍趨近正確答案, 而學習率太大或太小均無法得到良好的結果（學習率太大則會往大數值擴散；學習率太小則幾乎不更新就結束）。至於學習率這種依靠人工進行設定的值稱為超參數（hyperparameter），其最佳設定有賴以各種測試取得。至於梯度法的學習流程則如圖<a href="#org18a1a6f">49</a>所示。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">cat gradient_method_save.py</span>
<span class="linenr"> 2: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">coding: utf-8</span>
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr"> 5: </span>  plt.switch_backend(<span style="color: #98be65;">'agg'</span>)
<span class="linenr"> 6: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_gradient</span>(f, x): <span style="color: #5B6268;">#</span><span style="color: #5B6268;">f &#28858;&#35201;&#36914;&#34892;&#26368;&#20339;&#21270;&#30340;&#30446;&#27161;&#20989;&#25976;</span>
<span class="linenr"> 7: </span>      <span style="color: #dcaeea;">h</span> = 1e-<span style="color: #da8548; font-weight: bold;">4</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">0.0001</span>
<span class="linenr"> 8: </span>      <span style="color: #dcaeea;">grad</span> = np.zeros_like(x)
<span class="linenr"> 9: </span>      <span style="color: #51afef;">for</span> idx <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(x.size):
<span class="linenr">10: </span>          <span style="color: #dcaeea;">tmp_val</span> = x[idx]
<span class="linenr">11: </span>          <span style="color: #dcaeea;">x</span>[idx] = tmp_val + h <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">&#35336;&#31639;f(x+h)</span>
<span class="linenr">12: </span>          <span style="color: #dcaeea;">fxh1</span> = f(x)
<span class="linenr">13: </span>          <span style="color: #dcaeea;">x</span>[idx] = tmp_val - h <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;(x-h)</span>
<span class="linenr">14: </span>          <span style="color: #dcaeea;">fxh2</span> = f(x)
<span class="linenr">15: </span>          <span style="color: #dcaeea;">grad</span>[idx] = (fxh1 - fxh2) / (<span style="color: #da8548; font-weight: bold;">2</span>*h)
<span class="linenr">16: </span>          <span style="color: #dcaeea;">x</span>[idx] = tmp_val <span style="color: #5B6268;"># </span><span style="color: #5B6268;">restore original value</span>
<span class="linenr">17: </span>      <span style="color: #51afef;">return</span> grad
<span class="linenr">18: </span>
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">gradient_descent</span>(f, init_x, lr=<span style="color: #da8548; font-weight: bold;">0.01</span>, step_num=<span style="color: #da8548; font-weight: bold;">100</span>):
<span class="linenr">21: </span>      <span style="color: #dcaeea;">x</span> = init_x
<span class="linenr">22: </span>      <span style="color: #dcaeea;">x_history</span> = []
<span class="linenr">23: </span>
<span class="linenr">24: </span>      <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(step_num):
<span class="linenr">25: </span>          x_history.append( x.copy() )
<span class="linenr">26: </span>
<span class="linenr">27: </span>          <span style="color: #dcaeea;">grad</span> = numerical_gradient(f, x)
<span class="linenr">28: </span>          <span style="color: #dcaeea;">x</span> -= lr * grad
<span class="linenr">29: </span>
<span class="linenr">30: </span>      <span style="color: #51afef;">return</span> x, np.array(x_history)
<span class="linenr">31: </span>
<span class="linenr">32: </span>
<span class="linenr">33: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">function_2</span>(x):
<span class="linenr">34: </span>      <span style="color: #51afef;">return</span> x[<span style="color: #da8548; font-weight: bold;">0</span>]**<span style="color: #da8548; font-weight: bold;">2</span> + x[<span style="color: #da8548; font-weight: bold;">1</span>]**<span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr">35: </span>
<span class="linenr">36: </span>  <span style="color: #dcaeea;">init_x</span> = np.array([-<span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">4.0</span>])
<span class="linenr">37: </span>
<span class="linenr">38: </span>  <span style="color: #dcaeea;">lr</span> = <span style="color: #da8548; font-weight: bold;">0.1</span>
<span class="linenr">39: </span>  <span style="color: #dcaeea;">step_num</span> = <span style="color: #da8548; font-weight: bold;">20</span>
<span class="linenr">40: </span>  <span style="color: #dcaeea;">x</span>, <span style="color: #dcaeea;">x_history</span> = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)
<span class="linenr">41: </span>
<span class="linenr">42: </span>  plt.plot( [-<span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">5</span>], [<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #98be65;">'--b'</span>)
<span class="linenr">43: </span>  plt.plot( [<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">0</span>], [-<span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">5</span>], <span style="color: #98be65;">'--b'</span>)
<span class="linenr">44: </span>  plt.plot(x_history[:,<span style="color: #da8548; font-weight: bold;">0</span>], x_history[:,<span style="color: #da8548; font-weight: bold;">1</span>], <span style="color: #98be65;">'o'</span>)
<span class="linenr">45: </span>
<span class="linenr">46: </span>  plt.xlim(-<span style="color: #da8548; font-weight: bold;">3.5</span>, <span style="color: #da8548; font-weight: bold;">3.5</span>)
<span class="linenr">47: </span>  plt.ylim(-<span style="color: #da8548; font-weight: bold;">4.5</span>, <span style="color: #da8548; font-weight: bold;">4.5</span>)
<span class="linenr">48: </span>  plt.xlabel(<span style="color: #98be65;">"X0"</span>)
<span class="linenr">49: </span>  plt.ylabel(<span style="color: #98be65;">"X1"</span>)
<span class="linenr">50: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">51: </span>  plt.savefig(<span style="color: #98be65;">'gradient_process_method.png'</span>)
<span class="linenr">52: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">return "gradient_process_method.png"</span>
</pre>
</div>

<div id="org18a1a6f" class="figure">
<p><img src="images/gradient_process_method.png" alt="gradient_process_method.png" /><br />
</p>
<p><span class="figure-number">Figure 49: </span>\(f(x_0,x_1)=x_0^2+x_1^2\)的梯度法更新過程</p>
</div>
</div>
</li>

<li><a id="org6141c82"></a>神經網路的梯度<br />
<div class="outline-text-4" id="text-8-5-2">
<p>
神經網路的梯度指與權重參數有關的損失函數梯度，若有個形狀為\(2*3\)、權重為\(W\)的神經網路，以\(L\)代表損失函數，則可以用\(\frac{\partial{L}}{\partial{W}}\)來表示梯度，其公式如下：<br />
</p>
\begin{equation}
\label{orgef46cd2}
\begin{split}
W= 
 \begin{pmatrix}
  w_{11} & w_{12} & w_{13} \\
  w_{21} & w_{22} & w_{23} 
 \end{pmatrix} \\
\frac{\partial{L}}{\partial{W}}= 
 \begin{pmatrix}
  \frac{\partial{L}}{\partial{w_{11}}} & \frac{\partial{L}}{\partial{w_{12}}} & \frac{\partial{L}}{\partial{w_{13}}} \\
  \frac{\partial{L}}{\partial{w_{21}}} & \frac{\partial{L}}{\partial{w_{22}}} & \frac{\partial{L}}{\partial{w_{23}}} 
 \end{pmatrix} \\
\end{splix}
\end{equation}
<p>
\(\frac{\partial{L}}{\partial{W}}\)的各元素係由該元素的偏微分構成，如\(\frac{\partial{L}}{\partial{w_{11}}}\)即為在略改變\(w_{11}\)後損失函數所造成的變化，神經網路計算梯度的方式如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">softmax</span>(x):
<span class="linenr"> 3: </span>      <span style="color: #51afef;">if</span> x.ndim == <span style="color: #da8548; font-weight: bold;">2</span>:
<span class="linenr"> 4: </span>          <span style="color: #dcaeea;">x</span> = x.T
<span class="linenr"> 5: </span>          <span style="color: #dcaeea;">x</span> = x - np.<span style="color: #c678dd;">max</span>(x, axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 6: </span>          <span style="color: #dcaeea;">y</span> = np.exp(x) / np.<span style="color: #c678dd;">sum</span>(np.exp(x), axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 7: </span>          <span style="color: #51afef;">return</span> y.T
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>      <span style="color: #dcaeea;">x</span> = x - np.<span style="color: #c678dd;">max</span>(x)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28322;&#20986;&#23545;&#31574;</span>
<span class="linenr">10: </span>      <span style="color: #51afef;">return</span> np.exp(x) / np.<span style="color: #c678dd;">sum</span>(np.exp(x))
<span class="linenr">11: </span>
<span class="linenr">12: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">cross_entropy_error</span>(y, t):
<span class="linenr">13: </span>      <span style="color: #51afef;">if</span> y.ndim == <span style="color: #da8548; font-weight: bold;">1</span>:
<span class="linenr">14: </span>          <span style="color: #dcaeea;">t</span> = t.reshape(<span style="color: #da8548; font-weight: bold;">1</span>, t.size)
<span class="linenr">15: </span>          <span style="color: #dcaeea;">y</span> = y.reshape(<span style="color: #da8548; font-weight: bold;">1</span>, y.size)
<span class="linenr">16: </span>
<span class="linenr">17: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30417;&#30563;&#25968;&#25454;&#26159;one-hot-vector&#30340;&#24773;&#20917;&#19979;&#65292;&#36716;&#25442;&#20026;&#27491;&#30830;&#35299;&#26631;&#31614;&#30340;&#32034;&#24341;</span>
<span class="linenr">18: </span>      <span style="color: #51afef;">if</span> t.size == y.<span style="color: #dcaeea;">size</span>:
<span class="linenr">19: </span>          t = t.argmax(axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">20: </span>
<span class="linenr">21: </span>      <span style="color: #dcaeea;">batch_size</span> = y.shape[<span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr">22: </span>      <span style="color: #51afef;">return</span> -np.<span style="color: #c678dd;">sum</span>(np.log(y[np.arange(batch_size), t] + 1e-<span style="color: #da8548; font-weight: bold;">7</span>)) / batch_size
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">_numerical_gradient_no_batch</span>(f, x):
<span class="linenr">25: </span>      <span style="color: #dcaeea;">h</span> = 1e-<span style="color: #da8548; font-weight: bold;">4</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">0.0001</span>
<span class="linenr">26: </span>      <span style="color: #dcaeea;">grad</span> = np.zeros_like(x) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29986;&#29983;&#21644;x&#30456;&#21516;&#24418;&#29376;&#12289;&#20839;&#23481;&#22343;&#28858;0&#30340;&#38499;&#21015; </span>
<span class="linenr">27: </span>
<span class="linenr">28: </span>      <span style="color: #51afef;">for</span> idx <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(x.size):
<span class="linenr">29: </span>          <span style="color: #dcaeea;">tmp_val</span> = x[idx]
<span class="linenr">30: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;f(x+h)</span>
<span class="linenr">31: </span>          <span style="color: #dcaeea;">x</span>[idx] = <span style="color: #c678dd;">float</span>(tmp_val) + h
<span class="linenr">32: </span>          <span style="color: #dcaeea;">fxh1</span> = f(x) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x+h)</span>
<span class="linenr">33: </span>          <span style="color: #dcaeea;">x</span>[idx] = tmp_val - h
<span class="linenr">34: </span>          <span style="color: #dcaeea;">fxh2</span> = f(x) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x-h)</span>
<span class="linenr">35: </span>          <span style="color: #dcaeea;">grad</span>[idx] = (fxh1 - fxh2) / (<span style="color: #da8548; font-weight: bold;">2</span>*h)
<span class="linenr">36: </span>
<span class="linenr">37: </span>          <span style="color: #dcaeea;">x</span>[idx] = tmp_val <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24674;&#24489;&#21407;&#20540;</span>
<span class="linenr">38: </span>      <span style="color: #51afef;">return</span> grad
<span class="linenr">39: </span>
<span id="coderef-NumerGrad" class="coderef-off"><span class="linenr">40: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_gradient</span>(f, X):</span>
<span class="linenr">41: </span>      <span style="color: #51afef;">if</span> X.ndim == <span style="color: #da8548; font-weight: bold;">1</span>:
<span class="linenr">42: </span>          <span style="color: #51afef;">return</span> _numerical_gradient_no_batch(f, X)
<span class="linenr">43: </span>      <span style="color: #51afef;">else</span>:
<span class="linenr">44: </span>          grad = np.zeros_like(X)
<span class="linenr">45: </span>          <span style="color: #51afef;">for</span> idx, x <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(X):
<span class="linenr">46: </span>              <span style="color: #dcaeea;">grad</span>[idx] = _numerical_gradient_no_batch(f, x)
<span class="linenr">47: </span>          <span style="color: #51afef;">return</span> grad
<span class="linenr">48: </span>
<span class="linenr">49: </span>  <span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">simpleNet</span>:
<span class="linenr">50: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>):
<span class="linenr">51: </span>          <span style="color: #51afef;">self</span>.W = np.random.randn(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>)  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20197;&#24120;&#24907;&#20998;&#20296;&#21021;&#22987;&#21270;&#27402;&#37325;&#20998;&#37197;</span>
<span class="linenr">52: </span>
<span class="linenr">53: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">predict</span>(<span style="color: #51afef;">self</span>, x):
<span class="linenr">54: </span>          <span style="color: #51afef;">return</span> np.dot(x, <span style="color: #51afef;">self</span>.W)
<span class="linenr">55: </span>
<span class="linenr">56: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">loss</span>(<span style="color: #51afef;">self</span>, x, t):
<span class="linenr">57: </span>          <span style="color: #dcaeea;">z</span> = <span style="color: #51afef;">self</span>.predict(x)
<span class="linenr">58: </span>          <span style="color: #dcaeea;">y</span> = softmax(z)
<span class="linenr">59: </span>          <span style="color: #dcaeea;">loss</span> = cross_entropy_error(y, t)
<span class="linenr">60: </span>          <span style="color: #51afef;">return</span> loss
<span class="linenr">61: </span>
<span class="linenr">62: </span>  <span style="color: #dcaeea;">net</span> = simpleNet()
<span class="linenr">63: </span>  <span style="color: #51afef;">print</span>(net.W)
<span class="linenr">64: </span>  <span style="color: #dcaeea;">x</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.6</span>, <span style="color: #da8548; font-weight: bold;">0.9</span>])
<span class="linenr">65: </span>  <span style="color: #dcaeea;">p</span> = net.predict(x)
<span class="linenr">66: </span>  <span style="color: #51afef;">print</span>(p)
<span class="linenr">67: </span>  <span style="color: #dcaeea;">t</span> = np.array([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">68: </span>  <span style="color: #51afef;">print</span>(net.loss(x, t))
<span class="linenr">69: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"========================="</span>)
<span class="linenr">70: </span>  <span style="color: #dcaeea;">f</span> = <span style="color: #51afef;">lambda</span> <span style="color: #dcaeea;">w</span>: net.loss(x,t)
<span class="linenr">71: </span>  dW = numerical_gradient(f, net.W)
<span class="linenr">72: </span>  <span style="color: #51afef;">print</span>(dW)
</pre>
</div>

<pre class="example">
[[-0.17058278  0.11372612 -1.53368818]
 [ 0.19931583 -0.60202593  0.07473724]]
[ 0.07703458 -0.47358766 -0.85294939]
1.6086010849717751
=========================
[[ 0.30439054  0.17550882 -0.47989936]
 [ 0.45658581  0.26326323 -0.71984904]]
</pre>


<p>
simpleNet 以一個\(2*3\)的陣列做為權重參數 W，x 為輸入資料，t為正確答案標籤，此處我們希望針對 loss 函數進行梯度法取得最小值，所以定義一個函數 f，代入 x 及 t 傳回損失函數，然後將函數 f 做為參數傳給 numerical\textunderscore{}gradient 進行梯度下降運算，由運算結果 dW 的陣列內容可以看出，每增加一個\(h\)，\(w_{11}\)就增加 0.45、而\(w_{23}\)則下降 0.72。由此看來，為了要減少損失函數，應該持續增加\(w_{11}\)、減少\(w{23}\)，而且，\(w{23}\)的貢獻最大。<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgd64fa0d" class="outline-3">
<h3 id="orgd64fa0d"><span class="section-number-3">8.6</span> 學習演算法</h3>
<div class="outline-text-3" id="text-8-6">
<p>
至此，神經網路的學習步驟大致如下：<br />
</p>
<ol class="org-ol">
<li>小批次: 即從訓練資料中所隨機挑選部份數據<br /></li>
<li>計算梯度: 然後朝著「減少小批次損失函數」為目標前進<br /></li>
<li>更新參數: 每前進一步，就更新參數<br /></li>
<li>回到步驟 1<br /></li>
</ol>

<p>
上述作法稱之為「準確率梯度下降法(<i>stochastic gradient descent</i>)」，意味著「針對所選出的資料進行梯度下降」，一般以 SGD 為名。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org03b8b54"></a>Gradient Descent<br />
<ol class="org-ol">
<li><a id="orgb59ffe1"></a>Watch: <a href="https://www.youtube.com/watch?v=yKKNr-QKz2Q">https://www.youtube.com/watch?v=yKKNr-QKz2Q</a><br /></li>
<li><a id="org4c58008"></a>Gradient Descent (GD)<br /></li>
<li><a id="org55e35f0"></a>Batch Gradient Descent (BGD)<br /></li>
<li><a id="org1702e45"></a>Mini-Batch Gradient Descent (MBGD)<br /></li>
<li><a id="orgad8a4e3"></a>Stochastic Gradient Descent (SGD): 亂數選一個<br />
<div class="outline-text-5" id="text-8-6-1-5">
<ul class="org-ul">
<li>\( L=\sum_n{(\hat y ^n - (b+\sum {w_ix_i^n)})}^2\)<br /></li>
<li>Gradient Descent: \( \theta ^i = \theta ^{i-1} - \eta \triangledown L(\theta {i-1}) \)<br /></li>
<li>Stochastic Gradient Descent: \( L^n = (\hat y ^n - (b+\sum {w_ix_i^n)})}^2, \theta ^i=\theta ^{i-1} - \eta \triangledown L(\theta {i-1}) \)<br /></li>
</ul>
</div>
</li>
</ol>
</li>

<li><a id="org0949a44"></a>優化器演算法 Optimizer<br />
<ol class="org-ol">
<li><a id="orgbad971c"></a>Momentum<br />
<div class="outline-text-5" id="text-8-6-2-1">
<p>
主要是用在計算參數更新方向前會考慮前一次參數更新的方向(<br />
</p>
</div>
</li>

<li><a id="orgb45b453"></a>Adaptive Learning Rates<br />
<ol class="org-ol">
<li><a id="orgab135d0"></a>Reduce the learning rate by some factor every few epochs.<br />
<div class="outline-text-6" id="text-8-6-2-2-1">
<ul class="org-ul">
<li>At the beginning, we are far from the destination, so we use larger learning rate<br /></li>
<li>After several epochs, we are close to the destination, so we reduce the learning rate<br /></li>
<li>e.g. \(\frac{1}{t}\)decay: \(\eta^t=\frac{\eta}{\sqrt{t+1}} \)<br /></li>
</ul>
</div>
</li>
<li><a id="org098feb6"></a>Learning cannot be on-size-fits-all<br />
<div class="outline-text-6" id="text-8-6-2-2-2">
<ul class="org-ul">
<li>Given different parameters different learning rate<br /></li>
</ul>
</div>
</li>
</ol>
</li>
<li><a id="org1d0c766"></a>Adaptive Learning Rates 範例<br />
<ol class="org-ol">
<li><a id="org62517eb"></a>Adagrad (John Duchi, JMLR&rsquo;11)<br />
<div class="outline-text-6" id="text-8-6-2-3-1">
<ul class="org-ul">
<li>Original: \( w \leftarrow w - \frac{\eta\partial L}{\partial w} \)<br /></li>
<li>Adagrad: \( w \leftarrow - \frac{\eta_w\partial L}{\partial w}, where \partial_w = \frac{\eta}{\sqrt{\sum^t_i=0 (g^i)^2}} \), \(g^i\)is \(\frac{\partial L}{\partial w}\)obtained at the i-th update, and \(\partial_w\)means parameter dependent learning rate.<br /></li>
</ul>
</div>
</li>
<li><a id="org3dedbb5"></a>RMSprop<br />
<div class="outline-text-6" id="text-8-6-2-3-2">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=O3sxAc4hxZU">https://www.youtube.com/watch?v=O3sxAc4hxZU</a><br /></li>
</ul>
</div>
</li>
<li><a id="orgb45e481"></a>Adadelta (Metthew D. Zeiler, arXiv&rsquo;12)<br /></li>
<li><a id="org2eaa89a"></a>&ldquo;No more pesky learning rates&rdquo; (Tom Schaul, arXiv&rsquo;12)<br /></li>
<li><a id="org0b74d3e"></a>AdaSecant (Caglar Gulcehre, arXiv&rsquo;14)<br /></li>
<li><a id="org5df84bf"></a>Adam (Diederik P. Kingma, ICLR&rsquo;15)<br />
<div class="outline-text-6" id="text-8-6-2-3-6">
<ul class="org-ul">
<li>Momentum: 計算參數更新方向前會考慮前一次參數更新的方向<br /></li>
<li>RMSprop: 在學習率上依據梯度的大小對學習率進行加強或是衰減<br /></li>
<li>Adam: 合併上述，各自做偏差的修正<br /></li>
<li>Video: <a href="https://www.youtube.com/watch?v=_JB0AO7QxSA">Standfard: Lecture 7 | Training Neural Networks II</a><br /></li>
</ul>
</div>
</li>
<li><a id="orgf2cb64d"></a>Nadam<br />
<div class="outline-text-6" id="text-8-6-2-3-7">
<ul class="org-ul">
<li><a href="http://cs229.standford.edu/proj2015/054_report.pdf">http://cs229.standford.edu/proj2015/054_report.pdf</a><br /></li>
</ul>
</div>
</li>
</ol>
</li>
</ol>
</li>

<li><a id="org289dc36"></a>雙層神經網路的類別<br />
<div class="outline-text-4" id="text-8-6-3">
<p>
以下以一個雙層神經網路的實作來進行說明：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">coding: utf-8</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> sys, os
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 4: </span>  sys.path.append(os.pardir) 
<span class="linenr"> 5: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">from common.functions import *</span>
<span class="linenr"> 6: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">from common.gradient import numerical_gradient</span>
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">sigmoid</span>(x):
<span class="linenr"> 9: </span>      <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span> / (<span style="color: #da8548; font-weight: bold;">1</span> + np.exp(-x))
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">softmax</span>(a):
<span id="coderef-softmax-overflow" class="coderef-off"><span class="linenr">12: </span>      <span style="color: #dcaeea;">exp_a</span> = np.exp(a)</span>
<span id="coderef-softmax-overflow2" class="coderef-off"><span class="linenr">13: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">exp_a = np.exp(a - np.max(a))</span></span>
<span class="linenr">14: </span>      <span style="color: #dcaeea;">sum_exp_a</span> = np.<span style="color: #c678dd;">sum</span>(exp_a) 
<span class="linenr">15: </span>      <span style="color: #dcaeea;">y</span> = exp_a / sum_exp_a
<span class="linenr">16: </span>      <span style="color: #51afef;">return</span>(y)
<span class="linenr">17: </span>
<span class="linenr">18: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">cross_entropy_error</span>(y, t):
<span class="linenr">19: </span>      <span style="color: #dcaeea;">delta</span> = 1e-<span style="color: #da8548; font-weight: bold;">7</span>
<span class="linenr">20: </span>      <span style="color: #51afef;">return</span> -np.<span style="color: #c678dd;">sum</span>(t * np.log(y + delta))
<span class="linenr">21: </span>
<span class="linenr">22: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_gradient</span>(f, x):
<span class="linenr">23: </span>      <span style="color: #dcaeea;">h</span> = 1e-<span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr">24: </span>      <span style="color: #dcaeea;">grad</span> = np.zeros_like(x)
<span class="linenr">25: </span>      <span style="color: #dcaeea;">it</span> = np.nditer(x, flags=[<span style="color: #98be65;">'multi_index'</span>], op_flags=[<span style="color: #98be65;">'readwrite'</span>])
<span class="linenr">26: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25976;&#20540;&#24494;&#20998;&#35336;&#31639;&#21443;&#25976;&#26799;&#24230;</span>
<span class="linenr">27: </span>      <span style="color: #51afef;">while</span> <span style="color: #51afef;">not</span> it.<span style="color: #dcaeea;">finished</span>:
<span class="linenr">28: </span>          idx = it.multi_index
<span class="linenr">29: </span>          <span style="color: #dcaeea;">tmp_val</span> = x[idx]
<span class="linenr">30: </span>          <span style="color: #dcaeea;">x</span>[idx] = <span style="color: #c678dd;">float</span>(tmp_val) + h
<span class="linenr">31: </span>          <span style="color: #dcaeea;">fxh1</span> = f(x)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x+h)</span>
<span class="linenr">32: </span>
<span class="linenr">33: </span>          <span style="color: #dcaeea;">x</span>[idx] = tmp_val - h
<span class="linenr">34: </span>          <span style="color: #dcaeea;">fxh2</span> = f(x)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x-h)</span>
<span class="linenr">35: </span>          <span style="color: #dcaeea;">grad</span>[idx] = (fxh1 - fxh2) / (<span style="color: #da8548; font-weight: bold;">2</span>*h)
<span class="linenr">36: </span>
<span class="linenr">37: </span>          <span style="color: #dcaeea;">x</span>[idx] = tmp_val  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20516;&#12434;&#20803;&#12395;&#25147;&#12377;</span>
<span class="linenr">38: </span>          it.iternext()
<span class="linenr">39: </span>      <span style="color: #51afef;">return</span> grad
<span class="linenr">40: </span>
<span class="linenr">41: </span>  <span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">TwoLayerNet</span>:
<span id="coderef-TLN-init" class="coderef-off"><span class="linenr">42: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>, input_size, hidden_size, output_size, weight_init_std=<span style="color: #da8548; font-weight: bold;">0.01</span>):</span>
<span class="linenr">43: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38568;&#27231;&#20998;&#37197;weight, bias&#35373;&#28858;0</span>
<span id="coderef-TLN-params" class="coderef-off"><span class="linenr">44: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">params</span> = {}</span>
<span class="linenr">45: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">params</span>[<span style="color: #98be65;">'W1'</span>] = weight_init_std * np.random.randn(input_size, hidden_size)
<span class="linenr">46: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">params</span>[<span style="color: #98be65;">'b1'</span>] = np.zeros(hidden_size)
<span class="linenr">47: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">params</span>[<span style="color: #98be65;">'W2'</span>] = weight_init_std * np.random.randn(hidden_size, output_size)
<span class="linenr">48: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">params</span>[<span style="color: #98be65;">'b2'</span>] = np.zeros(output_size)
<span class="linenr">49: </span>
<span id="coderef-TLN-pred" class="coderef-off"><span class="linenr">50: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">predict</span>(<span style="color: #51afef;">self</span>, x):</span>
<span class="linenr">51: </span>          <span style="color: #dcaeea;">W1</span>, <span style="color: #dcaeea;">W2</span> = <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W1'</span>], <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W2'</span>]
<span class="linenr">52: </span>          <span style="color: #dcaeea;">b1</span>, <span style="color: #dcaeea;">b2</span> = <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b1'</span>], <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b2'</span>]
<span class="linenr">53: </span>
<span class="linenr">54: </span>          <span style="color: #dcaeea;">a1</span> = np.dot(x, W1) + b1
<span class="linenr">55: </span>          <span style="color: #dcaeea;">z1</span> = sigmoid(a1)
<span class="linenr">56: </span>          <span style="color: #dcaeea;">a2</span> = np.dot(z1, W2) + b2
<span class="linenr">57: </span>          <span style="color: #dcaeea;">y</span> = softmax(a2)
<span class="linenr">58: </span>          <span style="color: #51afef;">return</span> y
<span class="linenr">59: </span>
<span class="linenr">60: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">x : &#36664;&#20837;&#36039;&#26009;, t : &#35347;&#32244;&#36039;&#26009;</span>
<span id="coderef-TLN-loss" class="coderef-off"><span class="linenr">61: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">loss</span>(<span style="color: #51afef;">self</span>, x, t):</span>
<span class="linenr">62: </span>          y = <span style="color: #51afef;">self</span>.predict(x)
<span class="linenr">63: </span>          <span style="color: #51afef;">return</span> cross_entropy_error(y, t)
<span class="linenr">64: </span>
<span id="coderef-TLN-accu" class="coderef-off"><span class="linenr">65: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">accuracy</span>(<span style="color: #51afef;">self</span>, x, t):</span>
<span class="linenr">66: </span>          <span style="color: #dcaeea;">y</span> = <span style="color: #51afef;">self</span>.predict(x)
<span class="linenr">67: </span>          <span style="color: #dcaeea;">y</span> = np.argmax(y, axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">68: </span>          <span style="color: #dcaeea;">t</span> = np.argmax(t, axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">69: </span>          <span style="color: #dcaeea;">accuracy</span> = np.<span style="color: #c678dd;">sum</span>(y == t) / <span style="color: #c678dd;">float</span>(x.shape[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">70: </span>          <span style="color: #51afef;">return</span> accuracy
<span class="linenr">71: </span>
<span class="linenr">72: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35336;&#31639;&#21508;&#21443;&#25976;&#30340;&#26799;&#24230; </span>
<span id="coderef-TLN-numgrad" class="coderef-off"><span class="linenr">73: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_gradient</span>(<span style="color: #51afef;">self</span>, x, t):</span>
<span class="linenr">74: </span>          <span style="color: #dcaeea;">loss_W</span> = <span style="color: #51afef;">lambda</span> <span style="color: #dcaeea;">W</span>: <span style="color: #51afef;">self</span>.loss(x, t)
<span id="coderef-TLN-grads" class="coderef-off"><span class="linenr">75: </span>          grads = {}</span>
<span class="linenr">76: </span>          <span style="color: #dcaeea;">grads</span>[<span style="color: #98be65;">'W1'</span>] = numerical_gradient(loss_W, <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W1'</span>])
<span class="linenr">77: </span>          <span style="color: #dcaeea;">grads</span>[<span style="color: #98be65;">'b1'</span>] = numerical_gradient(loss_W, <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b1'</span>])
<span class="linenr">78: </span>          <span style="color: #dcaeea;">grads</span>[<span style="color: #98be65;">'W2'</span>] = numerical_gradient(loss_W, <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W2'</span>])
<span class="linenr">79: </span>          <span style="color: #dcaeea;">grads</span>[<span style="color: #98be65;">'b2'</span>] = numerical_gradient(loss_W, <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b2'</span>])
<span id="coderef-TLN-grad" class="coderef-off"><span class="linenr">80: </span>          <span style="color: #51afef;">return</span> grads</span>
<span class="linenr">81: </span>
<span class="linenr">82: </span>  <span style="color: #51afef;">if</span> <span style="color: #c678dd;">__name__</span> == <span style="color: #98be65;">"__main__"</span>:
<span class="linenr">83: </span>      <span style="color: #dcaeea;">net</span> = TwoLayerNet(input_size=<span style="color: #da8548; font-weight: bold;">784</span>, hidden_size=<span style="color: #da8548; font-weight: bold;">100</span>, output_size=<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">84: </span>      <span style="color: #dcaeea;">x</span> = np.random.rand(<span style="color: #da8548; font-weight: bold;">100</span>, <span style="color: #da8548; font-weight: bold;">784</span>)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29986;&#29983;100&#24373;&#34395;&#25836;&#36039;&#26009;,&#36664;&#20837;&#24433;&#20687;&#23610;&#21515;&#28858;28*28</span>
<span class="linenr">85: </span>      <span style="color: #dcaeea;">y</span> = net.predict(x)            
<span class="linenr">86: </span>      <span style="color: #dcaeea;">t</span> = np.random.rand(<span style="color: #da8548; font-weight: bold;">100</span>, <span style="color: #da8548; font-weight: bold;">10</span>)   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29986;&#29983;100&#20491;&#34395;&#25836;&#30340;&#27491;&#30906;&#31572;&#26696;&#27161;&#31844;,&#27599;&#20491;&#27161;&#31844;&#26377;10&#20491;&#39006;&#21029;</span>
<span class="linenr">87: </span>      <span style="color: #dcaeea;">rands</span> = net.numerical_gradient(x, t) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;&#26799;&#24230;</span>
<span class="linenr">88: </span>      <span style="color: #51afef;">print</span>(rands[<span style="color: #98be65;">'W1'</span>])
<span class="linenr">89: </span>      <span style="color: #51afef;">print</span>(rands[<span style="color: #98be65;">'b1'</span>])
<span class="linenr">90: </span>      <span style="color: #51afef;">print</span>(rands[<span style="color: #98be65;">'W2'</span>].shape)
<span class="linenr">91: </span>      <span style="color: #51afef;">print</span>(rands[<span style="color: #98be65;">'b2'</span>].shape)
</pre>
</div>

<pre class="example">
[[ 0.0226828   0.02099976  0.0072832  ... -0.01241413  0.01743683
   0.00993156]
 [ 0.01682113  0.02138894  0.00436742 ... -0.01206379  0.01297583
   0.01304035]
 [ 0.01799636  0.01291382  0.01139191 ... -0.01196413  0.00845844
   0.00695222]
 ...
 [ 0.01469396  0.01507025  0.01288363 ... -0.01490424  0.0104561
   0.01669903]
 [ 0.01329594  0.01683885  0.0005939  ... -0.01176064  0.01377726
   0.01745393]
 [ 0.00360546  0.02353488  0.00761169 ... -0.01557991  0.00776723
   0.01624418]]
[ 0.03290075  0.03490604  0.01284017 -0.01780122 -0.00668078  0.00027693
  0.07401821  0.02162429  0.03047587  0.02048195  0.01182109 -0.0355056
  0.02144739 -0.01236585  0.03770265 -0.01983078  0.03263528 -0.02679656
  0.01665097  0.03983289  0.01158896  0.00058352  0.01279443  0.01702231
  0.00547242  0.04439308 -0.06806267  0.02272559 -0.01069871 -0.01259218
  0.00342859 -0.03044122  0.02350026 -0.02573413 -0.00946125  0.00146879
 -0.03312301  0.01705135  0.01477106  0.00567291  0.04036002  0.00070219
 -0.02369712  0.06778704 -0.02958217  0.02881278  0.03482847  0.0426526
 -0.00032182 -0.00342597 -0.03101994  0.006919    0.00050134 -0.0290734
  0.02713591 -0.0429896  -0.03356116  0.04456035  0.03420876 -0.04981342
 -0.00948666  0.03227471  0.00046413 -0.00104241 -0.03213071 -0.07219453
  0.01028855 -0.00079787  0.00348281 -0.02041278  0.00601914 -0.00067298
 -0.01878876  0.02557294 -0.02868863 -0.03383165  0.03133762  0.01079506
 -0.02313358  0.02697757  0.01809054  0.03784744 -0.00824714  0.05378442
  0.01241693  0.03035761  0.00919517 -0.03753735 -0.06883637  0.00995454
  0.01601707 -0.02033777 -0.02233426  0.00668454  0.0055338  -0.00663979
  0.00126532 -0.02274279  0.02845813  0.02777448]
(100, 10)
(10,)
</pre>

<p>
上述程式變數補充說明如表<a href="#orgc3aa1fa">3</a>，<br />
</p>
<table id="orgc3aa1fa" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 3:</span> TwoLayerNet 變數說明</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">變數</th>
<th scope="col" class="org-left">行數</th>
<th scope="col" class="org-left">說明</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">params</td>
<td class="org-left"><a href="#coderef-TLN-params" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-TLN-params');" onmouseout="CodeHighlightOff(this, 'coderef-TLN-params');">44</a></td>
<td class="org-left">神經網路參數，為字典型態變數</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">params[&rsquo;W1&rsquo;]為第 1 層權重、params[&rsquo;b1&rsquo;]為第 1 層偏權值</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">grads</td>
<td class="org-left"><a href="#coderef-TLN-grads" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-TLN-grads');" onmouseout="CodeHighlightOff(this, 'coderef-TLN-grads');">75</a></td>
<td class="org-left">numerical_\textunderscore{}gradient 函數的傳回值，為字典型態變數</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">grads[&rsquo;W1&rsquo;]為第 1 層權重的梯度、grads[&rsquo;b1&rsquo;&rsquo;為第 1 層偏權值的梯度</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">input\textunderscore{}size</td>
<td class="org-left"><a href="#coderef-TLN-init" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-TLN-init');" onmouseout="CodeHighlightOff(this, 'coderef-TLN-init');">42</a></td>
<td class="org-left">輸入層神經元數量</td>
</tr>

<tr>
<td class="org-left">hidden\textunderscore{}size</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">隱藏層神經元數量</td>
</tr>

<tr>
<td class="org-left">output\textunderscore{}size</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">輸出層神經元數量</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">predict(self, x)</td>
<td class="org-left"><a href="#coderef-TLN-pred" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-TLN-pred');" onmouseout="CodeHighlightOff(this, 'coderef-TLN-pred');">50</a></td>
<td class="org-left">傳入輸入資料(x)為參數，進行辨識預測</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">loss(self, x, t)</td>
<td class="org-left"><a href="#coderef-TLN-loss" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-TLN-loss');" onmouseout="CodeHighlightOff(this, 'coderef-TLN-loss');">61</a></td>
<td class="org-left">傳入影像資料(x)、正確答案標籤(t)為參數，計算損失函數自然不是逐一計算</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">accuracy(self, x, t)</td>
<td class="org-left"><a href="#coderef-TLN-accu" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-TLN-accu');" onmouseout="CodeHighlightOff(this, 'coderef-TLN-accu');">65</a></td>
<td class="org-left">計算準確度</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">numericalk\textunderscore{}gradient</td>
<td class="org-left"><a href="#coderef-TLN-numgrad" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-TLN-numgrad');" onmouseout="CodeHighlightOff(this, 'coderef-TLN-numgrad');">73</a></td>
<td class="org-left">計算權重值及偏權值梯度</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">gradient(self, x, t)</td>
<td class="org-left"><a href="#coderef-TLN-grad" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-TLN-grad');" onmouseout="CodeHighlightOff(this, 'coderef-TLN-grad');">80</a></td>
<td class="org-left">同上，速度較快</td>
</tr>
</tbody>
</table>
<p>
雙層神經網路則如圖<a href="#orgf6fe0f5">50</a>所示<br />
</p>

<div id="orgf6fe0f5" class="figure">
<p><img src="images/tln.png" alt="tln.png" /><br />
</p>
<p><span class="figure-number">Figure 50: </span>雙層神經網路</p>
</div>
</div>
</li>

<li><a id="org0070d3c"></a>小批次學習<br />
<div class="outline-text-4" id="text-8-6-4">
<p>
以下以前節之 TwoLayerNet 類別、以小批次學習來解決 MNIST 手寫字母辨識問題。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  2: </span>  <span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr">  3: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr">  4: </span>
<span class="linenr">  5: </span>  <span style="color: #dcaeea;">mnist</span> = tf.keras.datasets.mnist
<span class="linenr">  6: </span>  (x_train, t_train), (x_test, t_test) = mnist.load_data()
<span class="linenr">  7: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#22294;&#29255;&#36681;&#25563;&#28858;&#19968;&#20491;60000*784&#30340;&#21521;&#37327;&#65292;&#20006;&#19988;&#27161;&#28310;&#21270;</span>
<span class="linenr">  8: </span>  <span style="color: #dcaeea;">x_train</span> = x_train.reshape(x_train.shape[<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>)
<span class="linenr">  9: </span>  <span style="color: #dcaeea;">x_test</span> = x_test.reshape(x_test.shape[<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>)
<span class="linenr"> 10: </span>  <span style="color: #dcaeea;">x_train</span> = x_train.astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 11: </span>  <span style="color: #dcaeea;">x_test</span> = x_test.astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 12: </span>  <span style="color: #dcaeea;">x_train</span> = x_train/<span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr"> 13: </span>  <span style="color: #dcaeea;">x_test</span> = x_test/<span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr"> 14: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;y&#36681;&#25563;&#25104;one-hot encoding </span>
<span class="linenr"> 15: </span>  <span style="color: #dcaeea;">t_train</span> = np_utils.to_categorical(t_train, <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr"> 16: </span>  <span style="color: #dcaeea;">t_test</span> = np_utils.to_categorical(t_test, <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr"> 17: </span>
<span class="linenr"> 18: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">========================================================</span>
<span class="linenr"> 19: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">sigmoid</span>(x):
<span class="linenr"> 20: </span>      <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span> / (<span style="color: #da8548; font-weight: bold;">1</span> + np.exp(-x))
<span class="linenr"> 21: </span>
<span class="linenr"> 22: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">softmax</span>(a):
<span id="coderef-softmax-overflow" class="coderef-off"><span class="linenr"> 23: </span>      <span style="color: #dcaeea;">exp_a</span> = np.exp(a)</span>
<span id="coderef-softmax-overflow2" class="coderef-off"><span class="linenr"> 24: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">exp_a = np.exp(a - np.max(a))</span></span>
<span class="linenr"> 25: </span>      <span style="color: #dcaeea;">sum_exp_a</span> = np.<span style="color: #c678dd;">sum</span>(exp_a) 
<span class="linenr"> 26: </span>      <span style="color: #dcaeea;">y</span> = exp_a / sum_exp_a
<span class="linenr"> 27: </span>      <span style="color: #51afef;">return</span>(y)
<span class="linenr"> 28: </span>
<span class="linenr"> 29: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">cross_entropy_error</span>(y, t):
<span class="linenr"> 30: </span>      <span style="color: #dcaeea;">delta</span> = 1e-<span style="color: #da8548; font-weight: bold;">7</span>
<span class="linenr"> 31: </span>      <span style="color: #51afef;">return</span> -np.<span style="color: #c678dd;">sum</span>(t * np.log(y + delta))
<span class="linenr"> 32: </span>
<span class="linenr"> 33: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_gradient</span>(f, x):
<span class="linenr"> 34: </span>      <span style="color: #dcaeea;">h</span> = 1e-<span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr"> 35: </span>      <span style="color: #dcaeea;">grad</span> = np.zeros_like(x)
<span class="linenr"> 36: </span>      <span style="color: #dcaeea;">it</span> = np.nditer(x, flags=[<span style="color: #98be65;">'multi_index'</span>], op_flags=[<span style="color: #98be65;">'readwrite'</span>])
<span class="linenr"> 37: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25976;&#20540;&#24494;&#20998;&#35336;&#31639;&#21443;&#25976;&#26799;&#24230;</span>
<span class="linenr"> 38: </span>      <span style="color: #51afef;">while</span> <span style="color: #51afef;">not</span> it.<span style="color: #dcaeea;">finished</span>:
<span class="linenr"> 39: </span>          idx = it.multi_index
<span class="linenr"> 40: </span>          <span style="color: #dcaeea;">tmp_val</span> = x[idx]
<span class="linenr"> 41: </span>          <span style="color: #dcaeea;">x</span>[idx] = <span style="color: #c678dd;">float</span>(tmp_val) + h
<span class="linenr"> 42: </span>          <span style="color: #dcaeea;">fxh1</span> = f(x)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x+h)</span>
<span class="linenr"> 43: </span>
<span class="linenr"> 44: </span>          <span style="color: #dcaeea;">x</span>[idx] = tmp_val - h
<span class="linenr"> 45: </span>          <span style="color: #dcaeea;">fxh2</span> = f(x)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x-h)</span>
<span class="linenr"> 46: </span>          <span style="color: #dcaeea;">grad</span>[idx] = (fxh1 - fxh2) / (<span style="color: #da8548; font-weight: bold;">2</span>*h)
<span class="linenr"> 47: </span>
<span class="linenr"> 48: </span>          <span style="color: #dcaeea;">x</span>[idx] = tmp_val  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20516;&#12434;&#20803;&#12395;&#25147;&#12377;</span>
<span class="linenr"> 49: </span>          it.iternext()
<span class="linenr"> 50: </span>      <span style="color: #51afef;">return</span> grad
<span class="linenr"> 51: </span>
<span class="linenr"> 52: </span>  <span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">TwoLayerNet</span>:
<span id="coderef-TLN-init" class="coderef-off"><span class="linenr"> 53: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>, input_size, hidden_size, output_size, weight_init_std=<span style="color: #da8548; font-weight: bold;">0.01</span>):</span>
<span class="linenr"> 54: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38568;&#27231;&#20998;&#37197;weight, bias&#35373;&#28858;0</span>
<span id="coderef-TLN-params" class="coderef-off"><span class="linenr"> 55: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">params</span> = {}</span>
<span class="linenr"> 56: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">params</span>[<span style="color: #98be65;">'W1'</span>] = weight_init_std * np.random.randn(input_size, hidden_size)
<span class="linenr"> 57: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">params</span>[<span style="color: #98be65;">'b1'</span>] = np.zeros(hidden_size)
<span class="linenr"> 58: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">params</span>[<span style="color: #98be65;">'W2'</span>] = weight_init_std * np.random.randn(hidden_size, output_size)
<span class="linenr"> 59: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">params</span>[<span style="color: #98be65;">'b2'</span>] = np.zeros(output_size)
<span class="linenr"> 60: </span>
<span id="coderef-TLN-pred" class="coderef-off"><span class="linenr"> 61: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">predict</span>(<span style="color: #51afef;">self</span>, x):</span>
<span class="linenr"> 62: </span>          <span style="color: #dcaeea;">W1</span>, <span style="color: #dcaeea;">W2</span> = <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W1'</span>], <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W2'</span>]
<span class="linenr"> 63: </span>          <span style="color: #dcaeea;">b1</span>, <span style="color: #dcaeea;">b2</span> = <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b1'</span>], <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b2'</span>]
<span class="linenr"> 64: </span>
<span class="linenr"> 65: </span>          <span style="color: #dcaeea;">a1</span> = np.dot(x, W1) + b1
<span class="linenr"> 66: </span>          <span style="color: #dcaeea;">z1</span> = sigmoid(a1)
<span class="linenr"> 67: </span>          <span style="color: #dcaeea;">a2</span> = np.dot(z1, W2) + b2
<span class="linenr"> 68: </span>          <span style="color: #dcaeea;">y</span> = softmax(a2)
<span class="linenr"> 69: </span>          <span style="color: #51afef;">return</span> y
<span class="linenr"> 70: </span>
<span class="linenr"> 71: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">x : &#36664;&#20837;&#36039;&#26009;, t : &#35347;&#32244;&#36039;&#26009;</span>
<span id="coderef-TLN-loss" class="coderef-off"><span class="linenr"> 72: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">loss</span>(<span style="color: #51afef;">self</span>, x, t):</span>
<span class="linenr"> 73: </span>          y = <span style="color: #51afef;">self</span>.predict(x)
<span class="linenr"> 74: </span>          <span style="color: #51afef;">return</span> cross_entropy_error(y, t)
<span class="linenr"> 75: </span>
<span id="coderef-TLN-accu" class="coderef-off"><span class="linenr"> 76: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">accuracy</span>(<span style="color: #51afef;">self</span>, x, t):</span>
<span class="linenr"> 77: </span>          <span style="color: #dcaeea;">y</span> = <span style="color: #51afef;">self</span>.predict(x)
<span class="linenr"> 78: </span>          <span style="color: #dcaeea;">y</span> = np.argmax(y, axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 79: </span>          <span style="color: #dcaeea;">t</span> = np.argmax(t, axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 80: </span>          <span style="color: #dcaeea;">accuracy</span> = np.<span style="color: #c678dd;">sum</span>(y == t) / <span style="color: #c678dd;">float</span>(x.shape[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 81: </span>          <span style="color: #51afef;">return</span> accuracy
<span class="linenr"> 82: </span>
<span class="linenr"> 83: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35336;&#31639;&#21508;&#21443;&#25976;&#30340;&#26799;&#24230; </span>
<span id="coderef-TLN-numgrad" class="coderef-off"><span class="linenr"> 84: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_gradient</span>(<span style="color: #51afef;">self</span>, x, t):</span>
<span class="linenr"> 85: </span>          <span style="color: #dcaeea;">loss_W</span> = <span style="color: #51afef;">lambda</span> <span style="color: #dcaeea;">W</span>: <span style="color: #51afef;">self</span>.loss(x, t)
<span id="coderef-TLN-grads" class="coderef-off"><span class="linenr"> 86: </span>          grads = {}</span>
<span class="linenr"> 87: </span>          <span style="color: #dcaeea;">grads</span>[<span style="color: #98be65;">'W1'</span>] = numerical_gradient(loss_W, <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W1'</span>])
<span class="linenr"> 88: </span>          <span style="color: #dcaeea;">grads</span>[<span style="color: #98be65;">'b1'</span>] = numerical_gradient(loss_W, <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b1'</span>])
<span class="linenr"> 89: </span>          <span style="color: #dcaeea;">grads</span>[<span style="color: #98be65;">'W2'</span>] = numerical_gradient(loss_W, <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W2'</span>])
<span class="linenr"> 90: </span>          <span style="color: #dcaeea;">grads</span>[<span style="color: #98be65;">'b2'</span>] = numerical_gradient(loss_W, <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b2'</span>])
<span class="linenr"> 91: </span>          <span style="color: #51afef;">return</span> grads
<span class="linenr"> 92: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====================================================================</span>
<span class="linenr"> 93: </span>  <span style="color: #dcaeea;">train_loss_list</span> = []
<span class="linenr"> 94: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36229;&#21443;&#25976;&#35373;&#23450;</span>
<span class="linenr"> 95: </span>  <span style="color: #dcaeea;">iters_num</span> = <span style="color: #da8548; font-weight: bold;">10000</span>
<span class="linenr"> 96: </span>  <span style="color: #dcaeea;">train_size</span> = x_train.shape[<span style="color: #da8548; font-weight: bold;">0</span>]
<span id="coderef-batchSize" class="coderef-off"><span class="linenr"> 97: </span>  <span style="color: #dcaeea;">batch_size</span> = <span style="color: #da8548; font-weight: bold;">100</span></span>
<span class="linenr"> 98: </span>  <span style="color: #dcaeea;">learning_rate</span> = <span style="color: #da8548; font-weight: bold;">0.1</span>
<span class="linenr"> 99: </span>
<span class="linenr">100: </span>  <span style="color: #dcaeea;">network</span> = TwoLayerNet(input_size=<span style="color: #da8548; font-weight: bold;">784</span>, hidden_size=<span style="color: #da8548; font-weight: bold;">50</span>, output_size=<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">101: </span>
<span class="linenr">102: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">for i in range(iters_num): </span>
<span id="coderef-forIters" class="coderef-off"><span class="linenr">103: </span>  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">3</span>):</span>
<span class="linenr">104: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21462;&#24471;&#23567;&#25209;&#27425;</span>
<span id="coderef-batchMask" class="coderef-off"><span class="linenr">105: </span>      <span style="color: #dcaeea;">batch_mask</span> = np.random.choice(train_size, batch_size)</span>
<span class="linenr">106: </span>      <span style="color: #dcaeea;">x_batch</span> = x_train[batch_mask]
<span class="linenr">107: </span>      <span style="color: #dcaeea;">t_batch</span> = t_train[batch_mask]
<span class="linenr">108: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;&#26799;&#24230;</span>
<span id="coderef-batchGrad" class="coderef-off"><span class="linenr">109: </span>      <span style="color: #dcaeea;">grad</span> = network.numerical_gradient(x_batch, t_batch)</span>
<span class="linenr">110: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26356;&#26032;&#21443;&#25976;</span>
<span id="coderef-updateParams" class="coderef-off"><span class="linenr">111: </span>      <span style="color: #51afef;">for</span> key <span style="color: #51afef;">in</span> (<span style="color: #98be65;">'W1'</span>, <span style="color: #98be65;">'b1'</span>, <span style="color: #98be65;">'W2'</span>, <span style="color: #98be65;">'b2'</span>):</span>
<span class="linenr">112: </span>          network.<span style="color: #dcaeea;">params</span>[key] -= learning_rate + grad[key]
<span class="linenr">113: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35352;&#37636;&#23416;&#32722;&#36942;&#31243;</span>
<span class="linenr">114: </span>      <span style="color: #dcaeea;">loss</span> = network.loss(x_batch, t_batch)
<span class="linenr">115: </span>      train_loss_list.append(loss)
<span class="linenr">116: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36664;&#20986;&#32080;&#26524;</span>
<span class="linenr">117: </span>      <span style="color: #51afef;">print</span>(train_loss_list)
</pre>
</div>

<pre class="example">
[990.0180885577449]
[990.0180885577449, 995.0232568391127]
[990.0180885577449, 995.0232568391127, 1150.311320292553]
</pre>


<p>
上述程式碼中，小批次大小為 100（第<a href="#coderef-batchSize" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-batchSize');" onmouseout="CodeHighlightOff(this, 'coderef-batchSize');">97</a>行），每次從 60000 個訓練資料中隨機取出 100 筆資料（第<a href="#coderef-batchMask" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-batchMask');" onmouseout="CodeHighlightOff(this, 'coderef-batchMask');">105</a>行，包含影像資料與正確答案標籤資料），接下來以此 100 筆資料計算梯度（第<a href="#coderef-batchGrad" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-batchGrad');" onmouseout="CodeHighlightOff(this, 'coderef-batchGrad');">109</a>行），然後利用準確率梯度下降法（SGD）更新參數（第<a href="#coderef-updateParams" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-updateParams');" onmouseout="CodeHighlightOff(this, 'coderef-updateParams');">111</a>行）,然後記錄損失函數，如此整個步驟重複執行 10000 次（第<a href="#coderef-forIters" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-forIters');" onmouseout="CodeHighlightOff(this, 'coderef-forIters');">103</a>行）。<br />
</p>
</div>
</li>

<li><a id="orgb060c11"></a>模型成效評估<br />
<div class="outline-text-4" id="text-8-6-5">
<p>
如何評估<br />
</p>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-org1c2f8e3" class="outline-2">
<h2 id="org1c2f8e3"><span class="section-number-2">9</span> 建構良好的訓練集：數據預處理</h2>
<div class="outline-text-2" id="text-9">
<p>
進行數運模式運算之前，需要進行的數據預處理工作大致可分為以下幾點：<br />
</p>
<ol class="org-ol">
<li>數據遺漏值處理<br /></li>
<li>數據分類編碼<br /></li>
<li>數據訓練集與測試集之分割<br /></li>
<li>數據特徵選取<br /></li>
</ol>
</div>

<div id="outline-container-orgbebc2ba" class="outline-3">
<h3 id="orgbebc2ba"><span class="section-number-3">9.1</span> 處理數據遺漏</h3>
<div class="outline-text-3" id="text-9-1">
<p>
現實世界中可能會因各種原因導致數據缺失或遺漏(如問卷被刻意留白)，這些部份通常會以「空白」、「NaN」或「NULL」來取代。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="orgb71198d"></a>遺漏值的識別<br />
<div class="outline-text-4" id="text-9-1-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #dcaeea;">csv_data</span> = <span style="color: #98be65;">'''A,X,B,C,D</span>
<span class="linenr"> 2: </span><span style="color: #98be65;">  1.0,,2.0,3.0,4.0</span>
<span class="linenr"> 3: </span><span style="color: #98be65;">  5.0,,6.0,,8.0</span>
<span class="linenr"> 4: </span><span style="color: #98be65;">  10.0,,11.0,12.0</span>
<span class="linenr"> 5: </span><span style="color: #98be65;">  ,,,,'''</span>
<span class="linenr"> 6: </span>  <span style="color: #51afef;">import</span> sys
<span class="linenr"> 7: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">python 2.7&#38656;&#36914;&#34892;unicode&#36681;&#30908;</span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">if</span> (sys.version_info &lt; (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">0</span>)):
<span class="linenr">10: </span>      <span style="color: #dcaeea;">csv_data</span> = <span style="color: #c678dd;">unicode</span>(csv_data)
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#31243;&#24335;&#27284;&#20013;&#30340;csv&#36039;&#26009;</span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> io <span style="color: #51afef;">import</span> StringIO
<span class="linenr">13: </span>  <span style="color: #dcaeea;">df</span> = pd.read_csv(StringIO(csv_data))
<span class="linenr">14: </span>  <span style="color: #51afef;">print</span>(df)
<span class="linenr">15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21015;&#20986;&#27599;&#34892;&#26377;&#30340;null&#20491;&#25976;</span>
<span class="linenr">16: </span>  <span style="color: #51afef;">print</span>(df.isnull().<span style="color: #c678dd;">sum</span>()) 
<span class="linenr">17: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">access the underlying NumPy array</span>
<span class="linenr">18: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">via the `values` attribute</span>
<span class="linenr">19: </span>  df.values
<span class="linenr">20: </span>
<span class="linenr">21: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21083;&#38500;&#26377;&#36986;&#22833;&#20540;&#30340;&#36039;&#26009;&#21015;</span>
<span class="linenr">22: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#21034;&#25481;&#26377;&#36986;&#22833;&#20540;&#30340;&#21015;:df.dropna(axis=1)'</span>)
<span class="linenr">23: </span>  <span style="color: #51afef;">print</span>(df.dropna(axis=<span style="color: #da8548; font-weight: bold;">0</span>))
<span class="linenr">24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21083;&#38500;&#26377;&#36986;&#22833;&#20540;&#30340;&#36039;&#26009;&#34892;</span>
<span class="linenr">25: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#21034;&#25481;&#26377;&#36986;&#22833;&#20540;&#30340;&#34892;:df.dropna(axis=1)'</span>)
<span class="linenr">26: </span>  <span style="color: #51afef;">print</span>(df.dropna(axis=<span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr">27: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21083;&#38500;&#25972;&#21015;&#28858;NaN&#32773;</span>
<span class="linenr">28: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#21083;&#38500;&#25972;&#34892;&#28858;NaN&#32773;:df.dropna(how=\'all\')'</span>)
<span class="linenr">29: </span>  <span style="color: #51afef;">print</span>(df.dropna(how=<span style="color: #98be65;">'all'</span>) )
<span class="linenr">30: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21034;&#38500;&#26377;&#20540;&#20491;&#25976;&#20302;&#26044;thresh&#30340;&#21015;</span>
<span class="linenr">31: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#21034;&#38500;&#26377;&#20540;&#20491;&#25976;&#20302;&#26044;thresh&#30340;&#21015;:df.dropna(thresh=4)'</span>)
<span class="linenr">32: </span>  <span style="color: #51afef;">print</span>(df.dropna(thresh=<span style="color: #da8548; font-weight: bold;">4</span>))
<span class="linenr">33: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21034;&#38500;&#29305;&#23450;&#34892;(&#22914;&#31532;C&#34892;)&#20013;&#26377;NaN&#20043;&#21015;</span>
<span class="linenr">34: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#21034;&#38500;&#29305;&#23450;&#34892;(&#22914;&#31532;C&#34892;)&#20013;&#26377;NaN&#20043;&#21015;:df.dropna(subset=[\'C\'])'</span>)
<span class="linenr">35: </span>  <span style="color: #51afef;">print</span>(df.dropna(subset=[<span style="color: #98be65;">'C'</span>]))
</pre>
</div>

<pre class="example">
      A   X     B     C    D
0   1.0 NaN   2.0   3.0  4.0
1   5.0 NaN   6.0   NaN  8.0
2  10.0 NaN  11.0  12.0  NaN
3   NaN NaN   NaN   NaN  NaN
A    1
X    4
B    1
C    2
D    2
dtype: int64
刪掉有遺失值的列:df.dropna(axis=1)
Empty DataFrame
Columns: [A, X, B, C, D]
Index: []
刪掉有遺失值的行:df.dropna(axis=1)
Empty DataFrame
Columns: []
Index: [0, 1, 2, 3]
剛除整行為NaN者:df.dropna(how='all')
      A   X     B     C    D
0   1.0 NaN   2.0   3.0  4.0
1   5.0 NaN   6.0   NaN  8.0
2  10.0 NaN  11.0  12.0  NaN
刪除有值個數低於thresh的列:df.dropna(thresh=4)
     A   X    B    C    D
0  1.0 NaN  2.0  3.0  4.0
刪除特定行(如第C行)中有NaN之列:df.dropna(subset=['C'])
      A   X     B     C    D
0   1.0 NaN   2.0   3.0  4.0
2  10.0 NaN  11.0  12.0  NaN
</pre>

<p>
雖然刪除包含遺漏值的數據似乎是個方便的方法，但終究可能會刪除過多的樣本，導致分析的結果並不可靠；或是因為刪除了特徵的時候，卻失去了重要的資訊。<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org1eb9197" class="outline-3">
<h3 id="org1eb9197"><span class="section-number-3">9.2</span> 填補遺遺漏值</h3>
<div class="outline-text-3" id="text-9-2">
<p>
最常見的「插補技術」之一為「平均插補」(mean imputation)，即，以整個特徵行的平均值來代替遺漏值。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #dcaeea;">csv_data</span> = <span style="color: #98be65;">'''A,X,B,C,D</span>
<span class="linenr"> 2: </span><span style="color: #98be65;">  1.0,,2.0,3.0,4.0</span>
<span class="linenr"> 3: </span><span style="color: #98be65;">  5.0,,6.0,,8.0</span>
<span class="linenr"> 4: </span><span style="color: #98be65;">  10.0,,11.0,12.0</span>
<span class="linenr"> 5: </span><span style="color: #98be65;">  ,,,,'''</span>
<span class="linenr"> 6: </span>  <span style="color: #51afef;">import</span> sys
<span class="linenr"> 7: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">python 2.7&#38656;&#36914;&#34892;unicode&#36681;&#30908;</span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">if</span> (sys.version_info &lt; (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">0</span>)):
<span class="linenr">10: </span>      <span style="color: #dcaeea;">csv_data</span> = <span style="color: #c678dd;">unicode</span>(csv_data)
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#31243;&#24335;&#27284;&#20013;&#30340;csv&#36039;&#26009;</span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> io <span style="color: #51afef;">import</span> StringIO
<span class="linenr">13: </span>  <span style="color: #dcaeea;">df</span> = pd.read_csv(StringIO(csv_data))
<span class="linenr">14: </span>
<span class="linenr">15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">impute missing values via the column mean</span>
<span class="linenr">16: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> Imputer
<span class="linenr">17: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">axis=0: &#20197;&#34892;&#30340;&#24179;&#22343;&#20540;&#20358;&#35036;</span>
<span class="linenr">18: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">axis=1: &#20197;&#21015;&#30340;&#24179;&#22343;&#20540;&#20358;&#35036;</span>
<span class="linenr">19: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">strategy&#30340;&#36984;&#38917;&#26377;: median(&#20013;&#20301;&#25976;)&#12289;most_freqent(&#26368;&#38971;&#32321;&#20986;&#29694;&#32773;)</span>
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">most_freqent&#22312;&#20570;&#28858;&#20998;&#39006;&#29305;&#24501;&#26178;&#24456;&#26377;&#29992;</span>
<span class="linenr">21: </span>  imr = Imputer(missing_values=<span style="color: #98be65;">'NaN'</span>, strategy=<span style="color: #98be65;">'mean'</span>, axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">22: </span>  <span style="color: #dcaeea;">imr</span> = imr.fit(df.values)
<span class="linenr">23: </span>  <span style="color: #dcaeea;">imputed_data</span> = imr.transform(df.values)
<span class="linenr">24: </span>  <span style="color: #51afef;">print</span>(df)
<span class="linenr">25: </span>  <span style="color: #51afef;">print</span>(imputed_data)
</pre>
</div>

<pre class="example">
      A   X     B     C    D
0   1.0 NaN   2.0   3.0  4.0
1   5.0 NaN   6.0   NaN  8.0
2  10.0 NaN  11.0  12.0  NaN
3   NaN NaN   NaN   NaN  NaN
[[ 1.          2.          3.          4.        ]
 [ 5.          6.          7.5         8.        ]
 [10.         11.         12.          6.        ]
 [ 5.33333333  6.33333333  7.5         6.        ]]
</pre>


<p>
Imputer 類別在 scikit-learn 中屬於 transformer 類別，主要的工作是做「數據轉換」，這些 estimator 有兩種基本方法：fit 與 transform，fit 方法是用來進行參數學習。<br />
</p>
</div>
</div>

<div id="outline-container-org4de56f0" class="outline-3">
<h3 id="org4de56f0"><span class="section-number-3">9.3</span> 處理數據中的分類特徵編碼問題</h3>
<div class="outline-text-3" id="text-9-3">
</div>
<ol class="org-ol">
<li><a id="org07e4b47"></a>categorical feature<br />
<div class="outline-text-4" id="text-9-3-1">
<p>
真實世界的數據集往往包含各種「類別特徵」(categorical feature)，類別特徵可再分為<br />
</p>
<ul class="org-ul">
<li>nominal feature: 名義特徵<br /></li>
<li>ordinal feature: 次序特徵<br /></li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">2: </span>  <span style="color: #dcaeea;">df</span> = pd.DataFrame([[<span style="color: #98be65;">'green'</span>, <span style="color: #98be65;">'M'</span>, <span style="color: #da8548; font-weight: bold;">10.1</span>, <span style="color: #98be65;">'class2'</span>],
<span class="linenr">3: </span>                     [<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'L'</span>, <span style="color: #da8548; font-weight: bold;">13.5</span>, <span style="color: #98be65;">'class1'</span>],
<span class="linenr">4: </span>                     [<span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'XL'</span>, <span style="color: #da8548; font-weight: bold;">15.3</span>, <span style="color: #98be65;">'class2'</span>]])
<span class="linenr">5: </span>
<span class="linenr">6: </span>  df.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>, <span style="color: #98be65;">'price'</span>, <span style="color: #98be65;">'classlabel'</span>]
<span class="linenr">7: </span>  <span style="color: #51afef;">print</span>(df)
</pre>
</div>

<pre class="example">
   color size  price classlabel
0  green    M   10.1     class2
1    red    L   13.5     class1
2   blue   XL   15.3     class2
</pre>
</div>
</li>

<li><a id="orgaadd1c5"></a>對應 ordinal feature<br />
<div class="outline-text-4" id="text-9-3-2">
<p>
自定一個 mapping dictionary，即 size_mapping，然後將 classlabel 對應到 size_mapping 中的鍵值(程式第<a href="#coderef-sizeMapping" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-sizeMapping');" onmouseout="CodeHighlightOff(this, 'coderef-sizeMapping');">11</a>行)。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span>  <span style="color: #dcaeea;">df</span> = pd.DataFrame([[<span style="color: #98be65;">'green'</span>, <span style="color: #98be65;">'M'</span>, <span style="color: #da8548; font-weight: bold;">10.1</span>, <span style="color: #98be65;">'class2'</span>],
<span class="linenr"> 3: </span>                     [<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'L'</span>, <span style="color: #da8548; font-weight: bold;">13.5</span>, <span style="color: #98be65;">'class1'</span>],
<span class="linenr"> 4: </span>                     [<span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'XL'</span>, <span style="color: #da8548; font-weight: bold;">15.3</span>, <span style="color: #98be65;">'class2'</span>]])
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  df.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>, <span style="color: #98be65;">'price'</span>, <span style="color: #98be65;">'classlabel'</span>]
<span class="linenr"> 7: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">Mapping ordinal features</span>
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">size_mapping</span> = {<span style="color: #98be65;">'XL'</span>: <span style="color: #da8548; font-weight: bold;">3</span>,
<span class="linenr"> 9: </span>                  <span style="color: #98be65;">'L'</span>: <span style="color: #da8548; font-weight: bold;">2</span>,
<span class="linenr">10: </span>                  <span style="color: #98be65;">'M'</span>: <span style="color: #da8548; font-weight: bold;">1</span>}
<span id="coderef-sizeMapping" class="coderef-off"><span class="linenr">11: </span>  <span style="color: #dcaeea;">df</span>[<span style="color: #98be65;">'size'</span>] = df[<span style="color: #98be65;">'size'</span>].<span style="color: #c678dd;">map</span>(size_mapping)</span>
<span class="linenr">12: </span>  <span style="color: #51afef;">print</span>(df)
</pre>
</div>

<pre class="example">
   color  size  price classlabel
0  green     1   10.1     class2
1    red     2   13.5     class1
2   blue     3   15.3     class2
</pre>
</div>
</li>

<li><a id="orgd837b00"></a>對應 nominal feature<br />
<div class="outline-text-4" id="text-9-3-3">
<p>
許多機器學習的函式庫需要將「類別標籤」編碼為整數值。方法之一是以列舉方式為這些 nominal features 自 0 開始編號，先以 enumerate 方式建立一個 mapping dictionary: class_mapping(程式第<a href="#coderef-classMapping" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-classMapping');" onmouseout="CodeHighlightOff(this, 'coderef-classMapping');">10</a>行)，然後利用這個字典將類別特徵轉換為整數值。<br />
</p>

<p>
此外，也可以利用已產生的對應字典，藉由借調 key-value 來產生「反轉字典」(第<a href="#coderef-invClassMapping" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-invClassMapping');" onmouseout="CodeHighlightOff(this, 'coderef-invClassMapping');">18</a>行)，將對調產生的整數還原回原始類別特徵。<br />
</p>

<p>
scikit-learn 中有一個更為方便的 LabelEncoder 類別則可以直接完成上述工作(第<a href="#coderef-labelEncoder" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-labelEncoder');" onmouseout="CodeHighlightOff(this, 'coderef-labelEncoder');">25</a>行)。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span>  <span style="color: #dcaeea;">df</span> = pd.DataFrame([[<span style="color: #98be65;">'green'</span>, <span style="color: #98be65;">'M'</span>, <span style="color: #da8548; font-weight: bold;">10.1</span>, <span style="color: #98be65;">'class2'</span>],
<span class="linenr"> 3: </span>                     [<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'L'</span>, <span style="color: #da8548; font-weight: bold;">13.5</span>, <span style="color: #98be65;">'class1'</span>],
<span class="linenr"> 4: </span>                     [<span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'XL'</span>, <span style="color: #da8548; font-weight: bold;">15.3</span>, <span style="color: #98be65;">'class2'</span>]])
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  df.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>, <span style="color: #98be65;">'price'</span>, <span style="color: #98be65;">'classlabel'</span>]
<span class="linenr"> 7: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#21033;&#23565;&#25033;&#23383;&#20856;</span>
<span class="linenr"> 8: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> np
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">class_mapping</span> = {
<span id="coderef-classMapping" class="coderef-off"><span class="linenr">10: </span>      <span style="color: #dcaeea;">label</span>: idx <span style="color: #51afef;">for</span> idx, label <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(df[<span style="color: #98be65;">'classlabel'</span>]))</span>
<span class="linenr">11: </span>  }
<span class="linenr">12: </span>  <span style="color: #51afef;">print</span>(class_mapping)
<span class="linenr">13: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#39006;&#21029;&#29305;&#24501;&#36681;&#25563;&#28858;&#25972;&#25976;&#20540;</span>
<span class="linenr">14: </span>  df[<span style="color: #98be65;">'classlabel'</span>] = df[<span style="color: #98be65;">'classlabel'</span>].<span style="color: #c678dd;">map</span>(class_mapping)
<span class="linenr">15: </span>  <span style="color: #51afef;">print</span>(df)
<span class="linenr">16: </span>
<span class="linenr">17: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29986;&#29983;&#21453;&#36681;&#23383;&#20856;&#65292;&#23559;&#25972;&#25976;&#36996;&#21407;&#33267;&#21407;&#22987;&#30340;&#39006;&#21029;&#27161;&#31844;</span>
<span id="coderef-invClassMapping" class="coderef-off"><span class="linenr">18: </span>  <span style="color: #dcaeea;">inv_class_mapping</span> = {<span style="color: #dcaeea;">v</span>: k <span style="color: #51afef;">for</span> k, v <span style="color: #51afef;">in</span> class_mapping.items()}</span>
<span class="linenr">19: </span>  df[<span style="color: #98be65;">'classlabel'</span>] = df[<span style="color: #98be65;">'classlabel'</span>].<span style="color: #c678dd;">map</span>(inv_class_mapping) 
<span class="linenr">20: </span>  <span style="color: #51afef;">print</span>(df)
<span class="linenr">21: </span>
<span class="linenr">22: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Label encoding with sklearn's LabelEncoder</span>
<span class="linenr">23: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> LabelEncoder
<span class="linenr">24: </span>  <span style="color: #dcaeea;">class_le</span> = LabelEncoder()
<span id="coderef-labelEncoder" class="coderef-off"><span class="linenr">25: </span>  <span style="color: #dcaeea;">y</span> = class_le.fit_transform(df[<span style="color: #98be65;">'classlabel'</span>].values)</span>
<span class="linenr">26: </span>  <span style="color: #51afef;">print</span>(y)
<span class="linenr">27: </span>  <span style="color: #dcaeea;">df</span>[<span style="color: #98be65;">'classlabel'</span>] = y 
<span class="linenr">28: </span>  <span style="color: #51afef;">print</span>(df) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39006;&#21029;&#33287;&#25976;&#23383;&#30340;&#23565;&#25033;&#19981;&#19968;&#23450;&#33287;&#33258;&#35330;&#23383;&#20856;&#19968;&#33268;</span>
<span class="linenr">29: </span>
</pre>
</div>

<pre class="example">
{'class2': 0, 'class1': 1}
   color size  price  classlabel
0  green    M   10.1           0
1    red    L   13.5           1
2   blue   XL   15.3           0
   color size  price classlabel
0  green    M   10.1     class2
1    red    L   13.5     class1
2   blue   XL   15.3     class2
[1 0 1]
   color size  price  classlabel
0  green    M   10.1           1
1    red    L   13.5           0
2   blue   XL   15.3           1
</pre>
</div>
</li>

<li><a id="orgbf80e25"></a>對 nominal feature 執行 one-hot encoding<br />
<div class="outline-text-4" id="text-9-3-4">
<p>
scikit-learn 的 LabelENcoder 類別可以用來將「類別特徵」編碼為整數值，但這樣會引發另一個問題，如果我們將上述資料中的 color 特徵轉換為整數值，如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span>  <span style="color: #dcaeea;">df</span> = pd.DataFrame([[<span style="color: #98be65;">'green'</span>, <span style="color: #98be65;">'M'</span>, <span style="color: #da8548; font-weight: bold;">10.1</span>, <span style="color: #98be65;">'class2'</span>],
<span class="linenr"> 3: </span>                     [<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'L'</span>, <span style="color: #da8548; font-weight: bold;">13.5</span>, <span style="color: #98be65;">'class1'</span>],
<span class="linenr"> 4: </span>                     [<span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'XL'</span>, <span style="color: #da8548; font-weight: bold;">15.3</span>, <span style="color: #98be65;">'class2'</span>]])
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  df.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>, <span style="color: #98be65;">'price'</span>, <span style="color: #98be65;">'classlabel'</span>]
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">X</span> = df[[<span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>, <span style="color: #98be65;">'price'</span>, <span style="color: #98be65;">'classlabel'</span>]].values
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;LabelEncoder&#36681;&#25563;</span>
<span class="linenr">11: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> LabelEncoder
<span class="linenr">12: </span>  <span style="color: #dcaeea;">color_le</span> = LabelEncoder()
<span class="linenr">13: </span>  <span style="color: #51afef;">print</span>(X[:,<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">14: </span>  <span style="color: #dcaeea;">X</span>[:,<span style="color: #da8548; font-weight: bold;">0</span>] = color_le.fit_transform(X[:,<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">15: </span>  <span style="color: #51afef;">print</span>(X[:,<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">16: </span>
</pre>
</div>

<pre class="example">
['green' 'red' 'blue']
[1 2 0]
</pre>


<p>
由輸出結果可以發現，經過類別編碼後的顏色特徵，由原本不具次序的特徵變成存在大小關係(red&gt;green&gt;blue)，這明顯會影響 model 運算的結果。針對此一問題，常見的解決方案是 one-hot encoding，其原理是：對特徵值中的每個值，建立一個新的「虛擬特徵」(dummy feature)。方法有二：<br />
</p>
<ul class="org-ul">
<li>利用 ColumnTransformer 函式庫的 ColumnTransformer 類別，將特徵值轉換 One-Hot Encoding 的對應矩陣，如程式第<a href="#coderef-FitTransform" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-FitTransform');" onmouseout="CodeHighlightOff(this, 'coderef-FitTransform');">24</a>行。<br /></li>
<li>利用 Pandas 套件的 get_dummies 類別，一次將矩陣內指定之 column 轉換為 One-Hot encoding，如程式第<a href="#coderef-GetDummies" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-GetDummies');" onmouseout="CodeHighlightOff(this, 'coderef-GetDummies');">28</a>行。這種轉換只有字串數據會被轉換，其他內容則否。<br /></li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span>  <span style="color: #dcaeea;">df</span> = pd.DataFrame([[<span style="color: #98be65;">'green'</span>, <span style="color: #98be65;">'M'</span>, <span style="color: #da8548; font-weight: bold;">10.1</span>, <span style="color: #98be65;">'class2'</span>],
<span class="linenr"> 3: </span>                     [<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'L'</span>, <span style="color: #da8548; font-weight: bold;">13.5</span>, <span style="color: #98be65;">'class1'</span>],
<span class="linenr"> 4: </span>                     [<span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'XL'</span>, <span style="color: #da8548; font-weight: bold;">15.3</span>, <span style="color: #98be65;">'class2'</span>]])
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  df.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>, <span style="color: #98be65;">'price'</span>, <span style="color: #98be65;">'classlabel'</span>]
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">X</span> = df[[<span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>, <span style="color: #98be65;">'price'</span>, <span style="color: #98be65;">'classlabel'</span>]].values
<span class="linenr"> 9: </span>  <span style="color: #51afef;">print</span>(df)
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">one-hot encoding: ColumnTransformer / fit_transform</span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> LabelEncoder
<span class="linenr">13: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> OneHotEncoder
<span class="linenr">14: </span>  <span style="color: #51afef;">from</span> sklearn.compose <span style="color: #51afef;">import</span> ColumnTransformer
<span class="linenr">15: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np 
<span class="linenr">16: </span>
<span class="linenr">17: </span>  <span style="color: #dcaeea;">X</span> = df[[<span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>, <span style="color: #98be65;">'price'</span>]].values
<span class="linenr">18: </span>  
<span class="linenr">19: </span>  <span style="color: #dcaeea;">ct</span> = ColumnTransformer(
<span class="linenr">20: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">The column numbers to be transformed (here is [0] but can be [0, 1, 3])</span>
<span class="linenr">21: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Leave the rest of the columns untouched</span>
<span class="linenr">22: </span>      [(<span style="color: #98be65;">'OneHot'</span>, OneHotEncoder(), [<span style="color: #da8548; font-weight: bold;">0</span>])], remainder=<span style="color: #98be65;">'passthrough'</span> 
<span class="linenr">23: </span>  )
<span id="coderef-FitTransform" class="coderef-off"><span class="linenr">24: </span>  <span style="color: #51afef;">print</span>(ct.fit_transform(X))</span>
<span class="linenr">25: </span>
<span class="linenr">26: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">on-hot encoding: pandas / get_dummies </span>
<span class="linenr">27: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span id="coderef-GetDummies" class="coderef-off"><span class="linenr">28: </span>  <span style="color: #51afef;">print</span>(pd.get_dummies(df[[<span style="color: #98be65;">'price'</span>, <span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>]]))</span>
</pre>
</div>

<pre class="example">
   color size  price classlabel
0  green    M   10.1     class2
1    red    L   13.5     class1
2   blue   XL   15.3     class2
[[0.0 1.0 0.0 'M' 10.1]
 [0.0 0.0 1.0 'L' 13.5]
 [1.0 0.0 0.0 'XL' 15.3]]
   price  color_blue  color_green  color_red  size_L  size_M  size_XL
0   10.1           0            1          0       0       1        0
1   13.5           0            0          1       1       0        0
2   15.3           1            0          0       0       0        1
</pre>

<p>
應用 one-hot encoding 時，我們必須留意它所引入的「多元共線性」(multicollinearity)問題，這在某些狀況下(如要計算反矩陣)可能會產生一些問題，若特徵間有高度相關，則會難以計算反矩陣，導致數值不穩定的舘計。<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgdd4ba17" class="outline-3">
<h3 id="orgdd4ba17"><span class="section-number-3">9.4</span> 訓練集與測試集的數據分割</h3>
<div class="outline-text-3" id="text-9-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;"># Partitioning a dataset into a seperate training and test set</span>
<span class="linenr"> 2: </span>  <span style="color: #dcaeea;">df_wine</span> = pd.read_csv(<span style="color: #98be65;">'https://archive.ics.uci.edu/'</span>
<span class="linenr"> 3: </span>                        <span style="color: #98be65;">'ml/machine-learning-databases/wine/wine.data'</span>,
<span class="linenr"> 4: </span>                        header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">if the Wine dataset is temporarily unavailable from the</span>
<span class="linenr"> 7: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">UCI machine learning repository, un-comment the following line</span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">of code to load the dataset from a local path:</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">df_wine = pd.read_csv('wine.data', header=None)</span>
<span class="linenr">11: </span>
<span class="linenr">12: </span>
<span class="linenr">13: </span>  df_wine.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'Class label'</span>, <span style="color: #98be65;">'Alcohol'</span>, <span style="color: #98be65;">'Malic acid'</span>, <span style="color: #98be65;">'Ash'</span>,
<span class="linenr">14: </span>                     <span style="color: #98be65;">'Alcalinity of ash'</span>, <span style="color: #98be65;">'Magnesium'</span>, <span style="color: #98be65;">'Total phenols'</span>,
<span class="linenr">15: </span>                     <span style="color: #98be65;">'Flavanoids'</span>, <span style="color: #98be65;">'Nonflavanoid phenols'</span>, <span style="color: #98be65;">'Proanthocyanins'</span>,
<span class="linenr">16: </span>                     <span style="color: #98be65;">'Color intensity'</span>, <span style="color: #98be65;">'Hue'</span>, <span style="color: #98be65;">'OD280/OD315 of diluted wines'</span>,
<span class="linenr">17: </span>                     <span style="color: #98be65;">'Proline'</span>]
<span class="linenr">18: </span>
<span class="linenr">19: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Class labels'</span>, np.unique(df_wine[<span style="color: #98be65;">'Class label'</span>]))
<span class="linenr">20: </span>  df_wine.head()
<span class="linenr">21: </span>
<span class="linenr">22: </span>  <span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">y</span> = df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">1</span>:].values, df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">0</span>].values
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> =    train_test_split(X, y, 
<span class="linenr">25: </span>                       test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>, 
<span class="linenr">26: </span>                       random_state=<span style="color: #da8548; font-weight: bold;">0</span>, 
<span class="linenr">27: </span>                       stratify=y)
<span class="linenr">28: </span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orga1dde0e" class="outline-3">
<h3 id="orga1dde0e"><span class="section-number-3">9.5</span> 縮放特徵值、維持特徵值影響比例：正規化(normalization)</h3>
<div class="outline-text-3" id="text-9-5">
<p>
「特徵縮放」(Feature scaling)是資料預處理的一個關鍵，「決策樹」和「隨機森林」是極少數無需進行 feature scaling 的分類技術；對多數機器學習演算法而言，若特徵值經過適當的縮放，都能有更佳成效。<br />
</p>

<p>
Feature scaling 的重要性可以以下例子看出，假設有兩個特徵值(a, b)，其中 a 的測量範圍為 1 到 10，b 的測量值範圍為 1 到 100000，以典型分類演算法的做法，一定是忙於最佳化特徵值 b；若以 KNN 的演算法，也會被特徵值 b 所技配。<br />
</p>

<p>
正規化有兩種常用的方法，可以將不同規模的特徵轉化為相同的規模：常態化(normalization)和標準化(standardization)：<br />
</p>
</div>
<ol class="org-ol">
<li><a id="orgd1c191c"></a>常態化<br />
<div class="outline-text-4" id="text-9-5-1">
<p>
將特徵值縮化為 0~1 間，這是「最小最大縮放」(min-max scaling)的一個特例，某一特徵值的常態化做法如下：<br />
\[x_{norm}^i = \frac{x^i-x_{min}}{x_{max}-x_{min}}\]<br />
若以 scikit-learn 套件來完成實作，其程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> MinMaxScaler
<span class="linenr">2: </span>  <span style="color: #dcaeea;">mms</span> = MinMaxScaler()
<span class="linenr">3: </span>  <span style="color: #dcaeea;">X_train_norm</span> = mms.fit_transform(X_train)
<span class="linenr">4: </span>  <span style="color: #dcaeea;">X_test_norm</span> = mms.fit_transform(X_test)
</pre>
</div>
</div>
</li>

<li><a id="org6f06e7b"></a>標準化<br />
<div class="outline-text-4" id="text-9-5-2">
<p>
雖說常態化簡單實用，但對許多機器學習演算法來說(特別是梯度下降法的最佳化)，標準化則更為實際，我們可令標準化後的特徵值其平均數為 0、標準差為 1，這樣一來，特徵值會滿足常態分佈，進而使演算法對於離群值不那麼敏感。標準化的公式如下：<br />
\[x_{std}^i = \frac{x^i-\mu_x}{\sigma_x}\]<br />
若以 scikit-learn 套件來完成實作，其程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr">2: </span>  <span style="color: #dcaeea;">stdsc</span> = StandardScaler()
<span class="linenr">3: </span>  <span style="color: #dcaeea;">X_train_std</span> = stdsc.fit_transform(X_train)
<span class="linenr">4: </span>  <span style="color: #dcaeea;">X_test_std</span> = stdsc.transform(X_test)
</pre>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-org9c44297" class="outline-3">
<h3 id="org9c44297"><span class="section-number-3">9.6</span> 選取有意義的特徵</h3>
<div class="outline-text-3" id="text-9-6">
<p>
overfitting 的產生原因是模型過度遷就於訓練數據，導致面對新數據(測試集)時成效不彰，我們稱這種模型具有較高變異性(high variance)，一般的解決策略有：<br />
</p>
<ul class="org-ul">
<li>收集更多的訓練數據集<br /></li>
<li>經由正規化，對於過度複雜的模型引進一個「懲罰」(penalty)<br /></li>
<li>以較少的參數做出較簡單的模型(使用更簡單的模型)<br /></li>
<li>減少數據維度<br /></li>
</ul>
</div>

<ol class="org-ol">
<li><a id="org9a482f6"></a>L1L2 regularzation<br />
<div class="outline-text-4" id="text-9-6-1">
<p>
一個典型的解釋<sup><a id="fnr.7" class="footref" href="#fn.7">7</a></sup>如圖<a href="#org4096b8c">51</a>，&ldquo;我們知道, 過擬合就是所謂的模型對可見的數據過度自信, 非常完美的擬合上了這些數據, 如果具備過擬合的能力, 那麼這個方程就可能是一個比較複雜的非線性方程 , 正是因為這裡的 x^3 和 x^2 使得這條虛線能夠被彎來彎去, 所以整個模型就會特別努力地去學習作用在 x^3 和 x^2 上的 c, d 參數. 但是我們期望模型要學到的卻是 這條藍色的曲線. 因為它能更有效地概括數據.而且只需要一個 y=a+bx 就能表達出數據的規律. 或者是說, 藍色的線最開始時, 和紅色線同樣也有 c d 兩個參數, 可是最終學出來時, c 和 d 都學成了 0, 雖然藍色方程的誤差要比紅色大, 但是概括起數據來還是藍色好. 那我們如何保證能學出來這樣的參數呢? 這就是 l1 l2 正規化出現的原因啦.&rdquo;<br />
</p>


<div id="org4096b8c" class="figure">
<p><img src="images/L1l2regularization2.png" alt="L1l2regularization2.png" /><br />
</p>
<p><span class="figure-number">Figure 51: </span>過擬合問題</p>
</div>

<p>
對於上述訓練出的兩個方程式，我們可以用\((y_{\theta}(x)-y)^2\)來計算模型預測值\(y(x)\)和真實數據\(y\)的誤差，而 L1, L2 就只是在這個誤差公式後加上一些式子來修正這個公式(如圖<a href="#orgabe4ded">52</a>)，其目的在於讓誤差的最佳化不僅取決於訓練數據擬合的優劣，同時也取決於參數值(如 c,d)的大小；L2 正規化以參數平方來做為計算方式，L1 正規化則是計算每個參數的絕對值。<br />
</p>

<div id="orgabe4ded" class="figure">
<p><img src="images/L1l2regularization3.png" alt="L1l2regularization3.png" /><br />
</p>
<p><span class="figure-number">Figure 52: </span>L1,L2 正規化公式</p>
</div>

<p>
進一步以 Tensorflow Playground 的圖示來觀察 L1,L2 正規化的差異<sup><a id="fnr.8" class="footref" href="#fn.8">8</a></sup>，如果把正規化(Regularization)設定為 L1，再執行訓練。可以看到很多權重都被設定為 0，特徵輸入與隱藏層的神經元被大大的減少，如圖<a href="#orge7b2873">53</a>，整個模型的複雜度簡化很多。L1 正規化確實有助於將我們的複雜模型縮減為更小的泛化模型。添加正規化後，我們看到無用的功能全部變為零，並且連接線變得稀疏並顯示為灰色。倖存下來的唯一特徵是 x_1 平方和 x_2 平方，這是有道理的，因為這 2 個特徵加在一起就構成了一個圓的方程。<br />
</p>


<div id="orge7b2873" class="figure">
<p><img src="images/L1l2regularization4.png" alt="L1l2regularization4.png" /><br />
</p>
<p><span class="figure-number">Figure 53: </span>L1 正規化</p>
</div>

<p>
反觀 L2 正規化，當我們訓練它時，每個權重與神經元都還是處於活動狀態，但是非常虛弱，如圖<a href="#orgebffde7">54</a>，L1 正規化使用其中一個特徵而將某些拋棄，而 L2 正規化將同時保留特徵並使權重值保持較小。因此，使用 L1，您可以得到一個較小的模型，但預測性可能較低。。所以：<br />
</p>

<ul class="org-ul">
<li>L1 正規化：有可能導致零權重，因刪除更多特徵而使模型稀疏。<br /></li>
<li>L2 正規化：會對更大的權重值造成更大的影響，將使權重值保持較小。<br /></li>
</ul>


<div id="orgebffde7" class="figure">
<p><img src="images/L1l2regularization5.png" alt="L1l2regularization5.png" /><br />
</p>
<p><span class="figure-number">Figure 54: </span>L2 正規化</p>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgf67362c" class="outline-3">
<h3 id="orgf67362c"><span class="section-number-3">9.7</span> 循序特徵選擇法</h3>
<div class="outline-text-3" id="text-9-7">
<p>
另一種降低模型複雜度以避免過度擬合的方式是經由「特徵選擇」(feature selection)來做「降維」(dimensionality reduction)，降維的做法有二：<br />
</p>
<ul class="org-ul">
<li>特徵選擇：feature selection, 由原本的特徵中，選出一個子集合<br /></li>
<li>特徵提取：feature extraction，由原本的特徵中，導出資訊來建構新的特徵<br /></li>
</ul>

<p>
循序特徵選擇法(sequential feature selection)為貪婪演算法的一種，目標在移除不相關或相關較低的特徵，以提高計算效率，這對於不支援「正規化」的演算法來說是很有用的。「循序向後選擇」(Sequential Backward Selection, SBS)便是一個典型的循序特徵選擇法，其做法是逐一從特徵空間中移除特徵，直到只剩下所要的特徵個數。為了達到這個目的，我們要定義一個最小化的「準則函數」(criterion function), 這個準則可以簡化為「模型在移除某特徵前/後的效能差異。SBS 的 python 實作如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Sequential feature selection algorithms</span>
<span class="linenr">  2: </span>  <span style="color: #51afef;">from</span> sklearn.base <span style="color: #51afef;">import</span> clone
<span class="linenr">  3: </span>  <span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> accuracy_score
<span class="linenr">  4: </span>  <span style="color: #51afef;">from</span> itertools <span style="color: #51afef;">import</span> combinations
<span class="linenr">  5: </span>  <span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">SBS</span>():
<span class="linenr">  6: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>, estimator, k_features, scoring=accuracy_score,
<span class="linenr">  7: </span>                   test_size=<span style="color: #da8548; font-weight: bold;">0.25</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>):
<span class="linenr">  8: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">scoring</span> = scoring
<span class="linenr">  9: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">estimator</span> = clone(estimator)
<span class="linenr"> 10: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">k_features</span> = k_features
<span class="linenr"> 11: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">test_size</span> = test_size
<span class="linenr"> 12: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">random_state</span> = random_state
<span class="linenr"> 13: </span>
<span class="linenr"> 14: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">fit</span>(<span style="color: #51afef;">self</span>, X, y):
<span class="linenr"> 15: </span>
<span class="linenr"> 16: </span>          <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> =             train_test_split(X, y, test_size=<span style="color: #51afef;">self</span>.test_size,
<span class="linenr"> 17: </span>                               random_state=<span style="color: #51afef;">self</span>.random_state)
<span class="linenr"> 18: </span>
<span class="linenr"> 19: </span>          <span style="color: #dcaeea;">dim</span> = X_train.shape[<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr"> 20: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">indices_</span> = <span style="color: #c678dd;">tuple</span>(<span style="color: #c678dd;">range</span>(dim))
<span class="linenr"> 21: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">subsets_</span> = [<span style="color: #51afef;">self</span>.indices_]
<span class="linenr"> 22: </span>          <span style="color: #dcaeea;">score</span> = <span style="color: #51afef;">self</span>._calc_score(X_train, y_train, 
<span class="linenr"> 23: </span>                                   X_test, y_test, <span style="color: #51afef;">self</span>.indices_)
<span class="linenr"> 24: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">scores_</span> = [score]
<span class="linenr"> 25: </span>
<span id="coderef-fitWhile" class="coderef-off"><span class="linenr"> 26: </span>          <span style="color: #51afef;">while</span> dim &gt; <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">k_features</span>:</span>
<span class="linenr"> 27: </span>              scores = []
<span class="linenr"> 28: </span>              <span style="color: #dcaeea;">subsets</span> = []
<span class="linenr"> 29: </span>
<span class="linenr"> 30: </span>              <span style="color: #51afef;">for</span> p <span style="color: #51afef;">in</span> combinations(<span style="color: #51afef;">self</span>.indices_, r=dim - <span style="color: #da8548; font-weight: bold;">1</span>):
<span class="linenr"> 31: </span>                  <span style="color: #dcaeea;">score</span> = <span style="color: #51afef;">self</span>._calc_score(X_train, y_train, 
<span id="coderef-scoreXtest" class="coderef-off"><span class="linenr"> 32: </span>                                           X_test, y_test, p)</span>
<span class="linenr"> 33: </span>                  scores.append(score)
<span class="linenr"> 34: </span>                  subsets.append(p)
<span class="linenr"> 35: </span>
<span class="linenr"> 36: </span>              <span style="color: #dcaeea;">best</span> = np.argmax(scores)
<span class="linenr"> 37: </span>              <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">indices_</span> = subsets[best] 
<span class="linenr"> 38: </span>              <span style="color: #51afef;">self</span>.subsets_.append(<span style="color: #51afef;">self</span>.indices_)
<span class="linenr"> 39: </span>              <span style="color: #dcaeea;">dim</span> -= <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 40: </span>
<span id="coderef-bestScore" class="coderef-off"><span class="linenr"> 41: </span>              <span style="color: #51afef;">self</span>.scores_.append(scores[best])</span>
<span class="linenr"> 42: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">k_score_</span> = <span style="color: #51afef;">self</span>.scores_[-<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr"> 43: </span>
<span class="linenr"> 44: </span>          <span style="color: #51afef;">return</span> <span style="color: #51afef;">self</span>
<span class="linenr"> 45: </span>
<span class="linenr"> 46: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">transform</span>(<span style="color: #51afef;">self</span>, X):
<span class="linenr"> 47: </span>          <span style="color: #51afef;">return</span> X[:, <span style="color: #51afef;">self</span>.indices_]
<span class="linenr"> 48: </span>
<span class="linenr"> 49: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">_calc_score</span>(<span style="color: #51afef;">self</span>, X_train, y_train, X_test, y_test, indices):
<span class="linenr"> 50: </span>          <span style="color: #51afef;">self</span>.estimator.fit(X_train[:, indices], y_train)
<span class="linenr"> 51: </span>          <span style="color: #dcaeea;">y_pred</span> = <span style="color: #51afef;">self</span>.estimator.predict(X_test[:, indices])
<span class="linenr"> 52: </span>          <span style="color: #dcaeea;">score</span> = <span style="color: #51afef;">self</span>.scoring(y_test, y_pred)
<span class="linenr"> 53: </span>          <span style="color: #51afef;">return</span> score
<span class="linenr"> 54: </span>
<span class="linenr"> 55: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 56: </span>  <span style="color: #51afef;">from</span> sklearn.neighbors <span style="color: #51afef;">import</span> KNeighborsClassifier
<span class="linenr"> 57: </span>
<span class="linenr"> 58: </span>  <span style="color: #dcaeea;">knn</span> = KNeighborsClassifier(n_neighbors=<span style="color: #da8548; font-weight: bold;">5</span>)
<span class="linenr"> 59: </span>
<span class="linenr"> 60: </span>  <span style="color: #5B6268;">##</span><span style="color: #5B6268;">========</span>
<span class="linenr"> 61: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr"> 62: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr"> 63: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 64: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 65: </span>  <span style="color: #dcaeea;">df_wine</span> = pd.read_csv(<span style="color: #98be65;">'https://archive.ics.uci.edu/'</span>
<span class="linenr"> 66: </span>                      <span style="color: #98be65;">'ml/machine-learning-databases/wine/wine.data'</span>,
<span class="linenr"> 67: </span>                      header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr"> 68: </span>  df_wine.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'Class label'</span>, <span style="color: #98be65;">'Alcohol'</span>, <span style="color: #98be65;">'Malic acid'</span>, <span style="color: #98be65;">'Ash'</span>,
<span class="linenr"> 69: </span>                     <span style="color: #98be65;">'Alcalinity of ash'</span>, <span style="color: #98be65;">'Magnesium'</span>, <span style="color: #98be65;">'Total phenols'</span>,
<span class="linenr"> 70: </span>                     <span style="color: #98be65;">'Flavanoids'</span>, <span style="color: #98be65;">'Nonflavanoid phenols'</span>, <span style="color: #98be65;">'Proanthocyanins'</span>,
<span class="linenr"> 71: </span>                     <span style="color: #98be65;">'Color intensity'</span>, <span style="color: #98be65;">'Hue'</span>, <span style="color: #98be65;">'OD280/OD315 of diluted wines'</span>,
<span class="linenr"> 72: </span>                     <span style="color: #98be65;">'Proline'</span>]
<span class="linenr"> 73: </span>  <span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">y</span> = df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">1</span>:].values, df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">0</span>].values
<span class="linenr"> 74: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr"> 75: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> =    train_test_split(X, y, 
<span class="linenr"> 76: </span>                       test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>, 
<span class="linenr"> 77: </span>                       random_state=<span style="color: #da8548; font-weight: bold;">0</span>, 
<span class="linenr"> 78: </span>                       stratify=y)
<span class="linenr"> 79: </span>
<span class="linenr"> 80: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#36039;&#26009;&#27161;&#28310;&#21270;: &#21033;&#29992;preprocessing&#27169;&#32068;&#35041;&#30340;StandardScaler&#39006;&#21029;</span>
<span class="linenr"> 81: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr"> 82: </span>  sc = StandardScaler() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23526;&#20363;&#21270;&#19968;&#20491;StandardScaler&#29289;&#20214;</span>
<span class="linenr"> 83: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span class="linenr"> 84: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span class="linenr"> 85: </span>  sc.fit(X_train) 
<span class="linenr"> 86: </span>  <span style="color: #dcaeea;">X_train_std</span> = sc.transform(X_train)
<span class="linenr"> 87: </span>  <span style="color: #dcaeea;">X_test_std</span> = sc.transform(X_test)
<span class="linenr"> 88: </span>
<span class="linenr"> 89: </span>  <span style="color: #5B6268;">##</span><span style="color: #5B6268;">===</span>
<span class="linenr"> 90: </span>
<span class="linenr"> 91: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">selecting features</span>
<span id="coderef-kFeatures" class="coderef-off"><span class="linenr"> 92: </span>  <span style="color: #dcaeea;">sbs</span> = SBS(knn, k_features=<span style="color: #da8548; font-weight: bold;">1</span>)</span>
<span class="linenr"> 93: </span>  sbs.fit(X_train_std, y_train)
<span class="linenr"> 94: </span>
<span class="linenr"> 95: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plotting performance of feature subsets</span>
<span class="linenr"> 96: </span>  <span style="color: #dcaeea;">k_feat</span> = [<span style="color: #c678dd;">len</span>(k) <span style="color: #51afef;">for</span> k <span style="color: #51afef;">in</span> sbs.subsets_]
<span class="linenr"> 97: </span>
<span id="coderef-accuracyScore" class="coderef-off"><span class="linenr"> 98: </span>  plt.plot(k_feat, sbs.scores_, marker=<span style="color: #98be65;">'o'</span>)</span>
<span class="linenr"> 99: </span>  plt.ylim([<span style="color: #da8548; font-weight: bold;">0.7</span>, <span style="color: #da8548; font-weight: bold;">1.02</span>])
<span class="linenr">100: </span>  plt.ylabel(<span style="color: #98be65;">'Accuracy'</span>)
<span class="linenr">101: </span>  plt.xlabel(<span style="color: #98be65;">'Number of features'</span>)
<span class="linenr">102: </span>  plt.grid()
<span class="linenr">103: </span>  plt.tight_layout()
<span class="linenr">104: </span>  plt.savefig(<span style="color: #98be65;">'04_08.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">105: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span id="coderef-sbsSubsets" class="coderef-off"><span class="linenr">106: </span>  <span style="color: #51afef;">print</span>(sbs.subsets_) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20840;&#37096;&#21015;&#20986;&#65292;&#25214;&#21040;3&#20491;&#29305;&#24501;&#20540;&#26159;&#22312;&#31532;&#24190;&#20491;&#20301;&#32622;</span></span>
<span class="linenr">107: </span>  <span style="color: #51afef;">print</span>(<span style="color: #c678dd;">list</span>(sbs.subsets_[<span style="color: #da8548; font-weight: bold;">10</span>]))
<span class="linenr">108: </span>  <span style="color: #dcaeea;">k3</span> = <span style="color: #c678dd;">list</span>(sbs.subsets_[<span style="color: #da8548; font-weight: bold;">10</span>])
<span class="linenr">109: </span>  <span style="color: #51afef;">print</span>(df_wine.columns[<span style="color: #da8548; font-weight: bold;">1</span>:][k3])
<span class="linenr">110: </span>  <span style="color: #5B6268;">## </span><span style="color: #5B6268;">&#27604;&#36611;&#20840;&#37096;&#29305;&#24501;&#20540;&#33287;&#19977;&#20491;&#29305;&#24501;&#20540;&#30340;&#25928;&#33021;</span>
<span class="linenr">111: </span>  knn.fit(X_train_std, y_train)
<span class="linenr">112: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Training accuracy (FULL):'</span>, knn.score(X_train_std, y_train))
<span class="linenr">113: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Test accuracy (FULL):'</span>, knn.score(X_test_std, y_test))
<span class="linenr">114: </span>  knn.fit(X_train_std[:, k3], y_train)
<span class="linenr">115: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Training accuracy (K3):'</span>, knn.score(X_train_std[:,k3], y_train))
<span class="linenr">116: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Test accuracy (K3):'</span>, knn.score(X_test_std[:,k3], y_test))
<span class="linenr">117: </span>
</pre>
</div>

<pre class="example">
[(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), (0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12), (0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11), (0, 1, 2, 3, 4, 5, 6, 7, 9, 11), (0, 1, 2, 3, 4, 5, 7, 9, 11), (0, 1, 2, 3, 5, 7, 9, 11), (0, 1, 2, 3, 5, 7, 11), (0, 1, 2, 3, 5, 11), (0, 1, 2, 3, 11), (0, 1, 2, 11), (0, 1, 11), (0, 11), (0,)]
[0, 1, 11]
Index(['Alcohol', 'Malic acid', 'OD280/OD315 of diluted wines'], dtype='object')
Training accuracy (FULL): 0.967741935483871
Test accuracy (FULL): 0.9629629629629629
Training accuracy (K3): 0.9516129032258065
Test accuracy (K3): 0.9259259259259259
</pre>



<div id="org60af404" class="figure">
<p><img src="images/04_08.png" alt="04_08.png" /><br />
</p>
<p><span class="figure-number">Figure 55: </span>SBS</p>
</div>

<p>
前述實作中，k_features 參數(程式第<a href="#coderef-kFeatures" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-kFeatures');" onmouseout="CodeHighlightOff(this, 'coderef-kFeatures');">92</a>行)定義了我們希望演算法「最後要保留多少特徵」，在預設情況下，以 accuracy_score(程式第<a href="#coderef-accuracyScore" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-accuracyScore');" onmouseout="CodeHighlightOff(this, 'coderef-accuracyScore');">98</a>行)來評估模型效能。在 fit 的 while 迴圈中(<a href="#coderef-fitWhile" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-fitWhile');" onmouseout="CodeHighlightOff(this, 'coderef-fitWhile');">26</a>行)，由 itertools 模組的 combinations 方法所產生的特徵子集合會被評估並降維，直到只剩下所要的特徵個數。<br />
</p>

<p>
在每次迭代中，演算法使用內部創建的測試數據集 X_test(第<a href="#coderef-scoreXtest" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-scoreXtest');" onmouseout="CodeHighlightOff(this, 'coderef-scoreXtest');">32</a>行)來評估特徵子集合，然後留下精確度最佳的特徵子集合所得分數，加入串列 self.scores_中(第<a href="#coderef-bestScore" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-bestScore');" onmouseout="CodeHighlightOff(this, 'coderef-bestScore');">41</a>行)，之後再以這些分數來評估結果。最後的特徵子集合「行索引」會被分派到變數 self.indices_中，然後以 transform 將這些所選定的特徵轉為新的數據陣列。<br />
</p>

<p>
由圖<a href="#org60af404">55</a>可以看到，當特徵數 k={3, 7, 8, 9, 10, 11, 12}時，KNN 分類器的準確率為 100%。若進一步想確定當 k=3 時，是哪三個特徵，則可以由 sbs.subset_中逐步探索出來(程式第<a href="#coderef-sbsSubsets" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-sbsSubsets');" onmouseout="CodeHighlightOff(this, 'coderef-sbsSubsets');">106</a>行)。<br />
</p>

<p>
進一步比較「全部特徵值」以及「三個特徵值」所得出的模型效能，可以看到即使只留下三個特徵值，模型的效能仍相去不遠，更重要的是，透過降低維度，可以有效的提升運算效能。<br />
</p>
</div>
</div>

<div id="outline-container-orgeec69d3" class="outline-3">
<h3 id="orgeec69d3"><span class="section-number-3">9.8</span> 以隨機森林評估特徵的重要性</h3>
<div class="outline-text-3" id="text-9-8">
<p>
隨機森林顧名思義，是用隨機的方式建立一個森林，森林裡面有很多的決策樹組成，隨機森林的每一棵決策樹之間是沒有關聯的。在得到森林之後，當有一個新的輸入樣本進入的時候，就讓森林中的每一棵決策樹分別進行一下判斷，看看這個樣本應該屬於哪一類（對於分類演算法），然後看看哪一類被選擇最多，就預測這個樣本為那一類<sup><a id="fnr.9" class="footref" href="#fn.9">9</a></sup>。上述 SBS 演算法係將低相關的特徵刪除、留下重要的特徵；而隨機森林則是利用許多決策樹來票選最後的決定。<br />
</p>


<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">df_wine</span> = pd.read_csv(<span style="color: #98be65;">'https://archive.ics.uci.edu/'</span>
<span class="linenr"> 7: </span>                      <span style="color: #98be65;">'ml/machine-learning-databases/wine/wine.data'</span>,
<span class="linenr"> 8: </span>                      header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr"> 9: </span>  df_wine.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'Class label'</span>, <span style="color: #98be65;">'Alcohol'</span>, <span style="color: #98be65;">'Malic acid'</span>, <span style="color: #98be65;">'Ash'</span>,
<span class="linenr">10: </span>                     <span style="color: #98be65;">'Alcalinity of ash'</span>, <span style="color: #98be65;">'Magnesium'</span>, <span style="color: #98be65;">'Total phenols'</span>,
<span class="linenr">11: </span>                     <span style="color: #98be65;">'Flavanoids'</span>, <span style="color: #98be65;">'Nonflavanoid phenols'</span>, <span style="color: #98be65;">'Proanthocyanins'</span>,
<span class="linenr">12: </span>                     <span style="color: #98be65;">'Color intensity'</span>, <span style="color: #98be65;">'Hue'</span>, <span style="color: #98be65;">'OD280/OD315 of diluted wines'</span>,
<span class="linenr">13: </span>                     <span style="color: #98be65;">'Proline'</span>]
<span class="linenr">14: </span>  <span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">y</span> = df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">1</span>:].values, df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">0</span>].values
<span class="linenr">15: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">16: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(X, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>,
<span class="linenr">17: </span>                                                      random_state=<span style="color: #da8548; font-weight: bold;">0</span>,
<span class="linenr">18: </span>                                                      stratify=y)
<span class="linenr">19: </span>
<span class="linenr">20: </span>
<span class="linenr">21: </span>  <span style="color: #51afef;">from</span> sklearn.ensemble <span style="color: #51afef;">import</span> RandomForestClassifier
<span class="linenr">22: </span>  <span style="color: #dcaeea;">feat_labels</span> = df_wine.columns[<span style="color: #da8548; font-weight: bold;">1</span>:]
<span class="linenr">23: </span>  <span style="color: #dcaeea;">forest</span> = RandomForestClassifier(n_estimators=<span style="color: #da8548; font-weight: bold;">500</span>,
<span class="linenr">24: </span>                                  random_state=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">25: </span>
<span class="linenr">26: </span>  forest.fit(X_train, y_train)
<span class="linenr">27: </span>  <span style="color: #dcaeea;">importances</span> = forest.feature_importances_
<span class="linenr">28: </span>
<span class="linenr">29: </span>  <span style="color: #dcaeea;">indices</span> = np.argsort(importances)[::-<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr">30: </span>
<span class="linenr">31: </span>  <span style="color: #51afef;">for</span> f <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(X_train.shape[<span style="color: #da8548; font-weight: bold;">1</span>]):
<span class="linenr">32: </span>      <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"%2d) %-*s %f"</span> % (f + <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">30</span>, 
<span class="linenr">33: </span>                              feat_labels[indices[f]], 
<span class="linenr">34: </span>                              importances[indices[f]]))
<span class="linenr">35: </span>
<span class="linenr">36: </span>  plt.title(<span style="color: #98be65;">'Feature Importance'</span>)
<span class="linenr">37: </span>  plt.bar(<span style="color: #c678dd;">range</span>(X_train.shape[<span style="color: #da8548; font-weight: bold;">1</span>]), 
<span class="linenr">38: </span>          importances[indices],
<span class="linenr">39: </span>          align=<span style="color: #98be65;">'center'</span>)
<span class="linenr">40: </span><span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">41: </span>  plt.xticks(<span style="color: #c678dd;">range</span>(X_train.shape[<span style="color: #da8548; font-weight: bold;">1</span>]), 
<span class="linenr">42: </span>             feat_labels[indices], rotation=<span style="color: #da8548; font-weight: bold;">90</span>)
<span class="linenr">43: </span>  plt.xlim([-<span style="color: #da8548; font-weight: bold;">1</span>, X_train.shape[<span style="color: #da8548; font-weight: bold;">1</span>]])
<span class="linenr">44: </span>  plt.tight_layout()
<span class="linenr">45: </span>  plt.savefig(<span style="color: #98be65;">'04_09.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">46: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">47: </span>
</pre>
</div>

<pre class="example">
 1) Proline                        0.185453
 2) Flavanoids                     0.174751
 3) Color intensity                0.143920
 4) OD280/OD315 of diluted wines   0.136162
 5) Alcohol                        0.118529
 6) Hue                            0.058739
 7) Total phenols                  0.050872
 8) Magnesium                      0.031357
 9) Malic acid                     0.025648
10) Proanthocyanins                0.025570
11) Alcalinity of ash              0.022366
12) Nonflavanoid phenols           0.013354
13) Ash                            0.013279
</pre>


<div id="orga22cdae" class="figure">
<p><img src="images/04_09.png" alt="04_09.png" /><br />
</p>
<p><span class="figure-number">Figure 56: </span>FandomForest</p>
</div>


<p>
由圖<a href="#orga22cdae">56</a>的特徵排序為從 500 棵「決策樹」的「不純度」中最具「判別性」的特徵排列順序，<br />
</p>
</div>
</div>
</div>

<div id="outline-container-org19d002e" class="outline-2">
<h2 id="org19d002e"><span class="section-number-2">10</span> 降維來壓縮數據</h2>
<div class="outline-text-2" id="text-10">
</div>
<div id="outline-container-org5afc65e" class="outline-3">
<h3 id="org5afc65e"><span class="section-number-3">10.1</span> 以主成份分析(PCA)對非監督式數據壓縮</h3>
<div class="outline-text-3" id="text-10-1">
<p>
「特徵選擇」需要原始的「特徵」；而「特徵提取」則是在於「轉換」數據，或是「投影」(project)數據到一個新的「特徵空間」，特徵提取不僅能改善儲存空間的使用或是提高學習演算法的計算效率，也可以有效地藉由降低「維數災難」來提高預測的正確性，特別是在處理非正規化模型時。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="orgc78d67e"></a>主成分分析 1<br />
<div class="outline-text-4" id="text-10-1-1">
<p>
「主成份分析」(principal component analysis, PCA)是一種非監督式線性變換技術」，經常應用於「特徵提取」與「降維」，其他應用包括「探索式數據分析」和「股票市場分析」中的雜訊消除、生物資訊學領域中的「基因數據分析」與「基因表現層分析」。<br />
</p>

<p>
這邊先簡單說維度詛咒，預測/分類能力通常是隨著維度數(變數)增加而上生，但當模型樣本數沒有繼續增加的情況下，預測/分類能力增加到一定程度之後，預測/分類能力會隨著維度的繼續增加而減小<sup><a id="fnr.10" class="footref" href="#fn.10">10</a></sup>。<br />
</p>

<p>
主成份分析的基本假設是希望資料可以在特徵空間找到一個投影軸(向量)投影後可以得到這組資料的最大變異量。以圖<a href="#org071a1d5">57</a>為例，PCA 的目的在於找到一個向量可以投影(圖中紅色的線)，讓投影後的資料變異量最大。<br />
</p>


<div id="org071a1d5" class="figure">
<p><img src="images/pca-1.png" alt="pca-1.png" /><br />
</p>
<p><span class="figure-number">Figure 57: </span>PCA-1 [fn:31]</p>
</div>
</div>

<ol class="org-ol">
<li><a id="org1a36bf7"></a>投影(projection)<br />
<div class="outline-text-5" id="text-10-1-1-1">
<p>
假設有一個點藍色的點對原點的向量為\(\vec{x_i}\)，有一個軸為 v，他的投影(正交為虛線和藍色線為 90 度)向量為紅色那條線，紅色線和黑色線的夾角為\(\theta\)，\(\vec{x_i}\)投影長度為藍色線，其長度公式為\(\left\|{x_i}\right\|cos\theta\)。<br />
</p>


<div id="org3353896" class="figure">
<p><img src="images/pca-2.png" alt="pca-2.png" /><br />
</p>
<p><span class="figure-number">Figure 58: </span>PCA-2 [fn:31]</p>
</div>

<p>
假設有一組資料六個點(\(x_1, x_2, x_3, x_4, x_5, x_6\))，有兩個投影向量\(\vec{v}\)和\(\vec{v'}\)(如圖<a href="#org6482a15">59</a>)，投影下來後，資料在\(\vec{v'}\)上的變異量比\(v\)上的變異量小。<br />
</p>


<div id="org6482a15" class="figure">
<p><img src="images/pca-3.png" alt="pca-3.png" /><br />
</p>
<p><span class="figure-number">Figure 59: </span>PCA-3 [fn:31]</p>
</div>

<p>
從圖<a href="#org40f05b1">60</a>也可以看出這些資料在\(v\)向量資料投影后有較大的變異量(較之投影於\(\vec{v'}\))。<br />
</p>


<div id="org40f05b1" class="figure">
<p><img src="images/pca-4.png" alt="pca-4.png" /><br />
</p>
<p><span class="figure-number">Figure 60: </span>PCA-4 [fn:31]</p>
</div>
</div>
</li>

<li><a id="org025479f"></a>變異量的計算<br />
<div class="outline-text-5" id="text-10-1-1-2">
<p>
典型的變異數公式如下：<br />
\(\sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (X -\mu)^2}\)<br />
</p>

<p>
若要計算前述所有資料點(\(x_1, x_2, x_3, x_4, x_5, x_6\))在\(v\)上的投影\(v^Tx_1, v^Tx_2, v^Tx_3, v^Tx_4, v^Tx_5, v^Tx_6\) ，則其變異數公式為<br />
\(\sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i -\mu)^2\)<br />
</p>

<p>
又因 PCA 之前提假設是將資 shift 到 0(即，變異數的平均數為 0)以簡化運算，其公式會變為<br />
\(\sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i -\mu)^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i - 0)^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i)^2\)<br />
</p>

<p>
而機器學習處理的資料點通常為多變量，故上述式子會以矩陣方式呈現<br />
</p>

<p>
\(\Sigma = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i)(v^Tx_i)^T = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_iv^Tx_iv) = v^T(\frac{1}{N}\sum\limits_{i=1}^Nx_iX_i^T)v = v^TCv\)<br />
</p>

<p>
其中 C 為共變異數矩陣(covariance matrix)<br />
</p>

<p>
\(C=\frac{1}{n}\sum\limits_{i=1}^nx_ix_i^T,\cdots x_i = \begin{bmatrix}
x_1^{(1)}     \\
x_2^{(2)}     \\
\vdots  \\
x_i^{(d)}     \\
\end{bmatrix}\)<br />
</p>

<p>
主成份分析的目的則是在找出一個投影向量讓投影後的資料變異量最大化（最佳化問題）：<br />
</p>

<p>
\(v = \mathop{\arg\max}\limits_{x \in \mathcal{R}^d,\left\|v\right\|=1} {v^TCv}\)<br />
</p>

<p>
進一步轉成 Lagrange、透過偏微分求解，其實就是解 C 的特徵值(eigenvalue, \(\lambda\))和特徵向量(eigenvector, \(v\))。<br />
</p>
</div>
</li>
</ol>
</li>

<li><a id="org44950eb"></a>主成份分析 2<br />
<div class="outline-text-4" id="text-10-1-2">
<p>
回到前述例子(身高和體重)，下左圖，經由 PCA 可以萃取出兩個特徵成分(投影軸，下圖右的兩條垂直的紅線，較長的紅線軸為變異量較大的主成份)。此範例算最大主成份的變異量為 13.26，第二大主成份的變異量為 1.23。<br />
</p>


<div id="org326ca68" class="figure">
<p><img src="images/pca-5.png" alt="pca-5.png" /><br />
</p>
<p><span class="figure-number">Figure 61: </span>PCA-5 [fn:31]</p>
</div>

<p>
PCA 投影完的資料為下圖，從下圖可知，PC1 的變異足以表示此筆資料資訊。<br />
</p>


<div id="org264853b" class="figure">
<p><img src="images/pca-6.png" alt="pca-6.png" /><br />
</p>
<p><span class="figure-number">Figure 62: </span>PCA-6 [fn:31]</p>
</div>

<p>
此做法可以有效的減少維度數，但整體變異量並沒有減少太多，此例從兩個變成只有一個，但變異量卻可以保留(13.26/(13.26+1.23)= 91.51%)，兩維度的資料做 PCA，對資料進行降維比較沒有感覺，但講解圖例比較容易。<br />
</p>
</div>
</li>

<li><a id="orga4a1df6"></a>主成份分析的主要步驟<br />
<div class="outline-text-4" id="text-10-1-3">
<ol class="org-ol">
<li>標準化數據集<br /></li>
<li>建立共變數矩陣<br /></li>
<li>從共變數矩陣分解出特徵值與特徵向量<br /></li>
<li>以遞減方式對特徵值進行排序，以便對特徵向量排名<br /></li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 5: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 6: </span>  <span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> PCA
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Extracting the principal components step-by-step</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">df_wine</span> = pd.read_csv(<span style="color: #98be65;">'https://archive.ics.uci.edu/ml/'</span>
<span class="linenr">11: </span>                        <span style="color: #98be65;">'machine-learning-databases/wine/wine.data'</span>,
<span class="linenr">12: </span>                        header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr">13: </span>
<span class="linenr">14: </span>  df_wine.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'Class label'</span>, <span style="color: #98be65;">'Alcohol'</span>, <span style="color: #98be65;">'Malic acid'</span>, <span style="color: #98be65;">'Ash'</span>,
<span class="linenr">15: </span>                     <span style="color: #98be65;">'Alcalinity of ash'</span>, <span style="color: #98be65;">'Magnesium'</span>, <span style="color: #98be65;">'Total phenols'</span>,
<span class="linenr">16: </span>                     <span style="color: #98be65;">'Flavanoids'</span>, <span style="color: #98be65;">'Nonflavanoid phenols'</span>, <span style="color: #98be65;">'Proanthocyanins'</span>,
<span class="linenr">17: </span>                     <span style="color: #98be65;">'Color intensity'</span>, <span style="color: #98be65;">'Hue'</span>,
<span class="linenr">18: </span>                     <span style="color: #98be65;">'OD280/OD315 of diluted wines'</span>, <span style="color: #98be65;">'Proline'</span>]
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #51afef;">print</span>(df_wine.head())
<span class="linenr">21: </span>
<span class="linenr">22: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Splitting the data into 70% training and 30% test subsets.</span>
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">y</span> = df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">1</span>:].values, df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">0</span>].values
<span class="linenr">25: </span>
<span class="linenr">26: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(X, y,
<span class="linenr">27: </span>                                     test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>,
<span class="linenr">28: </span>                                     stratify=y, random_state=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">29: </span>
<span class="linenr">30: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1. Standardizing the data.</span>
<span class="linenr">31: </span>  <span style="color: #dcaeea;">sc</span> = StandardScaler()
<span class="linenr">32: </span>  <span style="color: #dcaeea;">X_train_std</span> = sc.fit_transform(X_train)
<span class="linenr">33: </span>  <span style="color: #dcaeea;">X_test_std</span> = sc.transform(X_test)
<span class="linenr">34: </span>
<span class="linenr">35: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">2. Eigendecomposition of the covariance matrix.</span>
<span class="linenr">36: </span>  <span style="color: #dcaeea;">cov_mat</span> = np.cov(X_train_std.T)
<span class="linenr">37: </span>  <span style="color: #dcaeea;">eigen_vals</span>, <span style="color: #dcaeea;">eigen_vecs</span> = np.linalg.eig(cov_mat)
<span class="linenr">38: </span>
<span class="linenr">39: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'\nEigenvalues \n%s'</span> % eigen_vals)
<span class="linenr">40: </span>
<span class="linenr">41: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Total and explained variance</span>
<span class="linenr">42: </span>
<span class="linenr">43: </span>  <span style="color: #dcaeea;">tot</span> = <span style="color: #c678dd;">sum</span>(eigen_vals)
<span class="linenr">44: </span>  <span style="color: #dcaeea;">var_exp</span> = [(i / tot) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">sorted</span>(eigen_vals, reverse=<span style="color: #a9a1e1;">True</span>)]
<span class="linenr">45: </span>  <span style="color: #dcaeea;">cum_var_exp</span> = np.cumsum(var_exp)
<span class="linenr">46: </span>
<span class="linenr">47: </span>  plt.bar(<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">14</span>), var_exp, alpha=<span style="color: #da8548; font-weight: bold;">0.5</span>, align=<span style="color: #98be65;">'center'</span>,
<span class="linenr">48: </span>          label=<span style="color: #98be65;">'individual explained variance'</span>)
<span class="linenr">49: </span>  plt.step(<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">14</span>), cum_var_exp, where=<span style="color: #98be65;">'mid'</span>,
<span class="linenr">50: </span>           label=<span style="color: #98be65;">'cumulative explained variance'</span>)
<span class="linenr">51: </span>  plt.ylabel(<span style="color: #98be65;">'Explained variance ratio'</span>)
<span class="linenr">52: </span>  plt.xlabel(<span style="color: #98be65;">'Principal component index'</span>)
<span class="linenr">53: </span>  plt.legend(loc=<span style="color: #98be65;">'best'</span>)
<span class="linenr">54: </span>  plt.tight_layout()
<span class="linenr">55: </span>  plt.savefig(<span style="color: #98be65;">'05_02.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">56: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">57: </span>
</pre>
</div>

<pre class="example">
   Class label  Alcohol  ...  OD280/OD315 of diluted wines  Proline
0            1    14.23  ...                          3.92     1065
1            1    13.20  ...                          3.40     1050
2            1    13.16  ...                          3.17     1185
3            1    14.37  ...                          3.45     1480
4            1    13.24  ...                          2.93      735

[5 rows x 14 columns]

Eigenvalues 
[4.84274532 2.41602459 1.54845825 0.96120438 0.84166161 0.6620634
 0.51828472 0.34650377 0.3131368  0.10754642 0.21357215 0.15362835
 0.1808613 ]
</pre>


<div id="orgd4ca070" class="figure">
<p><img src="images/05_02.png" alt="05_02.png" /><br />
</p>
<p><span class="figure-number">Figure 63: </span>Principal component index</p>
</div>

<p>
雖然上圖的「解釋變異數」圖有點類似隨機森林評估特徵值重要性的結果，但二者最大的不同處在於 PCA 為一種非監督式方法，也就是說，關於類別標籤資訊是被忽略的。<br />
</p>
</div>
</li>

<li><a id="org2f4d6c4"></a>特徵轉換<br />
<div class="outline-text-4" id="text-10-1-4">
<p>
在分解「共變數矩陣」成為「特徵對」後，接下來要將資料集轉換為新的「主成份」，其步驟如下：<br />
</p>
<ol class="org-ol">
<li>選取\(k\)個最大特徵值所對應的 k 個特徵向量，其中\(k\)為新「特徵空間」的維數(\(k \le d\))。<br /></li>
<li>用最前面的\(k\)個特徵向量建立「投影矩陣」(project matrix)\(W\)。<br /></li>
<li>使用投影矩陣\(W\)，輸入值為\(d\)維數據集、輸出值為新的\(k\)維「特徵子空間」。<br /></li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 5: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 6: </span>  <span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> PCA
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Extracting the principal components step-by-step</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">df_wine</span> = pd.read_csv(<span style="color: #98be65;">'https://archive.ics.uci.edu/ml/'</span>
<span class="linenr">11: </span>                          <span style="color: #98be65;">'machine-learning-databases/wine/wine.data'</span>,
<span class="linenr">12: </span>                          header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr">13: </span>
<span class="linenr">14: </span>  <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',</span>
<span class="linenr">15: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Alcalinity of ash', 'Magnesium', 'Total phenols',</span>
<span class="linenr">16: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',</span>
<span class="linenr">17: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Color intensity', 'Hue',</span>
<span class="linenr">18: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'OD280/OD315 of diluted wines', 'Proline']</span>
<span class="linenr">19: </span>
<span class="linenr">20: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Splitting the data into 70% training and 30% test subsets.</span>
<span class="linenr">21: </span>  <span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">y</span> = df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">1</span>:].values, df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">0</span>].values
<span class="linenr">22: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(X, y,
<span class="linenr">23: </span>                                      test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>,
<span class="linenr">24: </span>                                      stratify=y, random_state=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">25: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1. Standardizing the data.</span>
<span class="linenr">26: </span>  <span style="color: #dcaeea;">sc</span> = StandardScaler()
<span class="linenr">27: </span>  <span style="color: #dcaeea;">X_train_std</span> = sc.fit_transform(X_train)
<span class="linenr">28: </span>  <span style="color: #dcaeea;">X_test_std</span> = sc.transform(X_test)
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">2. Eigendecomposition of the covariance matrix.</span>
<span class="linenr">30: </span>  <span style="color: #dcaeea;">cov_mat</span> = np.cov(X_train_std.T)
<span class="linenr">31: </span>  <span style="color: #dcaeea;">eigen_vals</span>, <span style="color: #dcaeea;">eigen_vecs</span> = np.linalg.eig(cov_mat)
<span class="linenr">32: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Total and explained variance</span>
<span class="linenr">33: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">tot = sum(eigen_vals)</span>
<span class="linenr">34: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]</span>
<span class="linenr">35: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">cum_var_exp = np.cumsum(var_exp)</span>
<span class="linenr">36: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Feature transformation</span>
<span class="linenr">37: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Make a list of (eigenvalue, eigenvector) tuples</span>
<span class="linenr">38: </span>  <span style="color: #dcaeea;">eigen_pairs</span> = [(np.<span style="color: #c678dd;">abs</span>(eigen_vals[i]), eigen_vecs[:, i])
<span class="linenr">39: </span>                  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(eigen_vals))]
<span class="linenr">40: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Sort the (eigenvalue, eigenvector) tuples from high to low</span>
<span class="linenr">41: </span>  eigen_pairs.sort(key=<span style="color: #51afef;">lambda</span> k: k[<span style="color: #da8548; font-weight: bold;">0</span>], reverse=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">42: </span>  <span style="color: #dcaeea;">w</span> = np.hstack((eigen_pairs[<span style="color: #da8548; font-weight: bold;">0</span>][<span style="color: #da8548; font-weight: bold;">1</span>][:, np.newaxis],
<span class="linenr">43: </span>                  eigen_pairs[<span style="color: #da8548; font-weight: bold;">1</span>][<span style="color: #da8548; font-weight: bold;">1</span>][:, np.newaxis]))
<span class="linenr">44: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Matrix W:\n'</span>, w)
<span id="coderef-x-train-dot" class="coderef-off"><span class="linenr">45: </span>  <span style="color: #51afef;">print</span>(X_train_std[<span style="color: #da8548; font-weight: bold;">0</span>].dot(w))</span>
<span id="coderef-x-train-pca" class="coderef-off"><span class="linenr">46: </span>  X_train_pca = X_train_std.dot(w)</span>
<span class="linenr">47: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot</span>
<span class="linenr">48: </span>  <span style="color: #dcaeea;">colors</span> = [<span style="color: #98be65;">'r'</span>, <span style="color: #98be65;">'b'</span>, <span style="color: #98be65;">'g'</span>]
<span class="linenr">49: </span>  <span style="color: #dcaeea;">markers</span> = [<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>]
<span class="linenr">50: </span>
<span class="linenr">51: </span>  <span style="color: #51afef;">for</span> l, c, m <span style="color: #51afef;">in</span> <span style="color: #c678dd;">zip</span>(np.unique(y_train), colors, markers):
<span class="linenr">52: </span>      plt.scatter(X_train_pca[y_train == l, <span style="color: #da8548; font-weight: bold;">0</span>], 
<span class="linenr">53: </span>                  X_train_pca[y_train == l, <span style="color: #da8548; font-weight: bold;">1</span>], 
<span class="linenr">54: </span>                  c=c, label=l, marker=m)
<span class="linenr">55: </span>
<span class="linenr">56: </span>  plt.xlabel(<span style="color: #98be65;">'PC 1'</span>)
<span class="linenr">57: </span>  plt.ylabel(<span style="color: #98be65;">'PC 2'</span>)
<span class="linenr">58: </span>  plt.legend(loc=<span style="color: #98be65;">'lower left'</span>)
<span class="linenr">59: </span>  plt.tight_layout()
<span class="linenr">60: </span>  plt.savefig(<span style="color: #98be65;">'05_03.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">61: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">62: </span>
</pre>
</div>

<pre class="example">
Matrix W:
 [[-0.13724218  0.50303478]
 [ 0.24724326  0.16487119]
 [-0.02545159  0.24456476]
 [ 0.20694508 -0.11352904]
 [-0.15436582  0.28974518]
 [-0.39376952  0.05080104]
 [-0.41735106 -0.02287338]
 [ 0.30572896  0.09048885]
 [-0.30668347  0.00835233]
 [ 0.07554066  0.54977581]
 [-0.32613263 -0.20716433]
 [-0.36861022 -0.24902536]
 [-0.29669651  0.38022942]]
[2.38299011 0.45458499]
</pre>

<p>
使用上述程式碼產生的 13*2 維的投影矩陣可以轉換一個樣本\(x\)(以\(1 \times 13\)維的列向量表示)到 PCA 子空間(\(x'\))(前兩個主成份)：\(x' = xW\)(程式碼第<a href="#coderef-x-train-dot" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-x-train-dot');" onmouseout="CodeHighlightOff(this, 'coderef-x-train-dot');">45</a>行)；同樣的，我們也可以將整個\(124 \times 13\)維的訓練數據集轉換到兩個主成份(\(124 \times 2\)維)(程式第<a href="#coderef-x-train-pca" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-x-train-pca');" onmouseout="CodeHighlightOff(this, 'coderef-x-train-pca');">46</a>行)，最後，將轉換過的\(124 \times 2\)維矩陣以二維散點圖表示：<br />
</p>


<div id="orgca55033" class="figure">
<p><img src="images/05_03.png" alt="05_03.png" /><br />
</p>
<p><span class="figure-number">Figure 64: </span>05_03</p>
</div>

<p>
由圖<a href="#orgca55033">64</a>中可看出，與第二個主成份(y 軸)相比，數據沿著第一主成份(x 軸)的分散程度更嚴重，而由此圖也可判斷，該數據應可以一個「線性分類器」進行有效分類。<br />
</p>
</div>
</li>

<li><a id="org6e8f5e5"></a>以 Scikit-learn 進行主成份分析<br />
<div class="outline-text-4" id="text-10-1-5">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr"> 4: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr"> 5: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 6: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 7: </span>  <span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> PCA
<span class="linenr"> 8: </span>  <span style="color: #51afef;">from</span> sklearn.linear_model <span style="color: #51afef;">import</span> LogisticRegression
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Extracting the principal components step-by-step</span>
<span class="linenr">11: </span>
<span class="linenr">12: </span>  <span style="color: #dcaeea;">df_wine</span> = pd.read_csv(<span style="color: #98be65;">'https://archive.ics.uci.edu/ml/'</span>
<span class="linenr">13: </span>                          <span style="color: #98be65;">'machine-learning-databases/wine/wine.data'</span>,
<span class="linenr">14: </span>                          header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',</span>
<span class="linenr">17: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Alcalinity of ash', 'Magnesium', 'Total phenols',</span>
<span class="linenr">18: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',</span>
<span class="linenr">19: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Color intensity', 'Hue',</span>
<span class="linenr">20: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'OD280/OD315 of diluted wines', 'Proline']</span>
<span class="linenr">21: </span>
<span class="linenr">22: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Splitting the data into 70% training and 30% test subsets.</span>
<span class="linenr">23: </span>  <span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">y</span> = df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">1</span>:].values, df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">0</span>].values
<span class="linenr">24: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(X, y,
<span class="linenr">25: </span>                                      test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>,
<span class="linenr">26: </span>                                      stratify=y, random_state=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">27: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1. Standardizing the data.</span>
<span class="linenr">28: </span>  <span style="color: #dcaeea;">sc</span> = StandardScaler()
<span class="linenr">29: </span>  <span style="color: #dcaeea;">X_train_std</span> = sc.fit_transform(X_train)
<span class="linenr">30: </span>  <span style="color: #dcaeea;">X_test_std</span> = sc.transform(X_test)
<span class="linenr">31: </span>
<span class="linenr">32: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr">33: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr">34: </span>      <span style="color: #dcaeea;">markers</span> = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr">35: </span>      <span style="color: #dcaeea;">colors</span> = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr">36: </span>      <span style="color: #dcaeea;">cmap</span> = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr">37: </span>
<span class="linenr">38: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr">39: </span>      <span style="color: #dcaeea;">x1_min</span>, <span style="color: #dcaeea;">x1_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">40: </span>      <span style="color: #dcaeea;">x2_min</span>, <span style="color: #dcaeea;">x2_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">41: </span>      <span style="color: #dcaeea;">xx1</span>, <span style="color: #dcaeea;">xx2</span> = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr">42: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr">43: </span>      <span style="color: #dcaeea;">Z</span> = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr">44: </span>      <span style="color: #dcaeea;">Z</span> = Z.reshape(xx1.shape)
<span class="linenr">45: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.4</span>, cmap=cmap)
<span class="linenr">46: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr">47: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr">48: </span>
<span class="linenr">49: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot class samples</span>
<span class="linenr">50: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr">51: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>], 
<span class="linenr">52: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">53: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.6</span>, 
<span class="linenr">54: </span>                      c=cmap(idx),
<span class="linenr">55: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr">56: </span>                      marker=markers[idx], 
<span class="linenr">57: </span>                      label=cl)
<span class="linenr">58: </span>
<span class="linenr">59: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Training logistic regression classifier using the first 2 principal components.</span>
<span class="linenr">60: </span>  <span style="color: #dcaeea;">pca</span> = PCA(n_components=<span style="color: #da8548; font-weight: bold;">2</span>)
<span id="coderef-pca-fit" class="coderef-off"><span class="linenr">61: </span>  <span style="color: #dcaeea;">X_train_pca</span> = pca.fit_transform(X_train_std)</span>
<span class="linenr">62: </span>  <span style="color: #dcaeea;">X_test_pca</span> = pca.transform(X_test_std)
<span class="linenr">63: </span>
<span class="linenr">64: </span>  <span style="color: #dcaeea;">lr</span> = LogisticRegression()
<span class="linenr">65: </span>  <span style="color: #dcaeea;">lr</span> = lr.fit(X_train_pca, y_train)
<span class="linenr">66: </span>
<span class="linenr">67: </span>  plot_decision_regions(X_train_pca, y_train, classifier=lr)
<span class="linenr">68: </span>  plt.xlabel(<span style="color: #98be65;">'PC 1'</span>)
<span class="linenr">69: </span>  plt.ylabel(<span style="color: #98be65;">'PC 2'</span>)
<span class="linenr">70: </span>  plt.legend(loc=<span style="color: #98be65;">'lower left'</span>)
<span class="linenr">71: </span>  plt.tight_layout()
<span class="linenr">72: </span>  plt.savefig(<span style="color: #98be65;">'05_04.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">73: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">74: </span>  plot_decision_regions(X_test_pca, y_test, classifier=lr)
<span class="linenr">75: </span>  plt.xlabel(<span style="color: #98be65;">'PC 1'</span>)
<span class="linenr">76: </span>  plt.ylabel(<span style="color: #98be65;">'PC 2'</span>)
<span class="linenr">77: </span>  plt.legend(loc=<span style="color: #98be65;">'lower left'</span>)
<span class="linenr">78: </span>  plt.tight_layout()
<span class="linenr">79: </span>  plt.savefig(<span style="color: #98be65;">'05_05.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">80: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
</pre>
</div>



<p>
PCA 類別是 scikit-learn 中許多轉換類別之一，首先使用訓練數據集來 fit 模型並轉換數據集(程式第<a href="#coderef-pca-fit" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-pca-fit');" onmouseout="CodeHighlightOff(this, 'coderef-pca-fit');">61</a>行)，最後以 Logistic 迴歸對數據進行分類。圖<a href="#org573e380">65</a>為訓練集資料的分類結果，圖<a href="#org07a0adc">66</a>測為測試資料集分類結果，可以看出二者差異不大。<br />
</p>


<div id="org573e380" class="figure">
<p><img src="images/05_04.png" alt="05_04.png" /><br />
</p>
<p><span class="figure-number">Figure 65: </span>PCA 訓練數據</p>
</div>


<div id="org07a0adc" class="figure">
<p><img src="images/05_05.png" alt="05_05.png" /><br />
</p>
<p><span class="figure-number">Figure 66: </span>PCA 測試數據</p>
</div>
</div>
</li>
</ol>
</div>


<div id="outline-container-org9dee650" class="outline-3">
<h3 id="org9dee650"><span class="section-number-3">10.2</span> 利用線性判別分析(LDA)做監督式數據壓縮</h3>
<div class="outline-text-3" id="text-10-2">
<p>
LDA 的全稱是 Linear Discriminant Analysis（線性判別分析），是一種 supervised learning。因為是由 Fisher 在 1936 年提出的，所以也叫 Fisher&rsquo;s Linear Discriminant。「線性判別分析」(linear discriminant analysis, LDA)為一種用來做「特徵提取」的技術，藉由降維來處理「維數災難」，可提高非正規化模型的計算效率。PCA 在於找出一個在數據集中最大化變異數的正交成分軸； 而 LDA 則是要找出可以最佳化類別分離的特徵子空間。<br />
</p>

<p>
從主觀的理解上，主成分分析到底是什麼？它其實是對數據在高維空間下的一個投影轉換，通過一定的投影規則將原來從一個角度看到的多個維度映射成較少的維度。到底什麼是映射，下面的圖就可以很好地解釋這個問題——正常角度看是兩個半橢圓形分佈的數據集，但經過旋轉（映射）之後是兩條線性分佈數據集。<sup><a id="fnr.11" class="footref" href="#fn.11">11</a></sup><br />
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


</table>
<p>
[[file:<img src="| images/lda-rot-1.jpg" alt="lda-rot-1.jpg" /> | images/lda-rot-2.jpg]] | [[file:<img src="images/lda-rot-3.jpg" alt="lda-rot-3.jpg" /> | images/lda-rot-4.jpg]] |<br />
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">3</td>
<td class="org-right">4</td>
</tr>
</tbody>
</table>
<p>
[[file:<img src="| images/lda-rot-5.jpg" alt="lda-rot-5.jpg" /> | images/lda-rot-6.jpg]] | [[file:<img src="images/lda-rot-7.jpg" alt="lda-rot-7.jpg" /> | images/lda-rot-8.jpg]] |<br />
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-right">5</td>
<td class="org-right">6</td>
<td class="org-right">7</td>
<td class="org-right">8</td>
</tr>
</tbody>
</table>

<p>
LDA 與 PCA 都是常用的降維方法，二者的區別在於<sup><a id="fnr.11.100" class="footref" href="#fn.11">11</a></sup>：<br />
</p>
<ul class="org-ul">
<li>出發思想不同。PCA 主要是從特徵的協方差角度，去找到比較好的投影方式，即選擇樣本點投影具有最大方差的方向（ 在信號處理中認為信號具有較大的方差，噪聲有較小的方差，信噪比就是信號與噪聲的方差比，越大越好。）；而 LDA 則更多的是考慮了分類標籤信息，尋求投影后不同類別之間數據點距離更大化以及同一類別數據點距離最小化，即選擇分類性能最好的方向。<br /></li>
<li>學習模式不同。PCA 屬於無監督式學習，因此大多場景下只作為數據處理過程的一部分，需要與其他算法結合使用，例如將 PCA 與聚類、判別分析、回歸分析等組合使用；LDA 是一種監督式學習方法，本身除了可以降維外，還可以進行預測應用，因此既可以組合其他模型一起使用，也可以獨立使用。<br /></li>
<li>降維後可用維度數量不同。LDA 降維後最多可生成 C-1 維子空間（分類標籤數-1），因此 LDA 與原始維度 N 數量無關，只有數據標籤分類數量有關；而 PCA 最多有 n 維度可用，即最大可以選擇全部可用維度。<br /></li>
</ul>

<p>
圖<a href="#orgbed7085">67</a>左側是 PCA 的降維思想，它所作的只是將整組數據整體映射到最方便表示這組數據的坐標軸上，映射時沒有利用任何數據內部的分類信息。因此，雖然 PCA 後的數據在表示上更加方便（降低了維數並能最大限度的保持原有信息），但在分類上也許會變得更加困難；圖<a href="#orgbed7085">67</a>右側是 LDA 的降維思想，可以看到 LDA 充分利用了數據的分類信息，將兩組數據映射到了另外一個坐標軸上，使得數據更易區分了（在低維上就可以區分，減少了運算量）。<br />
</p>


<div id="orgbed7085" class="figure">
<p><img src="images/pca-lda.png" alt="pca-lda.png" /><br />
</p>
<p><span class="figure-number">Figure 67: </span>PCA LDA 差異</p>
</div>

<p>
線性判別分析 LDA 算法由於其簡單有效性在多個領域都得到了廣泛地應用，是目前機器學習、數據挖掘領域經典且熱門的一個算法；但是算法本身仍然存在一些侷限性：<br />
</p>
<ul class="org-ul">
<li>當樣本數量遠小於樣本的特徵維數，樣本與樣本之間的距離變大使得距離度量失效，使 LDA 算法中的類內、類間離散度矩陣奇異，不能得到最優的投影方向，在人臉識別領域中表現得尤為突出<br /></li>
<li>LDA 不適合對非高斯分佈的樣本進行降維<br /></li>
<li>LDA 在樣本分類信息依賴方差而不是均值時，效果不好<br /></li>
<li>LDA 可能過度擬合數據<br /></li>
</ul>
</div>
</div>

<div id="outline-container-org35a9c43" class="outline-3">
<h3 id="org35a9c43"><span class="section-number-3">10.3</span> 利用核主成份分析(KPCA)處理非線性對應</h3>
</div>

<div id="outline-container-org69ecb2c" class="outline-3">
<h3 id="org69ecb2c"><span class="section-number-3">10.4</span> 相關資源</h3>
<div class="outline-text-3" id="text-10-4">
<ul class="org-ul">
<li><a href="https://blog.csdn.net/kuweicai/article/details/79255270">主成分分析（PCA）和線性判別分析（LDA）原理簡介</a><br /></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgcd65d61" class="outline-2">
<h2 id="orgcd65d61"><span class="section-number-2">11</span> 以少量資料集實做 CNN</h2>
<div class="outline-text-2" id="text-11">
<p>
使用少量資料訓練影像分類在實務的電腦視覺應用上十分常見，此處所謂少量樣本從幾百到幾萬張都算在內。此處以 4000 張為例(2000 cats v.s. 2000 dogs)，過程中使用 2000 張來訓練、1000 張用來驗證、1000 張用來測試。接下來導入以下技術來克服 overfitting:<br />
</p>

<ul class="org-ul">
<li>資料擴增法(data augmentation):這是常用於減輕電腦視覺 overfitting 的強大技術，可以改善神經網路的成效，提升到 82%的準確率。<br /></li>
<li>預先訓練神經網路的特徵萃取法(feature extraction with a pretrained network):應用於少量資料集的基本技術，可使神經網路成效達到 90%~96%的準確度。<br /></li>
<li>微調預先訓練神經網路法(fine-tuning a pretrained network):也是常用於深度學習少量資料集的技術，將使神經網路準確率提升到 97%。<br /></li>
</ul>
</div>

<div id="outline-container-org63e3959" class="outline-3">
<h3 id="org63e3959"><span class="section-number-3">11.1</span> 深度學習與少量資料的相關性</h3>
<div class="outline-text-3" id="text-11-1">
<p>
深度學習的基本特色是在它能自行在訓練資料中找到有趣的特徵，而不需要人為介入，但這只有在具備大量訓練樣本時才成立，特別是對於像圖片這類高維度(high-dimensional)的輸入樣本。所以也有人說深度學習一定要有大量資料才能進行。<br />
</p>

<p>
然而樣本數與神經網路的大小與深度息息相關。只用幾十個樣本不可能訓練出可以解決複雜問題的卷積神經網路；相反的，如果只是要用來解決簡單任務，而且已經做好了 well-regularized 的小 model，那麼幾百個樣本或許就足夠了。因為卷積神經網路可以學習局部 pattern 且具平移不變性，所以在感知問題上具有高度的資料效率性。<br />
</p>

<p>
此外，本質上，深度學習 model 是可高度再利用的。例如，使用大規模資料集訓練的影像 model 或語音轉文字的 model，只要進行小小的更改，便可以重新用於其他不同問題上。以電腦視覺的應用而言，許多預先訓練好的 model(通常是使用 Image-Net 資料集進行訓練)都是可公開下載的，以這些預先訓練好的 model 為基礎，再加以少量資料的訓練，就能產出更強大的 model。<br />
</p>
</div>
</div>

<div id="outline-container-orgf8208a5" class="outline-3">
<h3 id="orgf8208a5"><span class="section-number-3">11.2</span> 實作</h3>
<div class="outline-text-3" id="text-11-2">
</div>
<ol class="org-ol">
<li><a id="org58adc8d"></a>下載資料<br />
<div class="outline-text-4" id="text-11-2-1">
<p>
2013 年的 Kaggle 貓狗辨識大賽，最佳 model 即是使用 CNN，當時準確率達 95%，2013 年後的準確率已提高至 98%。本案例之資料來源：<a href="https://www.kaggle.com/c/dogs-vs-cats/data">https://www.kaggle.com/c/dogs-vs-cats/data</a>，由於原始圖片尺寸未做修改，大小各異，故需先額外處理，複製圖片到訓練、驗證和測試目錄的程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> os, shutil
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35299;&#22739;&#32302;&#36039;&#26009;&#22846;&#25152;&#22312;&#30340;&#30446;&#37636;&#36335;&#24465;</span>
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">original_dataset_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/train/train'</span>
<span class="linenr"> 5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#20786;&#23384;&#23569;&#37327;&#36039;&#26009;&#38598;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(base_dir): os.mkdir(base_dir)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22914;&#26524;&#30446;&#37636;&#19981;&#23384;&#22312;, &#25165;&#24314;&#31435;&#30446;&#37636;</span>
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20998;&#25286;&#25104;&#35347;&#32244;&#12289;&#39511;&#35657;&#33287;&#28204;&#35430;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">train_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr">11: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(train_dir): os.mkdir(train_dir)
<span class="linenr">12: </span>
<span class="linenr">13: </span>  <span style="color: #dcaeea;">validation_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)  
<span class="linenr">14: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(validation_dir): os.mkdir(validation_dir)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #dcaeea;">test_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'test'</span>)
<span class="linenr">17: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(test_dir): os.mkdir(test_dir)
<span class="linenr">18: </span>
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #dcaeea;">train_cats_dir</span> = os.path.join(train_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr">21: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(train_cats_dir): 
<span class="linenr">22: </span>      os.mkdir(train_cats_dir) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#35347;&#32244;&#35987;&#22294;&#29255;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #dcaeea;">train_dogs_dir</span> = os.path.join(train_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr">25: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(train_dogs_dir): 
<span class="linenr">26: </span>      os.mkdir(train_dogs_dir) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#35347;&#32244;&#29399;&#22294;&#29255;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">27: </span>
<span class="linenr">28: </span>  <span style="color: #dcaeea;">validation_cats_dir</span> = os.path.join(validation_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr">29: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(validation_cats_dir): 
<span class="linenr">30: </span>      os.mkdir(validation_cats_dir) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#39511;&#35657;&#35987;&#22294;&#29255;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">31: </span>
<span class="linenr">32: </span>  <span style="color: #dcaeea;">validation_dogs_dir</span> = os.path.join(validation_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr">33: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(validation_dogs_dir): 
<span class="linenr">34: </span>      os.mkdir(validation_dogs_dir) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#39511;&#35657;&#29399;&#22294;&#29255;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">35: </span>
<span class="linenr">36: </span>  <span style="color: #dcaeea;">test_cats_dir</span> = os.path.join(test_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr">37: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(test_cats_dir): 
<span class="linenr">38: </span>      os.mkdir(test_cats_dir) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#28204;&#35430;&#35987;&#22294;&#29255;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">39: </span>
<span class="linenr">40: </span>  <span style="color: #dcaeea;">test_dogs_dir</span> = os.path.join(test_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr">41: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(test_dogs_dir): 
<span class="linenr">42: </span>      os.mkdir(test_dogs_dir) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#28204;&#35430;&#29399;&#22294;&#29255;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">43: </span>
<span class="linenr">44: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35079;&#35069;&#21069;&#38754; 1000 &#24373;&#35987;&#22294;&#29255;&#21040; train_cats_dir &#35347;&#32244;&#30446;&#37636;</span>
<span class="linenr">45: </span>  <span style="color: #dcaeea;">fnames</span> = [<span style="color: #98be65;">'cat.{}.jpg'</span>.<span style="color: #c678dd;">format</span>(i) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1000</span>)]
<span class="linenr">46: </span>  <span style="color: #51afef;">for</span> fname <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">fnames</span>:
<span class="linenr">47: </span>      src = os.path.join(original_dataset_dir, fname)
<span class="linenr">48: </span>      <span style="color: #dcaeea;">dst</span> = os.path.join(train_cats_dir, fname)
<span class="linenr">49: </span>      shutil.copyfile(src, dst)
<span class="linenr">50: </span>
<span class="linenr">51: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35079;&#35069;&#19979; 500 &#24373;&#35987;&#22294;&#29255;&#21040; validation_cats_dir &#39511;&#35657;&#30446;&#37636;</span>
<span class="linenr">52: </span>  <span style="color: #dcaeea;">fnames</span> = [<span style="color: #98be65;">'cat.{}.jpg'</span>.<span style="color: #c678dd;">format</span>(i) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1000</span>, <span style="color: #da8548; font-weight: bold;">1500</span>)]
<span class="linenr">53: </span>  <span style="color: #51afef;">for</span> fname <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">fnames</span>:
<span class="linenr">54: </span>      src = os.path.join(original_dataset_dir, fname)
<span class="linenr">55: </span>      <span style="color: #dcaeea;">dst</span> = os.path.join(validation_cats_dir, fname)
<span class="linenr">56: </span>      shutil.copyfile(src, dst)
<span class="linenr">57: </span>
<span class="linenr">58: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35079;&#35069;&#19979; 500 &#24373;&#35987;&#22294;&#29255;&#21040; test_cats_dir &#28204;&#35430;&#30446;&#37636;</span>
<span class="linenr">59: </span>  <span style="color: #dcaeea;">fnames</span> = [<span style="color: #98be65;">'cat.{}.jpg'</span>.<span style="color: #c678dd;">format</span>(i) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1500</span>, <span style="color: #da8548; font-weight: bold;">2000</span>)]
<span class="linenr">60: </span>  <span style="color: #51afef;">for</span> fname <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">fnames</span>:
<span class="linenr">61: </span>      src = os.path.join(original_dataset_dir, fname)
<span class="linenr">62: </span>      <span style="color: #dcaeea;">dst</span> = os.path.join(test_cats_dir, fname)
<span class="linenr">63: </span>      shutil.copyfile(src, dst)
<span class="linenr">64: </span>
<span class="linenr">65: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35079;&#35069;&#21069;&#38754; 1000 &#24373;&#29399;&#22294;&#29255;&#21040; train_dogs_dir &#35347;&#32244;&#30446;&#37636;</span>
<span class="linenr">66: </span>  <span style="color: #dcaeea;">fnames</span> = [<span style="color: #98be65;">'dog.{}.jpg'</span>.<span style="color: #c678dd;">format</span>(i) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1000</span>)]
<span class="linenr">67: </span>  <span style="color: #51afef;">for</span> fname <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">fnames</span>:
<span class="linenr">68: </span>      src = os.path.join(original_dataset_dir, fname)
<span class="linenr">69: </span>      <span style="color: #dcaeea;">dst</span> = os.path.join(train_dogs_dir, fname)
<span class="linenr">70: </span>      shutil.copyfile(src, dst)
<span class="linenr">71: </span>
<span class="linenr">72: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35079;&#35069;&#19979; 500 &#24373;&#29399;&#22294;&#29255;&#21040; validation_dogs_dir &#39511;&#35657;&#30446;&#37636;</span>
<span class="linenr">73: </span>  <span style="color: #dcaeea;">fnames</span> = [<span style="color: #98be65;">'dog.{}.jpg'</span>.<span style="color: #c678dd;">format</span>(i) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1000</span>, <span style="color: #da8548; font-weight: bold;">1500</span>)]
<span class="linenr">74: </span>  <span style="color: #51afef;">for</span> fname <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">fnames</span>:
<span class="linenr">75: </span>      src = os.path.join(original_dataset_dir, fname)
<span class="linenr">76: </span>      <span style="color: #dcaeea;">dst</span> = os.path.join(validation_dogs_dir, fname)
<span class="linenr">77: </span>      shutil.copyfile(src, dst)
<span class="linenr">78: </span>
<span class="linenr">79: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35079;&#35069;&#19979; 500 &#24373;&#29399;&#22294;&#29255;&#21040; test_dogs_dir &#28204;&#35430;&#30446;&#37636;</span>
<span class="linenr">80: </span>  <span style="color: #dcaeea;">fnames</span> = [<span style="color: #98be65;">'dog.{}.jpg'</span>.<span style="color: #c678dd;">format</span>(i) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1500</span>, <span style="color: #da8548; font-weight: bold;">2000</span>)]
<span class="linenr">81: </span>  <span style="color: #51afef;">for</span> fname <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">fnames</span>:
<span class="linenr">82: </span>      src = os.path.join(original_dataset_dir, fname)
<span class="linenr">83: </span>      <span style="color: #dcaeea;">dst</span> = os.path.join(test_dogs_dir, fname)
<span class="linenr">84: </span>      shutil.copyfile(src, dst)
<span class="linenr">85: </span>
<span class="linenr">86: </span>      <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#35079;&#35069;&#23436;&#25104;'</span>)
</pre>
</div>

<pre class="example">
複製完成
</pre>


<p>
上述程式會產生三組資料集：訓練集狗貓各 1000、驗證集各 500、測試集各 500，可再以下列程式驗證：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> os, shutil
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35299;&#22739;&#32302;&#36039;&#26009;&#22846;&#25152;&#22312;&#30340;&#30446;&#37636;&#36335;&#24465;</span>
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">original_dataset_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/train/train'</span>
<span class="linenr"> 5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#20786;&#23384;&#23569;&#37327;&#36039;&#26009;&#38598;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20998;&#25286;&#25104;&#35347;&#32244;&#12289;&#39511;&#35657;&#33287;&#28204;&#35430;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">train_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr">10: </span>  <span style="color: #dcaeea;">validation_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)  
<span class="linenr">11: </span>  <span style="color: #dcaeea;">test_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'test'</span>)
<span class="linenr">12: </span>  <span style="color: #dcaeea;">train_cats_dir</span> = os.path.join(train_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr">13: </span>  <span style="color: #dcaeea;">train_dogs_dir</span> = os.path.join(train_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr">14: </span>  <span style="color: #dcaeea;">validation_cats_dir</span> = os.path.join(validation_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr">15: </span>  <span style="color: #dcaeea;">validation_dogs_dir</span> = os.path.join(validation_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr">16: </span>  <span style="color: #dcaeea;">test_cats_dir</span> = os.path.join(test_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr">17: </span>  <span style="color: #dcaeea;">test_dogs_dir</span> = os.path.join(test_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr">18: </span>
<span class="linenr">19: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#35347;&#32244;&#29992;&#30340;&#35987;&#29031;&#29255;&#24373;&#25976;:'</span>, <span style="color: #c678dd;">len</span>(os.listdir(train_cats_dir)))
<span class="linenr">20: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#35347;&#32244;&#29992;&#30340;&#29399;&#29031;&#29255;&#24373;&#25976;:'</span>, <span style="color: #c678dd;">len</span>(os.listdir(train_dogs_dir)))
<span class="linenr">21: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#39511;&#35657;&#29992;&#30340;&#35987;&#29031;&#29255;&#24373;&#25976;:'</span>, <span style="color: #c678dd;">len</span>(os.listdir(validation_cats_dir)))
<span class="linenr">22: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#39511;&#35657;&#29992;&#30340;&#29399;&#29031;&#29255;&#24373;&#25976;:'</span>, <span style="color: #c678dd;">len</span>(os.listdir(validation_dogs_dir)))
<span class="linenr">23: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#28204;&#35430;&#29992;&#30340;&#35987;&#29031;&#29255;&#24373;&#25976;:'</span>, <span style="color: #c678dd;">len</span>(os.listdir(test_cats_dir)))
<span class="linenr">24: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#28204;&#35430;&#29992;&#30340;&#29399;&#29031;&#29255;&#24373;&#25976;:'</span>, <span style="color: #c678dd;">len</span>(os.listdir(test_dogs_dir)))
</pre>
</div>

<pre class="example">
訓練用的貓照片張數: 1000
訓練用的狗照片張數: 1000
驗證用的貓照片張數: 500
驗證用的狗照片張數: 500
測試用的貓照片張數: 500
測試用的狗照片張數: 500 
</pre>
</div>
</li>

<li><a id="org14e2c36"></a>建立神經網路<br />
<div class="outline-text-4" id="text-11-2-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr"> 5: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">32</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>,
<span class="linenr"> 6: </span>                          input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>)))
<span class="linenr"> 7: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 8: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">64</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 9: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr">10: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">128</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">11: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr">12: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">128</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">13: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr">14: </span>  model.add(layers.Flatten())
<span class="linenr">15: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">512</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">16: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
<span class="linenr">17: </span>  model.summary()  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26597;&#30475;&#27169;&#22411;&#25688;&#35201;</span>
<span class="linenr">18: </span>
</pre>
</div>

<pre class="example">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 6272)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               3211776   
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513       
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
</pre>

<p>
在編譯時，以 RMSProp 優化器，由於使用 sigmoid 單元結束神經網路，所以配合使用 binary_crossentropy 二元交叉熵作為損失基準。<br />
</p>

<div class="org-src-container">
<pre class="src src-python" id="org37e7882"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr">2: </span>
<span class="linenr">3: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">4: </span>                optimizer=optimizers.RMSprop(lr=1e-<span style="color: #da8548; font-weight: bold;">4</span>),
<span class="linenr">5: </span>                metrics=[<span style="color: #98be65;">'acc'</span>])
</pre>
</div>
</div>
</li>

<li><a id="org910c895"></a>資料預處理<br />
<div class="outline-text-4" id="text-11-2-3">
<p>
資料在送入神經網路前應先將 JPEG 檔案格式化成適當的浮點數張量，其步驟如下：<br />
</p>
<ol class="org-ol">
<li>讀取影像檔<br /></li>
<li>將 JPEG 內容解碼為 RGB 的像素<br /></li>
<li>將 RGB 像素轉為浮點數張量<br /></li>
<li>將像素值(0~255)壓縮到[0,1]區間<br /></li>
</ol>

<p>
上述過程可以用 Keras 的 keras.preprocessing.image 模組來處理，它包含 ImageDataGenerator 類別，過程如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python" id="org029a44e"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> os, shutil
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35299;&#22739;&#32302;&#36039;&#26009;&#22846;&#25152;&#22312;&#30340;&#30446;&#37636;&#36335;&#24465;</span>
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">original_dataset_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/train/train'</span>
<span class="linenr"> 5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#20786;&#23384;&#23569;&#37327;&#36039;&#26009;&#38598;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20998;&#25286;&#25104;&#35347;&#32244;&#12289;&#39511;&#35657;&#33287;&#28204;&#35430;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">train_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr">10: </span>  <span style="color: #dcaeea;">validation_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)  
<span class="linenr">11: </span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr">13: </span>
<span id="coderef-ImageDataGenerator" class="coderef-off"><span class="linenr">14: </span>  <span style="color: #dcaeea;">train_datagen</span> = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35373;&#23450;&#35347;&#32244;&#12289;&#28204;&#35430;&#36039;&#26009;&#30340; Python &#29986;&#29983;&#22120;&#65292;&#20006;&#23559;&#22294;&#29255;&#20687;&#32032;&#20540;&#20381; 1/255 &#27604;&#20363;&#37325;&#26032;&#22739;&#32302;&#21040; [0, 1]</span></span>
<span class="linenr">15: </span>  <span style="color: #dcaeea;">test_datagen</span> = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>)
<span class="linenr">16: </span>
<span class="linenr">17: </span>  <span style="color: #dcaeea;">train_generator</span> = train_datagen.flow_from_directory(
<span class="linenr">18: </span>      train_dir,              <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30446;&#27161;&#30446;&#37636;</span>
<span class="linenr">19: </span>      target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35519;&#25972;&#25152;&#26377;&#24433;&#20687;&#22823;&#23567;&#25104; 150x150</span>
<span class="linenr">20: </span>      batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">21: </span>      class_mode=<span style="color: #98be65;">'binary'</span>)    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22240;&#28858;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109; binary_crossentropy &#20316;&#28858;&#25613;&#22833;&#20540;&#65292;&#25152;&#20197;&#38656;&#35201;&#20108;&#20301;&#20803;&#27161;&#31844;</span>
<span class="linenr">22: </span>
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #dcaeea;">validation_generator</span> = test_datagen.flow_from_directory(
<span class="linenr">25: </span>      validation_dir,
<span class="linenr">26: </span>      target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),
<span class="linenr">27: </span>      batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">28: </span>      class_mode=<span style="color: #98be65;">'binary'</span>)
<span class="linenr">29: </span>
<span class="linenr">30: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35264;&#23519;&#29986;&#29983;&#22120;&#30340;&#32080;&#26524;</span>
<span class="linenr">31: </span>  <span style="color: #51afef;">for</span> data_batch, labels_batch <span style="color: #51afef;">in</span> train_generator:
<span class="linenr">32: </span>      <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'data batch shape:'</span>, data_batch.shape)
<span class="linenr">33: </span>      <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'labels batch shape:'</span>, labels_batch.shape)
<span id="coderef-DataGeneratorBreak" class="coderef-off"><span class="linenr">34: </span>      <span style="color: #51afef;">break</span></span>
<span class="linenr">35: </span>
</pre>
</div>

<pre class="example">
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
data batch shape: (20, 150, 150, 3)
labels batch shape: (20,)
</pre>


<p>
結果顯示每批次產生出的資料為 20 張 150&times;150 的 RGB 影像以及 20 個 label(即答案)，需留意的是此處的 generator 會無 止盡的生成批次量樣本，也就會不停的持續循環產生影像到目標目錄中，所以要放 break。而上述程式中的 ImageDataGenerator(第<a href="#coderef-ImageDataGenerator" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-ImageDataGenerator');" onmouseout="CodeHighlightOff(this, 'coderef-ImageDataGenerator');">14</a>行)是一種產生器(Generator)，在 Python 中是一個持續迭代運作的物件，是一個可以與 for&#x2026;in 一起使用的物件，產生器是使用 yield 建構的。典型的產生器範例如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python" id="orgf5cf0af"><span class="linenr"> 1: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">generator</span>():
<span class="linenr"> 2: </span>      <span style="color: #dcaeea;">i</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr"> 3: </span>      <span style="color: #51afef;">while</span> <span style="color: #a9a1e1;">True</span>:
<span class="linenr"> 4: </span>          i += <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 5: </span>          <span style="color: #51afef;">yield</span> i
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">for</span> item <span style="color: #51afef;">in</span> generator():
<span class="linenr"> 8: </span>      <span style="color: #51afef;">print</span>(item)
<span class="linenr"> 9: </span>      <span style="color: #51afef;">if</span> item &gt; <span style="color: #da8548; font-weight: bold;">3</span>:
<span class="linenr">10: </span>          <span style="color: #51afef;">break</span>
</pre>
</div>

<pre class="example">
1
2
3
4
</pre>


<p>
建構好 model、整理完資料，接下來就可以調整 model 來搭配產生器所產生的資料，我們可以應用 model 的 fit_generator 方法，這個方法的第 1 個參數即是一個 Python 的產生器，然而由於資料是無止盡地產生，所以在宣告訓練時期之前，Keras model 需要知道從產生器抽取多少樣本，這就是 steps_per_epoch 參數的功能，它指定了從產生器取得的批次量，也就是說，model 在運行了 steps_per_epoch 次的梯度下降步驟後，訓練過程將進入下一個訓練週期(epochs)。在以下的例子中，每個批次量包含 20 個樣本，而目標樣本有 2000 個，所以就需要有 100 個批次量。<br />
</p>

<div class="org-src-container">
<pre class="src src-python" id="orgb69b075"><span class="linenr">1: </span>  <span style="color: #dcaeea;">history</span> = model.fit_generator(
<span class="linenr">2: </span>      train_generator,   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35373;&#23450;&#29986;&#29983;&#22120;</span>
<span class="linenr">3: </span>      steps_per_epoch=<span style="color: #da8548; font-weight: bold;">100</span>,   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35373;&#23450;&#24478;&#29986;&#29983;&#22120;&#25277;&#21462;100&#20491;&#25209;&#27425;&#37327;</span>
<span class="linenr">4: </span>      epochs=<span style="color: #da8548; font-weight: bold;">30</span>,
<span class="linenr">5: </span>      validation_data=validation_generator,
<span class="linenr">6: </span>      validation_steps=<span style="color: #da8548; font-weight: bold;">50</span>)
<span class="linenr">7: </span>
<span class="linenr">8: </span>  model.save(<span style="color: #98be65;">'cats_and_dogs_small_i.h5'</span>)
</pre>
</div>

<p>
使用上述 fit_generator 時，還可以傳遞 validation_data 參數，此參數可以接收一個資料產生器，也可以接收 Numpy 陣列，如果接收的資料來自產生器，則還要指定 validation_steps 參數，告訴程式要從產生器中抽取多少次批量進行評估。在完成訓練後把 model 存起來，並繪製訓練週期與驗證週期的 model 損失值與準確度。<br />
</p>
</div>
</li>

<li><a id="orgc7e5866"></a>完整程式<br />
<div class="outline-text-4" id="text-11-2-4">
<div class="org-src-container">
<pre class="src src-python" id="org2871041"><span class="linenr">  1: </span>  <span style="color: #51afef;">import</span> os, shutil
<span class="linenr">  2: </span>
<span class="linenr">  3: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35299;&#22739;&#32302;&#36039;&#26009;&#22846;&#25152;&#22312;&#30340;&#30446;&#37636;&#36335;&#24465;</span>
<span class="linenr">  4: </span>  <span style="color: #dcaeea;">original_dataset_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/train/train'</span>
<span class="linenr">  5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#20786;&#23384;&#23569;&#37327;&#36039;&#26009;&#38598;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">  6: </span>  <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr">  7: </span>
<span class="linenr">  8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20998;&#25286;&#25104;&#35347;&#32244;&#12289;&#39511;&#35657;&#33287;&#28204;&#35430;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">  9: </span>  <span style="color: #dcaeea;">train_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr"> 10: </span>  <span style="color: #dcaeea;">validation_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)
<span class="linenr"> 11: </span>  <span style="color: #dcaeea;">test_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'test'</span>)
<span class="linenr"> 12: </span>  <span style="color: #dcaeea;">train_cats_dir</span> = os.path.join(train_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr"> 13: </span>  <span style="color: #dcaeea;">train_dogs_dir</span> = os.path.join(train_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr"> 14: </span>  <span style="color: #dcaeea;">validation_cats_dir</span> = os.path.join(validation_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr"> 15: </span>  <span style="color: #dcaeea;">validation_dogs_dir</span> = os.path.join(validation_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr"> 16: </span>  <span style="color: #dcaeea;">test_cats_dir</span> = os.path.join(test_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr"> 17: </span>  <span style="color: #dcaeea;">test_dogs_dir</span> = os.path.join(test_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr"> 18: </span>
<span class="linenr"> 19: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#31435;&#27169;&#32068;</span>
<span class="linenr"> 20: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 21: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 22: </span>
<span class="linenr"> 23: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr"> 24: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">32</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>,
<span class="linenr"> 25: </span>                          input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>)))
<span class="linenr"> 26: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 27: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">64</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 28: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 29: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">128</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 30: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 31: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">128</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 32: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 33: </span>  model.add(layers.Flatten())
<span class="linenr"> 34: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">512</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 35: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
<span class="linenr"> 36: </span>  model.summary()  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26597;&#30475;&#27169;&#22411;&#25688;&#35201;</span>
<span class="linenr"> 37: </span>
<span class="linenr"> 38: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#37197;&#32622; model &#20197;&#36914;&#34892;&#35347;&#32244;</span>
<span class="linenr"> 39: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr"> 40: </span>
<span class="linenr"> 41: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr"> 42: </span>                optimizer=optimizers.RMSprop(lr=1e-<span style="color: #da8548; font-weight: bold;">4</span>),
<span class="linenr"> 43: </span>                metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr"> 44: </span>
<span class="linenr"> 45: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992; ImageDataGenerator &#29986;&#29983;&#22120;&#24478;&#30446;&#37636;&#20013;&#35712;&#21462;&#24433;&#20687;</span>
<span class="linenr"> 46: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr"> 47: </span>
<span id="coderef-ImageDataGenerator" class="coderef-off"><span class="linenr"> 48: </span>  <span style="color: #dcaeea;">train_datagen</span> = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35373;&#23450;&#35347;&#32244;&#12289;&#28204;&#35430;&#36039;&#26009;&#30340; Python &#29986;&#29983;&#22120;&#65292;&#20006;&#23559;&#22294;&#29255;&#20687;&#32032;&#20540;&#20381; 1/255 &#27604;&#20363;&#37325;&#26032;&#22739;&#32302;&#21040; [0, 1]</span></span>
<span class="linenr"> 49: </span>  <span style="color: #dcaeea;">test_datagen</span> = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>)
<span class="linenr"> 50: </span>
<span class="linenr"> 51: </span>  <span style="color: #dcaeea;">train_generator</span> = train_datagen.flow_from_directory(
<span class="linenr"> 52: </span>      train_dir,              <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30446;&#27161;&#30446;&#37636;</span>
<span class="linenr"> 53: </span>      target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35519;&#25972;&#25152;&#26377;&#24433;&#20687;&#22823;&#23567;&#25104; 150x150</span>
<span class="linenr"> 54: </span>      batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr"> 55: </span>      class_mode=<span style="color: #98be65;">'binary'</span>)    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22240;&#28858;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109; binary_crossentropy &#20316;&#28858;&#25613;&#22833;&#20540;&#65292;&#25152;&#20197;&#38656;&#35201;&#20108;&#20301;&#20803;&#27161;&#31844;</span>
<span class="linenr"> 56: </span>
<span class="linenr"> 57: </span>
<span class="linenr"> 58: </span>  <span style="color: #dcaeea;">validation_generator</span> = test_datagen.flow_from_directory(
<span class="linenr"> 59: </span>      validation_dir,
<span class="linenr"> 60: </span>      target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),
<span class="linenr"> 61: </span>      batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr"> 62: </span>      class_mode=<span style="color: #98be65;">'binary'</span>)
<span class="linenr"> 63: </span>
<span class="linenr"> 64: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;model</span>
<span class="linenr"> 65: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 66: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 67: </span>
<span class="linenr"> 68: </span>  <span style="color: #dcaeea;">history</span> = model.fit_generator(
<span class="linenr"> 69: </span>      train_generator,   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35373;&#23450;&#29986;&#29983;&#22120;</span>
<span class="linenr"> 70: </span>      steps_per_epoch=<span style="color: #da8548; font-weight: bold;">100</span>,   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35373;&#23450;&#24478;&#29986;&#29983;&#22120;&#25277;&#21462;100&#20491;&#25209;&#27425;&#37327;</span>
<span class="linenr"> 71: </span>      epochs=<span style="color: #da8548; font-weight: bold;">30</span>, verbose=<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">verbose=1, &#19981;&#39023;&#31034;&#35347;&#32244;&#36942;&#31243;</span>
<span class="linenr"> 72: </span>      validation_data=validation_generator,
<span class="linenr"> 73: </span>      validation_steps=<span style="color: #da8548; font-weight: bold;">50</span>)
<span class="linenr"> 74: </span>
<span class="linenr"> 75: </span>  model.save(<span style="color: #98be65;">'cats_and_dogs_small_i.h5'</span>)
<span class="linenr"> 76: </span>
<span class="linenr"> 77: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32362;&#35069;model&#30340;&#25613;&#22833;&#29575;&#33287;&#31934;&#30906;&#29575;</span>
<span class="linenr"> 78: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 79: </span>
<span class="linenr"> 80: </span>  <span style="color: #dcaeea;">acc</span> = history.history[<span style="color: #98be65;">'acc'</span>]
<span class="linenr"> 81: </span>  <span style="color: #dcaeea;">val_acc</span> = history.history[<span style="color: #98be65;">'val_acc'</span>]
<span class="linenr"> 82: </span>  <span style="color: #dcaeea;">loss</span> = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr"> 83: </span>  <span style="color: #dcaeea;">val_loss</span> = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr"> 84: </span>
<span class="linenr"> 85: </span>  <span style="color: #dcaeea;">epochs</span> = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(acc) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 86: </span>
<span class="linenr"> 87: </span>  plt.clf()
<span class="linenr"> 88: </span>  plt.plot(epochs, acc, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr"> 89: </span>  plt.plot(epochs, val_acc, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr"> 90: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr"> 91: </span>  plt.legend()
<span class="linenr"> 92: </span>  plt.plot()
<span class="linenr"> 93: </span>  plt.savefig(<span style="color: #98be65;">"cats-and-dogs-accuracy-v1.png"</span>)
<span class="linenr"> 94: </span>  plt.figure()
<span class="linenr"> 95: </span>
<span class="linenr"> 96: </span>  plt.clf()
<span class="linenr"> 97: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr"> 98: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr"> 99: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">100: </span>  plt.legend()
<span class="linenr">101: </span>  plt.plot()
<span class="linenr">102: </span>  plt.savefig(<span style="color: #98be65;">"cats-and-dogs-loss-v1.png"</span>)
<span class="linenr">103: </span>
</pre>
</div>

<pre class="example">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 6272)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               3211776   
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513       
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
</pre>


<div id="org7ee27d0" class="figure">
<p><img src="images/cats-and-dogs-accuracy-v1.png" alt="cats-and-dogs-accuracy-v1.png" /><br />
</p>
<p><span class="figure-number">Figure 68: </span>Cats and Dogs Accuracy V1</p>
</div>


<div id="orgd9e5836" class="figure">
<p><img src="images/cats-and-dogs-loss-v1.png" alt="cats-and-dogs-loss-v1.png" /><br />
</p>
<p><span class="figure-number">Figure 69: </span>Cats and Dogs Loss V1</p>
</div>

<p>
由圖<a href="#org7ee27d0">68</a>看出訓練準確度成線性成長直到逼近 100%，但驗證準確度則在第三個訓練週期後就停留在 70%；訓練損失分數也呈線性下降，但驗證損失分數則約在第 12 週期後達到最低點。這些都是明顯的 overfitting 訊號。<br />
</p>

<p>
由於訓練樣本數(2000)相對較少，overfitting 將成為訓練 model 的首要顧慮因素，幾種緩解 overfitting 的技術有：<br />
</p>
<ul class="org-ul">
<li>dropout<br /></li>
<li>權重調整(L2 regularization)<br /></li>
<li>資料擴增法(data augmentation)<br /></li>
</ul>
</div>
</li>
</ol>
</div>

<div id="outline-container-org5bf6f16" class="outline-3">
<h3 id="org5bf6f16"><span class="section-number-3">11.3</span> 改善#1: 使用資料擴增法(data augmentation)</h3>
<div class="outline-text-3" id="text-11-3">
<p>
Overfitting 的部份成因是由於樣本太少導致無法訓練出具備普適性、可套用到新資料的 model，想像一下如果有無限量的資料，則 model 將會因應用手邊資料的各種可能面向，也就不致於 overfitting。資料擴增就是由現有訓練樣本生成更多訓練資料的方法，主要是透過隨機變換原始資料，以產生相似的影像，進而增加訓練樣本數。最終目標是在訓練時，model 不會看到兩次完全相同的影像。<br />
</p>

<p>
在 Keras 中，我們可以藉由設定 ImageDataGenerator，在讀取影像時執行隨機變換(random transformation)來達到資料擴增，至於變換的方向則可以在 ImageDataGenerator 的參數中進一步指定。以下例來看：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">datagen</span> = ImageDataGenerator(
<span class="linenr">2: </span>      rotation_range=<span style="color: #da8548; font-weight: bold;">40</span>,       <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#26059;&#36681;&#35282;&#24230;&#20540;(0~180)</span>
<span class="linenr">3: </span>      width_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#27700;&#24179;&#38568;&#27231;&#24179;&#31227;(&#22294;&#29255;&#23532;&#24230;&#20043;&#30334;&#20998;&#27604;)</span>
<span class="linenr">4: </span>      height_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#22402;&#30452;&#38568;&#27231;&#24179;&#31227;(&#22294;&#29255;&#39640;&#24230;&#20043;&#30334;&#20998;&#27604;)</span>
<span class="linenr">5: </span>      shear_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,         <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38568;&#27231;&#20670;&#26012;(&#38918;&#26178;&#37912;&#20670;&#26012;&#35282;&#24230;)</span>
<span class="linenr">6: </span>      zoom_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,          <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38568;&#27231;&#32302;&#25918;(&#32302;&#25918;&#30334;&#20998;&#27604;)</span>
<span class="linenr">7: </span>      horizontal_flip=<span style="color: #a9a1e1;">True</span>,    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38568;&#27231;&#27700;&#24179;&#32763;&#36681;(&#24433;&#20687;&#38750;&#24038;&#21491;&#23565;&#31281;&#25165;&#26377;&#25928;)</span>
<span class="linenr">8: </span>      fill_mode=<span style="color: #98be65;">'nearest'</span>)     <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#26032;&#24314;&#24433;&#20687;&#22635;&#35036;&#20687;&#32032;&#26041;&#27861;</span>
</pre>
</div>

<p>
上述程式之 fill__{}mode 共提供四種像素填補方法：<br />
</p>
<ul class="org-ul">
<li>constant: 依照輸入的 cval(浮點數或整數)將影像邊界之外都以該值填補，例如 cval=k，則影像填補為 kkkkkkkk|abcd|kkkkkkkk<br /></li>
<li>nearest: 以最接近的像素值填補，如：aaaaaaaa|abcd|dddddddd<br /></li>
<li>reflect: 以影像重複填補(影像以一正一反方向)，如 abcddcba|abcd|dcbaabcd<br /></li>
<li>wrap: 以影像重複填補，如：abcdabcd|abcd|abcdabcd<br /></li>
</ul>

<p>
以下為實際運作的示範：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> matplotlib
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> platform
<span class="linenr"> 3: </span>  <span style="color: #51afef;">if</span> platform.system() == <span style="color: #98be65;">'Darwin'</span>:
<span class="linenr"> 4: </span>      matplotlib.use(<span style="color: #98be65;">'MacOSX'</span>)
<span class="linenr"> 5: </span>  <span style="color: #51afef;">else</span>:
<span class="linenr"> 6: </span>      matplotlib.use(<span style="color: #98be65;">'TkAgg'</span>)
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  datagen = ImageDataGenerator(
<span class="linenr">11: </span>      rotation_range=<span style="color: #da8548; font-weight: bold;">40</span>,       <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#26059;&#36681;&#35282;&#24230;&#20540;(0~180)</span>
<span class="linenr">12: </span>      width_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#27700;&#24179;&#38568;&#27231;&#24179;&#31227;(&#22294;&#29255;&#23532;&#24230;&#20043;&#30334;&#20998;&#27604;)</span>
<span class="linenr">13: </span>      height_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#22402;&#30452;&#38568;&#27231;&#24179;&#31227;(&#22294;&#29255;&#39640;&#24230;&#20043;&#30334;&#20998;&#27604;)</span>
<span class="linenr">14: </span>      shear_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,         <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38568;&#27231;&#20670;&#26012;(&#38918;&#26178;&#37912;&#20670;&#26012;&#35282;&#24230;)</span>
<span class="linenr">15: </span>      zoom_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,          <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38568;&#27231;&#32302;&#25918;(&#32302;&#25918;&#30334;&#20998;&#27604;)</span>
<span class="linenr">16: </span>      horizontal_flip=<span style="color: #a9a1e1;">True</span>,    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38568;&#27231;&#27700;&#24179;&#32763;&#36681;(&#24433;&#20687;&#38750;&#24038;&#21491;&#23565;&#31281;&#25165;&#26377;&#25928;)</span>
<span class="linenr">17: </span>      fill_mode=<span style="color: #98be65;">'nearest'</span>)     <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#26032;&#24314;&#24433;&#20687;&#22635;&#35036;&#20687;&#32032;&#26041;&#27861;</span>
<span class="linenr">18: </span>
<span class="linenr">19: </span>  <span style="color: #51afef;">import</span> os, shutil
<span class="linenr">20: </span>
<span class="linenr">21: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35299;&#22739;&#32302;&#36039;&#26009;&#22846;&#25152;&#22312;&#30340;&#30446;&#37636;&#36335;&#24465;</span>
<span class="linenr">22: </span>  <span style="color: #dcaeea;">original_dataset_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/train/train'</span>
<span class="linenr">23: </span>  <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr">24: </span>
<span class="linenr">25: </span>  <span style="color: #dcaeea;">train_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr">26: </span>  <span style="color: #dcaeea;">train_cats_dir</span> = os.path.join(train_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr">27: </span>
<span class="linenr">28: </span>  <span style="color: #51afef;">from</span> keras.preprocessing <span style="color: #51afef;">import</span> image
<span class="linenr">29: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">30: </span>
<span class="linenr">31: </span>  <span style="color: #dcaeea;">fnames</span> = [os.path.join(train_cats_dir, fname) <span style="color: #51afef;">for</span>
<span class="linenr">32: </span>      fname <span style="color: #51afef;">in</span> os.listdir(train_cats_dir)]
<span class="linenr">33: </span>
<span class="linenr">34: </span>  <span style="color: #dcaeea;">img_path</span> = fnames[<span style="color: #da8548; font-weight: bold;">3</span>] <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#36984;&#19968;&#24373;&#24433;&#20687;&#20358;&#25844;&#20805;</span>
<span class="linenr">35: </span>  <span style="color: #51afef;">print</span>(img_path)
<span class="linenr">36: </span>
<span class="linenr">37: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35712;&#21462;&#24433;&#20687;&#12289;&#35519;&#25972;&#22823;&#23567;</span>
<span class="linenr">38: </span>  <span style="color: #dcaeea;">img</span> = image.load_img(img_path, target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>)) 
<span class="linenr">39: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23559;&#20854;&#35519;&#25972;&#28858;shape=(150, 150, 3)</span>
<span class="linenr">40: </span>  <span style="color: #dcaeea;">x</span> = image.img_to_array(img)
<span class="linenr">41: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35519;&#25972;shape&#28858;(1, 150, 150, 3)</span>
<span class="linenr">42: </span>  <span style="color: #dcaeea;">x</span> = x.reshape((<span style="color: #da8548; font-weight: bold;">1</span>, ) + x.shape)
<span class="linenr">43: </span>  <span style="color: #51afef;">print</span>(x.shape)
<span class="linenr">44: </span>
<span class="linenr">45: </span>  <span style="color: #dcaeea;">i</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">46: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32362;&#35069;model&#30340;&#25613;&#22833;&#29575;&#33287;&#31934;&#30906;&#29575;</span>
<span class="linenr">47: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">48: </span>
<span class="linenr">49: </span>  <span style="color: #51afef;">for</span> batch <span style="color: #51afef;">in</span> datagen.flow(x, batch_size=<span style="color: #da8548; font-weight: bold;">1</span>):
<span class="linenr">50: </span>      plt.figure(i)
<span class="linenr">51: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">imgplot = plt.imshow(image.array_to_img(batch[0]))</span>
<span class="linenr">52: </span>      plt.imshow(image.array_to_img(batch[<span style="color: #da8548; font-weight: bold;">0</span>]))
<span class="linenr">53: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.clf()</span>
<span class="linenr">54: </span>      plt.plot()
<span class="linenr">55: </span>      plt.savefig(<span style="color: #98be65;">"CatsAugmentation"</span>+<span style="color: #c678dd;">str</span>(i)+<span style="color: #98be65;">".png"</span>)
<span class="linenr">56: </span>      <span style="color: #dcaeea;">i</span> += <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">57: </span>      <span style="color: #51afef;">if</span> i % <span style="color: #da8548; font-weight: bold;">4</span> == <span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr">58: </span>          <span style="color: #51afef;">break</span>
<span class="linenr">59: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">60: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.savefig("CatsAugmentation.png")</span>
</pre>
</div>

<pre class="example">
/Volumes/Vanessa/dogs-vs-cats/small/train/cats/cat.100.jpg
(1, 150, 150, 3)
</pre>


<p>
[[file:<img src="images/CatsAugmentation0.png" alt="CatsAugmentation0.png" />images/CatsAugmentation1.png]]<br />
[[file:<img src="images/CatsAugmentation2.png" alt="CatsAugmentation2.png" />images/CatsAugmentation3.png]]<br />
</p>

<p>
雖然資料擴增能擴充來自少量的原始圖片，但終究無法自行產生資訊，只能重新混合現有資訊，影像間仍是高度相關，仍不足以完全擺脫 overfitting 問題，所以進一步在密集連接的分類器前，在 model 中增加 Dropout 層(Fatten 層後)。<br />
</p>


<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #51afef;">import</span> matplotlib
<span class="linenr">  2: </span>  <span style="color: #51afef;">import</span> platform
<span class="linenr">  3: </span>  <span style="color: #51afef;">if</span> platform.system() == <span style="color: #98be65;">'Darwin'</span>:
<span class="linenr">  4: </span>      matplotlib.use(<span style="color: #98be65;">'MacOSX'</span>)
<span class="linenr">  5: </span>  <span style="color: #51afef;">else</span>:
<span class="linenr">  6: </span>      matplotlib.use(<span style="color: #98be65;">'TkAgg'</span>)
<span class="linenr">  7: </span>
<span class="linenr">  8: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr">  9: </span>
<span class="linenr"> 10: </span>  <span style="color: #51afef;">import</span> os, shutil
<span class="linenr"> 11: </span>
<span class="linenr"> 12: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35299;&#22739;&#32302;&#36039;&#26009;&#22846;&#25152;&#22312;&#30340;&#30446;&#37636;&#36335;&#24465;</span>
<span class="linenr"> 13: </span>  original_dataset_dir = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/train/train'</span>
<span class="linenr"> 14: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#20786;&#23384;&#23569;&#37327;&#36039;&#26009;&#38598;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr"> 15: </span>  <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr"> 16: </span>
<span class="linenr"> 17: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20998;&#25286;&#25104;&#35347;&#32244;&#12289;&#39511;&#35657;&#33287;&#28204;&#35430;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr"> 18: </span>  <span style="color: #dcaeea;">train_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr"> 19: </span>  <span style="color: #dcaeea;">validation_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)
<span class="linenr"> 20: </span>  <span style="color: #dcaeea;">test_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'test'</span>)
<span class="linenr"> 21: </span>  <span style="color: #dcaeea;">train_cats_dir</span> = os.path.join(train_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr"> 22: </span>  <span style="color: #dcaeea;">train_dogs_dir</span> = os.path.join(train_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr"> 23: </span>  <span style="color: #dcaeea;">validation_cats_dir</span> = os.path.join(validation_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr"> 24: </span>  <span style="color: #dcaeea;">validation_dogs_dir</span> = os.path.join(validation_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr"> 25: </span>  <span style="color: #dcaeea;">test_cats_dir</span> = os.path.join(test_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr"> 26: </span>  <span style="color: #dcaeea;">test_dogs_dir</span> = os.path.join(test_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr"> 27: </span>
<span class="linenr"> 28: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#31435;&#27169;&#32068;</span>
<span class="linenr"> 29: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 30: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 31: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> regularizers
<span class="linenr"> 32: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr"> 33: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">32</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>, 
<span class="linenr"> 34: </span>                          input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>)))
<span class="linenr"> 35: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 36: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">64</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 37: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 38: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">128</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 39: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 40: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">128</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 41: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 42: </span>  model.add(layers.Flatten())
<span class="linenr"> 43: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr"> 44: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">512</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 45: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
<span class="linenr"> 46: </span>  model.summary()  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26597;&#30475;&#27169;&#22411;&#25688;&#35201;</span>
<span class="linenr"> 47: </span>
<span class="linenr"> 48: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#37197;&#32622; model &#20197;&#36914;&#34892;&#35347;&#32244;</span>
<span class="linenr"> 49: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr"> 50: </span>
<span class="linenr"> 51: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr"> 52: </span>                optimizer=optimizers.RMSprop(lr=1e-<span style="color: #da8548; font-weight: bold;">4</span>),
<span class="linenr"> 53: </span>                metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr"> 54: </span>
<span class="linenr"> 55: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#36039;&#26009;&#25844;&#22686;</span>
<span class="linenr"> 56: </span>  <span style="color: #dcaeea;">train_datagen</span> = ImageDataGenerator(
<span class="linenr"> 57: </span>      rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>,
<span class="linenr"> 58: </span>      rotation_range=<span style="color: #da8548; font-weight: bold;">40</span>,
<span class="linenr"> 59: </span>      width_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 60: </span>      height_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 61: </span>      shear_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 62: </span>      zoom_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 63: </span>      horizontal_flip=<span style="color: #a9a1e1;">True</span>, )
<span class="linenr"> 64: </span>
<span class="linenr"> 65: </span>  <span style="color: #dcaeea;">test_datagen</span> = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35531;&#27880;&#24847;&#65281;&#39511;&#35657;&#36039;&#26009;&#19981;&#25033;&#35442;&#25844;&#20805;!!!</span>
<span class="linenr"> 66: </span>
<span class="linenr"> 67: </span>  <span style="color: #dcaeea;">train_generator</span> = train_datagen.flow_from_directory(
<span class="linenr"> 68: </span>    train_dir,    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30446;&#27161;&#30446;&#37636;</span>
<span class="linenr"> 69: </span>    target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>), <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25152;&#26377;&#22294;&#20687;&#22823;&#23567;&#35519;&#25972;&#25104; 150&#215;150 </span>
<span class="linenr"> 70: </span>    batch_size=<span style="color: #da8548; font-weight: bold;">32</span>,
<span class="linenr"> 71: </span>    class_mode=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22240;&#28858;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109; binary_crossentropy &#20316;&#28858;&#25613;&#22833;&#65292;&#25152;&#20197;&#38656;&#35201;&#20108;&#20803;&#27161;&#31844;</span>
<span class="linenr"> 72: </span>
<span class="linenr"> 73: </span>
<span class="linenr"> 74: </span>  <span style="color: #dcaeea;">validation_generator</span> = test_datagen.flow_from_directory(
<span class="linenr"> 75: </span>    validation_dir,
<span class="linenr"> 76: </span>    target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),
<span class="linenr"> 77: </span>    batch_size=<span style="color: #da8548; font-weight: bold;">32</span>,
<span class="linenr"> 78: </span>    class_mode=<span style="color: #98be65;">'binary'</span>)
<span class="linenr"> 79: </span>
<span class="linenr"> 80: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;</span>
<span class="linenr"> 81: </span>  <span style="color: #dcaeea;">history</span> = model.fit_generator(   
<span class="linenr"> 82: </span>    train_generator,
<span class="linenr"> 83: </span>    steps_per_epoch=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr"> 84: </span>    epochs=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr"> 85: </span>      verbose=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr"> 86: </span>    validation_data=validation_generator,
<span class="linenr"> 87: </span>    validation_steps=<span style="color: #da8548; font-weight: bold;">50</span>)
<span class="linenr"> 88: </span>
<span class="linenr"> 89: </span>  model.save(<span style="color: #98be65;">'cats_and_dogs_small_data_augmentation.h5'</span>)
<span class="linenr"> 90: </span>
<span class="linenr"> 91: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32362;&#35069;model&#30340;&#25613;&#22833;&#29575;&#33287;&#31934;&#30906;&#29575;</span>
<span class="linenr"> 92: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 93: </span>
<span class="linenr"> 94: </span>  <span style="color: #dcaeea;">acc</span> = history.history[<span style="color: #98be65;">'acc'</span>]
<span class="linenr"> 95: </span>  <span style="color: #dcaeea;">val_acc</span> = history.history[<span style="color: #98be65;">'val_acc'</span>]
<span class="linenr"> 96: </span>  <span style="color: #dcaeea;">loss</span> = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr"> 97: </span>  <span style="color: #dcaeea;">val_loss</span> = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr"> 98: </span>
<span class="linenr"> 99: </span>  <span style="color: #dcaeea;">epochs</span> = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(acc) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">100: </span>  plt.clf()
<span class="linenr">101: </span>  plt.plot(epochs, acc, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr">102: </span>  plt.plot(epochs, val_acc, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr">103: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">104: </span>  plt.legend()
<span class="linenr">105: </span>  plt.plot()
<span class="linenr">106: </span>  plt.savefig(<span style="color: #98be65;">"CatsDogsDataAugmentation-acc.png"</span>)
<span class="linenr">107: </span>  plt.figure()
<span class="linenr">108: </span>
<span class="linenr">109: </span>  plt.clf()
<span class="linenr">110: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">111: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">112: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">113: </span>  plt.legend()
<span class="linenr">114: </span>  plt.plot()
<span class="linenr">115: </span>  plt.savefig(<span style="color: #98be65;">"CatsDogsDataAugmentation-loss.png"</span>)
<span class="linenr">116: </span>
</pre>
</div>

<pre class="example">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 6272)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 6272)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               3211776   
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513       
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
100/100 [==============================] - 101s 1s/step - loss: 0.3521 - acc: 0.8438 - val_loss: 0.4726 - val_acc: 0.8061
</pre>


<div id="org4d5a4f8" class="figure">
<p><img src="images/CatsDogsDataAugmentation-acc.png" alt="CatsDogsDataAugmentation-acc.png" /><br />
</p>
<p><span class="figure-number">Figure 70: </span>Cats and Dogs Data Augmentation - Accuracy</p>
</div>


<div id="orga98e58a" class="figure">
<p><img src="images/CatsDogsDataAugmentation-loss.png" alt="CatsDogsDataAugmentation-loss.png" /><br />
</p>
<p><span class="figure-number">Figure 71: </span>Cats and Dogs Data Augmentation - Loss</p>
</div>


<p>
由圖<a href="#org4d5a4f8">70</a>和<a href="#orga98e58a">71</a>可以發現，在加入了 data augmentation 和 dropout 後，訓練曲線與驗證曲線漸趨一致，不再 overfitting，model 的準確度也達到 84%。但值的一題的是，同樣的資料集與演算法，在 Google colab 上以 GPU 執行的結果(下圖)與在本機執行(上圖)時並不相同。<br />
</p>


<div id="org97238bd" class="figure">
<p><img src="images/Cat-Dog-Data-Augmentation-Acc-colab.png" alt="Cat-Dog-Data-Augmentation-Acc-colab.png" /><br />
</p>
<p><span class="figure-number">Figure 72: </span>Cats and Dogs Data Augmentation on Google colab - Accuracy</p>
</div>


<div id="org881bf7c" class="figure">
<p><img src="images/Cat-Dog-Data-Augmentation-loss-colab.png" alt="Cat-Dog-Data-Augmentation-loss-colab.png" /><br />
</p>
<p><span class="figure-number">Figure 73: </span>Cats and Dogs Data Augmentation on Google colab - Loss</p>
</div>

<p>
在透過進一步 regularization 技術的使用，以及調整神經網路參數(如每個卷積層的過濾器數量、神經網路中的層數)，我們就能獲得更高的準確度(86%或 87%)，但在資料不及的情況下(如本例)，我們仍很難進一步提升準確度，此時，就要使用預先訓練 model。<br />
</p>
</div>
</div>


<div id="outline-container-orgac8c095" class="outline-3">
<h3 id="orgac8c095"><span class="section-number-3">11.4</span> 改善 2: 使用 pretrained network</h3>
<div class="outline-text-3" id="text-11-4">
<p>
Pretrained network，以簡單的話來說，就是「站在巨人的肩膀」<sup><a id="fnr.12" class="footref" href="#fn.12">12</a></sup>，所謂「巨人」，就是別人已經用 ImageNet 訓練好的模型，例如 Google 的 Inception Model、Microsoft 的 Resnet Model 等等，把它當作 Pre-trained Model，幫助我們提取出照片的特徵(feature)。順帶一提，所謂的 Transfer Learning 就是把 Pre-trained Model 最後一層拔掉 (註：最後一層是用來分類的)，加入新的一層，然後用新資料訓練新層的參數。<br />
</p>

<p>
能夠用來被當成 pretrained netwrok 的 model 通常是擁有大量資料集的大規模圖片分類模型，如果這個原始資料集足夠大量且具通用性，那麼 pretrained network 學習的空間層次特徵(spartial hierarchy features)就足以充當視覺世界的通用 model，其特徵對於許多不同的電腦視覺問題都同樣有效，即便是要辨識與原始任務完全不同的類別也能通用。<br />
</p>

<p>
例如，以 ImageNet 先訓練出一個神經網路(其辨識項目為日常生活用品)，然後重新訓練這個已訓練完成的神經網路，去識別和原始樣本天差地別的家具產品等。和許多淺層的神經網路相較，深度學習的關鍵優勢在於學習到的特徵可移植到不同問題上。<br />
</p>

<p>
以下由 Karen Simonyan 和 Andrew Zisserman 於 2014 年開發的 VGG16 架構。使用 pretrain network 有兩種方式：特徵萃取(feature extraction)和徵調(fine-tuning)。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org667438a"></a>特徵萃取<br />
<div class="outline-text-4" id="text-11-4-1">
<p>
Feature extraction 是使用 pretrained network 學習到的表示法，以這些表示法從新樣本中萃取有趣的特徵，然後將這些特徵輸入到從頭訓練的新分類器中進行處理。用於影像分類的 CNN 分為以下兩部份：以一系列的卷積層和池化層開始，以密集連接的分類器結束。第一部分稱為 model 的 convolutional base (卷積基底)，在 CNN 的情況下，特徵萃取以一個 pretrained network 做為 convolutional base，透過 convolutional base 處理新資料，<br />
</p>


<div id="org1073974" class="figure">
<p><img src="images/img-191126103936.jpg" alt="img-191126103936.jpg" /><br />
</p>
<p><span class="figure-number">Figure 74: </span>套用同樣的 convolutional base，交換分類器</p>
</div>

<p>
為何不連分類器也預先訓練？原因是 CNN 的特徵圖是來自影像上通用 pattern 的概念，因此無論面臨何種電腦視覺問題，都能通用；而分類器學習到的表示法可能只適用於 model 所訓練的類別，僅關於整個影像中該類別相關的機率。此外，卷積特徵圖仍會描述物件出現的位置，但密集層並沒有空間的概念，密集層學習到的表示法不再包含物件在輸入影像中位罝的任何訊息，所以只要是和物件出現位置相關的問題，密集層產生的特徵絕大多數是沒有用的。<br />
</p>

<p>
特定卷積層所萃取出來的表示法，其普適程度取於該層的深度，model 中較早出現的層會萃取局部、高度通用的特徵圖（例如可視邊緣、顏色或紋理），而較深入的層則會萃取更抽象的概念（如貓耳朵、狗眼），如果新的資料集與訓練原始 model 的資料集有很大的差別，最好使用 model 的前幾層來進行特徵萃取，而不是使用整個 convolutional base。以下以 ImageNet 訓練的 VGG16 所產生的 convolutional base 來實作，類似 pretrained 的影像分類 model 還有 Xception、Inception V3、ResNet50、VGG19、MobileNet，均已收錄於 keras.applications。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="orga8fa38e"></a>1. 初始化 model<br />
<div class="outline-text-5" id="text-11-4-1-1">
<p>
要使用這個 pretrained model，還需要傳三個參數給 VGG16 建構式：<br />
</p>
<ul class="org-ul">
<li>weights: 用於初始化 model 的權重檢查點<br /></li>
<li>include_top: 指在神經網路頂部有沒有包含密集連接的分類器。預設情況下，密集連接分類器對應於 ImageNet 的 1000 個類別。然而，我們實際想分類的可能沒這麼多層，所以這裡不一定要包含預設分類器。<br /></li>
<li>input_shape: qpaqamo 供給神經網路的影像張量 shape。這個參數為 optional，如果不傳，則神經網路能處理任何 shape 的輸入張量。<br /></li>
</ul>

<div class="org-src-container">
<pre class="src src-python" id="orgd353770"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr">2: </span>
<span class="linenr">3: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,
<span class="linenr">4: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr">5: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr">6: </span>  conv_base.summary()
</pre>
</div>

<pre class="example">
Model: "vgg16"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 150, 150, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
</pre>

<p>
由上述輸出觀察，最終特徵圖的 shape 為(4, 4, 512)，這算是神經網路的 top 層特徵，這個預訓練的 model 共有 13 層 Conv2D 層，最後要再接上密集連接分類器。做法有二：<br />
</p>


<ol class="org-ol">
<li>在資料集上執行 convolutional base，將輸出記錄到硬碟上的 Numpy 陣列，然後再輸入到獨立的密集分類層。這種解決方案只需要為每個輪入影像執行一次 convolutional base，而 convolutional base 是處理過程中成本最高的部份，所以這種做法速度快成本低。但也因如此，這種做法不允許使用資料擴增法。<br /></li>
<li>在頂部(最後端)增加 Dnese 層來擴展 model (conv_base)，並從輸入資料開始，從頭到尾執行整個處理過程。這種方式允許資料擴增技術，因為每次輸入影像在執行 convolutional base 時都會在 model 處理到。但這種方式的成本較高。<br /></li>
</ol>
</div>
</li>

<li><a id="orgb78ab5d"></a>2. 快速特徵萃取<br />
<div class="outline-text-5" id="text-11-4-1-2">
<p>
先執行 ImageDataGenerator，將影像轉換為 Numpy 陣列及其 label 向量，然後呼叫 conv_base model 的 predict 方法從這些影像中萃取特徵。<br />
</p>


<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,
<span class="linenr"> 4: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr"> 5: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">import</span> os
<span class="linenr"> 8: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 9: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr">12: </span>  <span style="color: #dcaeea;">train_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr">13: </span>  <span style="color: #dcaeea;">validation_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)
<span class="linenr">14: </span>  <span style="color: #dcaeea;">test_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'test'</span>)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #dcaeea;">datagen</span> = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>)
<span class="linenr">17: </span>  <span style="color: #dcaeea;">batch_size</span> = <span style="color: #da8548; font-weight: bold;">20</span>
<span class="linenr">18: </span>
<span class="linenr">19: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">extract_features</span>(directory, sample_count):
<span class="linenr">20: </span>      <span style="color: #dcaeea;">features</span> = np.zeros(shape=(sample_count, <span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr">21: </span>      <span style="color: #dcaeea;">labels</span> = np.zeros(shape=(sample_count))
<span class="linenr">22: </span>      <span style="color: #dcaeea;">generator</span> = datagen.flow_from_directory(directory,
<span class="linenr">23: </span>                                              target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),
<span class="linenr">24: </span>                                              batch_size=batch_size,
<span class="linenr">25: </span>                                              class_mode=<span style="color: #98be65;">'binary'</span>)
<span class="linenr">26: </span>      <span style="color: #dcaeea;">i</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">27: </span>      <span style="color: #51afef;">for</span> inputs_batch, labels_batch <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">generator</span>:
<span class="linenr">28: </span>          features_batch = conv_base.predict(inputs_batch)
<span class="linenr">29: </span>          <span style="color: #dcaeea;">features</span>[i * batch_size : (i + <span style="color: #da8548; font-weight: bold;">1</span>) * batch_size] = features_batch
<span class="linenr">30: </span>          <span style="color: #dcaeea;">labels</span>[i * batch_size : (i + <span style="color: #da8548; font-weight: bold;">1</span>) * batch_size] = labels_batch
<span class="linenr">31: </span>          <span style="color: #dcaeea;">i</span> += <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">32: </span>          <span style="color: #51afef;">print</span>(i, end=<span style="color: #98be65;">' '</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30001;&#26044;&#33795;&#21462;&#38656;&#35201;&#36611;&#38263;&#30340;&#26178;&#38291;&#65292;&#25105;&#20497;&#21360;&#20986; i &#20358;&#27298;&#35222;&#36914;&#24230;</span>
<span class="linenr">33: </span>          <span style="color: #51afef;">if</span> i * batch_size &gt;= <span style="color: #dcaeea;">sample_count</span>:
<span class="linenr">34: </span>              <span style="color: #51afef;">break</span>
<span class="linenr">35: </span>      <span style="color: #51afef;">return</span> features, labels
<span class="linenr">36: </span>
<span class="linenr">37: </span>  <span style="color: #dcaeea;">train_features</span>, train_labels = extract_features(train_dir, <span style="color: #da8548; font-weight: bold;">2000</span>)
<span class="linenr">38: </span>  <span style="color: #dcaeea;">validation_features</span>, <span style="color: #dcaeea;">validation_labels</span> = extract_features(validation_dir, <span style="color: #da8548; font-weight: bold;">1000</span>)
<span class="linenr">39: </span>  <span style="color: #dcaeea;">test_features</span>, <span style="color: #dcaeea;">test_labels</span> = extract_features(test_dir, <span style="color: #da8548; font-weight: bold;">1000</span>)
</pre>
</div>

<pre class="example">
Found 2000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 Found 1000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Found 1000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 
</pre>
</div>
</li>

<li><a id="org322389f"></a>3. 展平資料<br />
<div class="outline-text-5" id="text-11-4-1-3">
<p>
由於目前的萃取特徵 shape = (樣本數, 4, 4, 512)，為了要提供給密集層分類器，必須將資料展平為(樣本數, 8192)。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">train_features</span> = np.reshape(train_features, (<span style="color: #da8548; font-weight: bold;">2000</span>, <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr">2: </span>  <span style="color: #dcaeea;">validation_features</span> = np.reshape(validation_features, (<span style="color: #da8548; font-weight: bold;">1000</span>, <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr">3: </span>  <span style="color: #dcaeea;">test_features</span> = np.reshape(test_features, (<span style="color: #da8548; font-weight: bold;">1000</span>, <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">512</span>))
</pre>
</div>
</div>
</li>

<li><a id="org558ebf0"></a>4. 訓練<br />
<div class="outline-text-5" id="text-11-4-1-4">
<p>
接下來就可以建立我們的密集分類層（使用 dropout 和 regularization)在剛剛萃取的資料和標籤上進行訓練。因為只有兩個密集層，所以訓練的速度會很快。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr"> 6: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">256</span>, activation=<span style="color: #98be65;">'relu'</span>, input_dim=<span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr"> 7: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#19999;&#26820;&#27861;</span>
<span class="linenr"> 8: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=optimizers.RMSprop(lr=2e-<span style="color: #da8548; font-weight: bold;">5</span>),
<span class="linenr">11: </span>                loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">12: </span>                metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr">13: </span>
<span class="linenr">14: </span>  <span style="color: #dcaeea;">history</span> = model.fit(train_features, 
<span class="linenr">15: </span>                      train_labels,epochs=<span style="color: #da8548; font-weight: bold;">30</span>,
<span class="linenr">16: </span>                      batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">17: </span>                      validation_data=(validation_features, validation_labels))
</pre>
</div>
</div>
</li>

<li><a id="org2aeb274"></a>5. 繪圖<br />
<div class="outline-text-5" id="text-11-4-1-5">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">acc</span> = history.history[<span style="color: #98be65;">'acc'</span>]
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">val_acc</span> = history.history[<span style="color: #98be65;">'val_acc'</span>]
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">loss</span> = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">val_loss</span> = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">epochs</span> = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(acc) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  plt.plot(epochs, acc, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr">11: </span>  plt.plot(epochs, val_acc, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr">12: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">13: </span>  plt.legend()
<span class="linenr">14: </span>
<span class="linenr">15: </span>  plt.figure()
<span class="linenr">16: </span>
<span class="linenr">17: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">18: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">19: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">20: </span>  plt.legend()
<span class="linenr">21: </span>
<span class="linenr">22: </span>  plt.show()
</pre>
</div>
</div>
</li>

<li><a id="org594ea75"></a>6. 實際執行結果<br />
<div class="outline-text-5" id="text-11-4-1-6">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">#####</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,
<span class="linenr"> 5: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr"> 6: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #51afef;">import</span> os
<span class="linenr"> 9: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">10: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr">11: </span>
<span class="linenr">12: </span>  <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr">13: </span>  <span style="color: #dcaeea;">train_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr">14: </span>  <span style="color: #dcaeea;">validation_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)
<span class="linenr">15: </span>  <span style="color: #dcaeea;">test_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'test'</span>)
<span class="linenr">16: </span>
<span class="linenr">17: </span>  <span style="color: #dcaeea;">datagen</span> = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>)
<span class="linenr">18: </span>  <span style="color: #dcaeea;">batch_size</span> = <span style="color: #da8548; font-weight: bold;">20</span>
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">extract_features</span>(directory, sample_count):
<span class="linenr">21: </span>      <span style="color: #dcaeea;">features</span> = np.zeros(shape=(sample_count, <span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr">22: </span>      <span style="color: #dcaeea;">labels</span> = np.zeros(shape=(sample_count))
<span class="linenr">23: </span>      <span style="color: #dcaeea;">generator</span> = datagen.flow_from_directory(directory,
<span class="linenr">24: </span>                                              target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),
<span class="linenr">25: </span>                                              batch_size=batch_size,
<span class="linenr">26: </span>                                              class_mode=<span style="color: #98be65;">'binary'</span>)
<span class="linenr">27: </span>      <span style="color: #dcaeea;">i</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">28: </span>      <span style="color: #51afef;">for</span> inputs_batch, labels_batch <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">generator</span>:
<span class="linenr">29: </span>          features_batch = conv_base.predict(inputs_batch)
<span class="linenr">30: </span>          <span style="color: #dcaeea;">features</span>[i * batch_size : (i + <span style="color: #da8548; font-weight: bold;">1</span>) * batch_size] = features_batch
<span class="linenr">31: </span>          <span style="color: #dcaeea;">labels</span>[i * batch_size : (i + <span style="color: #da8548; font-weight: bold;">1</span>) * batch_size] = labels_batch
<span class="linenr">32: </span>          <span style="color: #dcaeea;">i</span> += <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">33: </span>          <span style="color: #51afef;">print</span>(i, end=<span style="color: #98be65;">' '</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30001;&#26044;&#33795;&#21462;&#38656;&#35201;&#36611;&#38263;&#30340;&#26178;&#38291;&#65292;&#25105;&#20497;&#21360;&#20986; i &#20358;&#27298;&#35222;&#36914;&#24230;</span>
<span class="linenr">34: </span>          <span style="color: #51afef;">if</span> i * batch_size &gt;= <span style="color: #dcaeea;">sample_count</span>:
<span class="linenr">35: </span>              <span style="color: #51afef;">break</span>
<span class="linenr">36: </span>      <span style="color: #51afef;">return</span> features, labels
<span class="linenr">37: </span>
<span class="linenr">38: </span>  <span style="color: #dcaeea;">train_features</span>, train_labels = extract_features(train_dir, <span style="color: #da8548; font-weight: bold;">2000</span>)
<span class="linenr">39: </span>  <span style="color: #dcaeea;">validation_features</span>, <span style="color: #dcaeea;">validation_labels</span> = extract_features(validation_dir, <span style="color: #da8548; font-weight: bold;">1000</span>)
<span class="linenr">40: </span>  <span style="color: #dcaeea;">test_features</span>, <span style="color: #dcaeea;">test_labels</span> = extract_features(test_dir, <span style="color: #da8548; font-weight: bold;">1000</span>)
<span class="linenr">41: </span>
<span class="linenr">42: </span>  <span style="color: #5B6268;">#####</span>
<span class="linenr">43: </span>  <span style="color: #dcaeea;">train_features</span> = np.reshape(train_features, (<span style="color: #da8548; font-weight: bold;">2000</span>, <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr">44: </span>  <span style="color: #dcaeea;">validation_features</span> = np.reshape(validation_features, (<span style="color: #da8548; font-weight: bold;">1000</span>, <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr">45: </span>  <span style="color: #dcaeea;">test_features</span> = np.reshape(test_features, (<span style="color: #da8548; font-weight: bold;">1000</span>, <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr">46: </span>
<span class="linenr">47: </span>
<span class="linenr">48: </span>  <span style="color: #5B6268;">#####</span>
<span class="linenr">49: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">50: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">51: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr">52: </span>
<span class="linenr">53: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">54: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">256</span>, activation=<span style="color: #98be65;">'relu'</span>, input_dim=<span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr">55: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#19999;&#26820;&#27861;</span>
<span class="linenr">56: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
<span class="linenr">57: </span>
<span class="linenr">58: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=optimizers.RMSprop(lr=2e-<span style="color: #da8548; font-weight: bold;">5</span>),
<span class="linenr">59: </span>                loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">60: </span>                metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr">61: </span>
<span class="linenr">62: </span>  <span style="color: #dcaeea;">history</span> = model.fit(train_features, 
<span class="linenr">63: </span>                      train_labels,epochs=<span style="color: #da8548; font-weight: bold;">30</span>,
<span class="linenr">64: </span>                      batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">65: </span>                      validation_data=(validation_features, validation_labels))
<span class="linenr">66: </span>
<span class="linenr">67: </span>
<span class="linenr">68: </span>  <span style="color: #5B6268;">#####</span>
<span class="linenr">69: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">70: </span>
<span class="linenr">71: </span>  <span style="color: #dcaeea;">acc</span> = history.history[<span style="color: #98be65;">'acc'</span>]
<span class="linenr">72: </span>  <span style="color: #dcaeea;">val_acc</span> = history.history[<span style="color: #98be65;">'val_acc'</span>]
<span class="linenr">73: </span>  <span style="color: #dcaeea;">loss</span> = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr">74: </span>  <span style="color: #dcaeea;">val_loss</span> = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr">75: </span>
<span class="linenr">76: </span>  <span style="color: #dcaeea;">epochs</span> = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(acc) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">77: </span>  plt.clf()
<span class="linenr">78: </span>  plt.plot(epochs, acc, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr">79: </span>  plt.plot(epochs, val_acc, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr">80: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">81: </span>  plt.legend()
<span class="linenr">82: </span>  plt.plot()
<span class="linenr">83: </span>  plt.savefig(<span style="color: #98be65;">"Pretrained-VGG16-1-acc.png"</span>)
<span class="linenr">84: </span>  plt.figure()
<span class="linenr">85: </span>
<span class="linenr">86: </span>  plt.clf()
<span class="linenr">87: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">88: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">89: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">90: </span>  plt.legend()
<span class="linenr">91: </span>  plt.plot()
<span class="linenr">92: </span>  plt.savefig(<span style="color: #98be65;">"Pretrained-VGG16-1-loss.png"</span>)
<span class="linenr">93: </span>
</pre>
</div>

<pre class="example">
Found 2000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 Found 1000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Found 1000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Train on 2000 samples, validate on 1000 samples
Epoch 30/30
1980/2000 [============================&gt;.] - ETA: 0s - loss: 0.0909 - acc: 0.9722
2000/2000 [==============================] - 3s 2ms/step - loss: 0.0905 - acc: 0.9725 - val_loss: 0.2450 - val_acc: 0.9020
</pre>


<div id="org04312cb" class="figure">
<p><img src="images/Pretrained-VGG16-1-acc.png" alt="Pretrained-VGG16-1-acc.png" /><br />
</p>
<p><span class="figure-number">Figure 75: </span>簡單特徵萃取的訓練和驗證準確度</p>
</div>


<div id="org2b7de6b" class="figure">
<p><img src="images/Pretrained-VGG16-1-loss.png" alt="Pretrained-VGG16-1-loss.png" /><br />
</p>
<p><span class="figure-number">Figure 76: </span>簡單特徵萃取的訓練和驗證損失</p>
</div>

<p>
圖中顯示可以達到 90%的驗證準確度，比較原來的 model 成效，雖然準確度有提高，但仍可看到 overfitting 的情況，即便 model 裡已套用了 dropout，也許是因為無法使用資料擴增法，對 overfitting 的防治仍然有限。<br />
</p>
</div>
</li>

<li><a id="orgd4798b1"></a>7. 加入資料擴增的特徵萃取<br />
<div class="outline-text-5" id="text-11-4-1-7">
<p>
將資料擴增加入特徵萃取的作法是擴展 conv_base model 並從輸入資料開始，從頭到尾執行整個處理過程，這種做法的運算成本非常昂貴，只能在 GPU 上執行，在 CPU 上絕對難以處理。由於 model 的行為與 layer 類似，因此可以將 model(如 conv\uunder{}base)視為 layer，增加到 Sequential model 中，就如同增加神經網路的 layer 一樣。其作法如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python" id="org93fe829"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21367;&#31309;&#22522;&#24213;</span>
<span class="linenr"> 6: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr"> 7: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">10: </span>  model.add(conv_base)        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#21367;&#31309;&#22522;&#24213;&#35222;&#28858;&#23652;&#21152;&#20837; Sequential &#27169;&#22411;&#20013;</span>
<span class="linenr">11: </span>  model.add(layers.Flatten()) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25892;&#24179;</span>
<span class="linenr">12: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">256</span>, activation=<span style="color: #98be65;">'relu'</span>)) 
<span class="linenr">13: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>)) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22686;&#21152;&#23494;&#38598;&#23652;&#20998;&#39006;&#22120;</span>
<span class="linenr">14: </span>  model.summary() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26597;&#30475;&#27169;&#22411;&#25688;&#35201;</span>
</pre>
</div>

<pre class="example">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
vgg16 (Model)                (None, 4, 4, 512)         14714688  
_________________________________________________________________
flatten_1 (Flatten)          (None, 8192)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 256)               2097408   
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 257       
=================================================================
Total params: 16,812,353
Trainable params: 16,812,353
Non-trainable params: 0
_________________________________________________________________
</pre>

<p>
如上圖，VGG16 的 convolutional base 有 14714688 個參數，在頂部(後端)增加的分類器有 200 多萬個參數。在加入資料擴增之前，凍結 convolutional base 是非常重要的，凍結(freeze)表示在訓練期間禁止更新權重，如果不這樣做，則 convolutional base 先前學習到的表示法就會在訓練期間被修改掉，因為頂部的 Dense 層是隨機初始化的，所以非常大量的權重更新將透過神經網路傳播，會導致先前學習到的表示法被破壞掉。<br />
</p>

<p>
在 Keras 中，可以透過設定模型的 trainable 屬性為 False 來凍結 convolutional base 神經網路：<br />
</p>

<div class="org-src-container">
<pre class="src src-python" id="org42f20c3"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21367;&#31309;&#22522;&#24213;</span>
<span class="linenr"> 6: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr"> 7: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">10: </span>  model.add(conv_base)        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#21367;&#31309;&#22522;&#24213;&#35222;&#28858;&#23652;&#21152;&#20837; Sequential &#27169;&#22411;&#20013;</span>
<span class="linenr">11: </span>  model.add(layers.Flatten()) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25892;&#24179;</span>
<span class="linenr">12: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">256</span>, activation=<span style="color: #98be65;">'relu'</span>)) 
<span class="linenr">13: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>)) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22686;&#21152;&#23494;&#38598;&#23652;&#20998;&#39006;&#22120;</span>
<span class="linenr">14: </span>
<span class="linenr">15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">freeze convolutional base</span>
<span class="linenr">16: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'This is the number of trainable weights '</span>
<span class="linenr">17: </span>  <span style="color: #98be65;">'before freezing the conv base:'</span>, <span style="color: #c678dd;">len</span>(model.trainable_weights))
<span class="linenr">18: </span>  conv_base.trainable = <span style="color: #a9a1e1;">False</span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20941;&#32080;&#27402;&#37325;</span>
<span class="linenr">19: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'This is the number of trainable weights '</span>
<span class="linenr">20: </span>  <span style="color: #98be65;">'after freezing the conv base:'</span>, <span style="color: #c678dd;">len</span>(model.trainable_weights))
</pre>
</div>

<pre class="example">
This is the number of trainable weights before freezing the conv base: 30
This is the number of trainable weights after freezing the conv base: 4
</pre>


<p>
由於 conv_base 被凍結更新權重，所以 model 只會訓練增力的兩個 Dense 層權重，每層有兩個參數要更新(主要權重矩陣和偏差向量)，所以一共剩 4 個 trainable weights，原本的 pretrained model 有 13 層 Conv2D，共 26 個 trainable weights。<br />
</p>

<p>
接下來就可以使用資料擴增來訓練 model:<br />
</p>

<div class="org-src-container">
<pre class="src src-python" id="orge033fb5"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21367;&#31309;&#22522;&#24213;</span>
<span class="linenr"> 6: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr"> 7: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">import</span> os
<span class="linenr">10: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">11: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr">12: </span>
<span class="linenr">13: </span>  <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr">14: </span>  <span style="color: #dcaeea;">train_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr">15: </span>  <span style="color: #dcaeea;">validation_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)
<span class="linenr">16: </span>  <span style="color: #dcaeea;">test_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'test'</span>)
<span class="linenr">17: </span>
<span class="linenr">18: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">19: </span>  model.add(conv_base)        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#21367;&#31309;&#22522;&#24213;&#35222;&#28858;&#23652;&#21152;&#20837; Sequential &#27169;&#22411;&#20013;</span>
<span class="linenr">20: </span>  model.add(layers.Flatten()) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25892;&#24179;</span>
<span class="linenr">21: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">256</span>, activation=<span style="color: #98be65;">'relu'</span>)) 
<span class="linenr">22: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>)) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22686;&#21152;&#23494;&#38598;&#23652;&#20998;&#39006;&#22120;</span>
<span class="linenr">23: </span>
<span class="linenr">24: </span>  conv_base.<span style="color: #dcaeea;">trainable</span> = <span style="color: #a9a1e1;">False</span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20941;&#32080;&#27402;&#37325;</span>
<span class="linenr">25: </span>
<span class="linenr">26: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">data augmentation</span>
<span class="linenr">27: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr">28: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr">29: </span>
<span class="linenr">30: </span>  <span style="color: #dcaeea;">train_datagen</span> = ImageDataGenerator( <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25844;&#20805;&#35347;&#32244;&#36039;&#26009;</span>
<span class="linenr">31: </span>    rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>,
<span class="linenr">32: </span>    rotation_range=<span style="color: #da8548; font-weight: bold;">40</span>,
<span class="linenr">33: </span>    width_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr">34: </span>    height_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr">35: </span>    shear_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr">36: </span>    zoom_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr">37: </span>    horizontal_flip=<span style="color: #a9a1e1;">True</span>,
<span class="linenr">38: </span>    fill_mode=<span style="color: #98be65;">'nearest'</span>)
<span class="linenr">39: </span>
<span class="linenr">40: </span>  <span style="color: #dcaeea;">test_datagen</span> = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35531;&#27880;&#24847;&#39511;&#35657;&#36039;&#26009;&#19981;&#25033;&#35442;&#25844;&#20805;</span>
<span class="linenr">41: </span>
<span class="linenr">42: </span>
<span class="linenr">43: </span>  <span style="color: #dcaeea;">train_generator</span> = train_datagen.flow_from_directory(
<span class="linenr">44: </span>    train_dir, <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30446;&#27161;&#30446;&#37636;&#36335;&#24465;</span>
<span class="linenr">45: </span>    target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>), <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35519;&#25972;&#25152;&#26377;&#22294;&#20687;&#22823;&#23567;&#25104; 150&#215;150 </span>
<span class="linenr">46: </span>    batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">47: </span>    class_mode=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22240;&#28858;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109; binary_crossentropy &#20316;&#28858;&#25613;&#22833;&#20998;&#25976;&#65292;&#25152;                       &#20197;&#38656;&#35201;&#20108;&#20803;&#27161;&#31844;</span>
<span class="linenr">48: </span>
<span class="linenr">49: </span>  <span style="color: #dcaeea;">validation_generator</span> = test_datagen.flow_from_directory(
<span class="linenr">50: </span>    validation_dir,
<span class="linenr">51: </span>    target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),
<span class="linenr">52: </span>    batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">53: </span>    class_mode=<span style="color: #98be65;">'binary'</span>)
<span class="linenr">54: </span>
<span class="linenr">55: </span>  model.<span style="color: #c678dd;">compile</span>( loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">56: </span>         optimizer=optimizers.RMSprop(lr=2e-<span style="color: #da8548; font-weight: bold;">5</span>),
<span class="linenr">57: </span>         metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr">58: </span>
<span class="linenr">59: </span>  <span style="color: #dcaeea;">history</span> = model.fit_generator(
<span class="linenr">60: </span>    train_generator,
<span class="linenr">61: </span>    steps_per_epoch=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr">62: </span>    epochs=<span style="color: #da8548; font-weight: bold;">30</span>,
<span class="linenr">63: </span>    validation_data=validation_generator,
<span class="linenr">64: </span>    validation_steps=<span style="color: #da8548; font-weight: bold;">50</span>)
<span class="linenr">65: </span>
<span class="linenr">66: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32362;&#35069;model&#30340;&#25613;&#22833;&#29575;&#33287;&#31934;&#30906;&#29575;</span>
<span class="linenr">67: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">68: </span>
<span class="linenr">69: </span>  <span style="color: #dcaeea;">acc</span> = history.history[<span style="color: #98be65;">'acc'</span>]
<span class="linenr">70: </span>  <span style="color: #dcaeea;">val_acc</span> = history.history[<span style="color: #98be65;">'val_acc'</span>]
<span class="linenr">71: </span>  <span style="color: #dcaeea;">loss</span> = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr">72: </span>  <span style="color: #dcaeea;">val_loss</span> = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr">73: </span>
<span class="linenr">74: </span>  <span style="color: #dcaeea;">epochs</span> = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(acc) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">75: </span>  plt.clf()
<span class="linenr">76: </span>  plt.plot(epochs, acc, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr">77: </span>  plt.plot(epochs, val_acc, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr">78: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">79: </span>  plt.legend()
<span class="linenr">80: </span>  plt.plot()
<span class="linenr">81: </span>  plt.savefig(<span style="color: #98be65;">"CatsDogsDataAugmentationPretrained-acc.png"</span>)
<span class="linenr">82: </span>  plt.figure()
<span class="linenr">83: </span>
<span class="linenr">84: </span>  plt.clf()
<span class="linenr">85: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">86: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">87: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">88: </span>  plt.legend()
<span class="linenr">89: </span>  plt.plot()
<span class="linenr">90: </span>  plt.savefig(<span style="color: #98be65;">"CatsDogsDataAugmentationPretrained-loss.png"</span>)
<span class="linenr">91: </span>
</pre>
</div>

<pre class="example">
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
Epoch 1/30
2900  1/100 [..............................] - ETA: 8:14 - loss: 0.6634 - acc: 0.5500
100/100 [==============================] - 575s 6s/step - loss: 0.2791 - acc: 0.8820 - val_loss: 0.4186 - val_acc: 0.8990
</pre>


<div id="orge183b08" class="figure">
<p><img src="images/CatsDogsDataAugmentationPretrained-acc.png" alt="CatsDogsDataAugmentationPretrained-acc.png" /><br />
</p>
<p><span class="figure-number">Figure 77: </span>Cats and Dogs Data Augmentation / Pretrained- Accuracy</p>
</div>


<div id="org6e7c0ea" class="figure">
<p><img src="images/CatsDogsDataAugmentationPretrained-loss.png" alt="CatsDogsDataAugmentationPretrained-loss.png" /><br />
</p>
<p><span class="figure-number">Figure 78: </span>Cats and Dogs Data Augmentation - Loss</p>
</div>

<p>
實作結果，驗證準確率達 90%，優於從頭訓練小型神經網路（結果與原書中達 96%有所出入）。<br />
</p>
</div>
</li>
</ol>
</li>

<li><a id="org3220f29"></a>微調<br />
<div class="outline-text-4" id="text-11-4-2">
<p>
微調(fine-tuning)為另一種廣泛使用的 model reuse 技術，本質上是特徵萃取的變化版，其做法是在特徵萃取的過程中不凍結整個 convolutional base，而是解凍 convolutional base 頂部的某些層以用於特徵萃取，並對於新增加於 model 的部份(如密集層分類器)與被解凍的部份層一起進行聯合訓練。<br />
</p>

<p>
微調神經網路的步驟如下：<br />
</p>
<ol class="org-ol">
<li>在已訓練過的基礎神經網路(即 convolutional base)上增加自定義神經網路<br /></li>
<li>凍結 convolutional base<br /></li>
<li>訓練步驟 1 增加的部份(即最頂端的分類器)<br /></li>
<li>解凍 convolutional base 的某幾層<br /></li>
<li>共同訓練解凍層和分類器<br /></li>
</ol>

<p>
以 VGG16 的模組架構為例，其分層架構如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python" id="orgd16aceb"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr">2: </span>
<span class="linenr">3: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,
<span class="linenr">4: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr">5: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr">6: </span>  conv_base.summary()
</pre>
</div>

<pre class="example">
Model: "vgg16"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 150, 150, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
</pre>

<p>
我們可以調整這個 convolutional base 的最頂層(block5)三層的卷積層，即 block5_conv1、block5_conv2、block5_conv3 三層，然後凍結 block4_pool 以下的所有層。之所以選擇只解凍 convolutional base 的最頂層，幾個考量原因如下：<br />
</p>

<ul class="org-ul">
<li>相對於 convolutional base 中的低層主要是對更通用、可重複使用的特徵進行編碼；更高層則是對更特定的特徵進行編碼，所以這些特徵需要重新調整才能適用於新的問題。如果是對低層進行微調，則會出現反效果。<br /></li>
</ul>
<ul class="org-ul">
<li>訓練的參數越多，就越可能 overfitting。convolutional base 有近 1500 萬個參數，因此在少量資料集上訓練會有風險。<br /></li>
</ul>

<p>
解凍部份 convolutional base 的方式如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python" id="org86c6dde"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,
<span class="linenr"> 4: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr"> 5: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 6: </span>  conv_base.summary()
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  conv_base.<span style="color: #dcaeea;">trainable</span> = <span style="color: #a9a1e1;">True</span> <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20808;&#35373;&#23450;&#25152;&#26377;layer&#37117;&#21487;&#35347;&#32244;?</span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">set_trainable</span> = <span style="color: #a9a1e1;">False</span> <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38928;&#35373;&#28858;&#20941;&#32080;</span>
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #51afef;">for</span> layer <span style="color: #51afef;">in</span> conv_base.layers: <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#30001;&#20302;&#21040;&#39640;</span>
<span class="linenr">12: </span>      <span style="color: #51afef;">if</span> layer.name == <span style="color: #98be65;">'block5_conv1'</span>: <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#30452;&#21040;&#20986;&#29694;block5_conv1&#36889;&#23652;&#24460;&#38283;&#22987;&#35299;&#20941;</span>
<span class="linenr">13: </span>          <span style="color: #dcaeea;">set_trainable</span> = <span style="color: #a9a1e1;">True</span>
<span class="linenr">14: </span>      <span style="color: #51afef;">if</span> <span style="color: #dcaeea;">set_trainable</span>:
<span class="linenr">15: </span>          layer.trainable = <span style="color: #a9a1e1;">True</span>
<span class="linenr">16: </span>      <span style="color: #51afef;">else</span>:
<span class="linenr">17: </span>          layer.trainable = <span style="color: #a9a1e1;">False</span>
</pre>
</div>

<p>
解凍完部份 layer 後即可開始徵調神經網路，這裡使用 RMSProp 優化器以非常低的學習率來微調，降低學習率的目的在減小 3 個解凍層的修改幅度，以免因為過大的修改損害到這些表示法。<br />
</p>

<div class="org-src-container">
<pre class="src src-python" id="orgc5e6366"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32232;&#35695;&#27169;&#22411;</span>
<span class="linenr"> 2: </span>  model.<span style="color: #c678dd;">compile</span>(
<span class="linenr"> 3: </span>      loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr"> 4: </span>      optimizer=optimizers.RMSprop(lr=1e-<span style="color: #da8548; font-weight: bold;">5</span>),
<span class="linenr"> 5: </span>      metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;&#27169;&#22411;</span>
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">history</span> = model.fit_generator(   
<span class="linenr"> 9: </span>      train_generator,
<span class="linenr">10: </span>      steps_per_epoch=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr">11: </span>      epochs=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr">12: </span>      validation_data=validation_generator,
<span class="linenr">13: </span>      validation_steps=<span style="color: #da8548; font-weight: bold;">50</span>)
</pre>
</div>





<div class="org-src-container">
<pre class="src src-python" id="org44bbaf1"><span class="linenr">  1: </span>  <span style="color: #51afef;">import</span> os
<span class="linenr">  2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  3: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr">  4: </span>
<span class="linenr">  5: </span>  <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr">  6: </span>  <span style="color: #dcaeea;">train_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr">  7: </span>  <span style="color: #dcaeea;">validation_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)
<span class="linenr">  8: </span>  <span style="color: #dcaeea;">test_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'test'</span>)
<span class="linenr">  9: </span>
<span class="linenr"> 10: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#37096;&#20221;&#20941;&#32080;</span>
<span class="linenr"> 11: </span>
<span class="linenr"> 12: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 13: </span>
<span class="linenr"> 14: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,
<span class="linenr"> 15: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr"> 16: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 17: </span>  conv_base.summary()
<span class="linenr"> 18: </span>
<span class="linenr"> 19: </span>  conv_base.<span style="color: #dcaeea;">trainable</span> = <span style="color: #a9a1e1;">True</span> <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20808;&#35373;&#23450;&#25152;&#26377;layer&#37117;&#21487;&#35347;&#32244;?</span>
<span class="linenr"> 20: </span>  <span style="color: #dcaeea;">set_trainable</span> = <span style="color: #a9a1e1;">False</span> <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38928;&#35373;&#28858;&#20941;&#32080;</span>
<span class="linenr"> 21: </span>
<span class="linenr"> 22: </span>  <span style="color: #51afef;">for</span> layer <span style="color: #51afef;">in</span> conv_base.layers: <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#30001;&#20302;&#21040;&#39640;</span>
<span class="linenr"> 23: </span>      <span style="color: #51afef;">if</span> layer.name == <span style="color: #98be65;">'block5_conv1'</span>: <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#30452;&#21040;&#20986;&#29694;block5_conv1&#36889;&#23652;&#24460;&#38283;&#22987;&#35299;&#20941;</span>
<span class="linenr"> 24: </span>          <span style="color: #dcaeea;">set_trainable</span> = <span style="color: #a9a1e1;">True</span>
<span class="linenr"> 25: </span>      <span style="color: #51afef;">if</span> <span style="color: #dcaeea;">set_trainable</span>:
<span class="linenr"> 26: </span>          layer.trainable = <span style="color: #a9a1e1;">True</span>
<span class="linenr"> 27: </span>      <span style="color: #51afef;">else</span>:
<span class="linenr"> 28: </span>          layer.trainable = <span style="color: #a9a1e1;">False</span>
<span class="linenr"> 29: </span>
<span class="linenr"> 30: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">data augmentation</span>
<span class="linenr"> 31: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr"> 32: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr"> 33: </span>
<span class="linenr"> 34: </span>  <span style="color: #dcaeea;">train_datagen</span> = ImageDataGenerator( <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25844;&#20805;&#35347;&#32244;&#36039;&#26009;</span>
<span class="linenr"> 35: </span>    rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>,
<span class="linenr"> 36: </span>    rotation_range=<span style="color: #da8548; font-weight: bold;">40</span>,
<span class="linenr"> 37: </span>    width_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 38: </span>    height_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 39: </span>    shear_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 40: </span>    zoom_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 41: </span>    horizontal_flip=<span style="color: #a9a1e1;">True</span>,
<span class="linenr"> 42: </span>    fill_mode=<span style="color: #98be65;">'nearest'</span>)
<span class="linenr"> 43: </span>
<span class="linenr"> 44: </span>  <span style="color: #dcaeea;">test_datagen</span> = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35531;&#27880;&#24847;&#39511;&#35657;&#36039;&#26009;&#19981;&#25033;&#35442;&#25844;&#20805;</span>
<span class="linenr"> 45: </span>
<span class="linenr"> 46: </span>
<span class="linenr"> 47: </span>  <span style="color: #dcaeea;">train_generator</span> = train_datagen.flow_from_directory(
<span class="linenr"> 48: </span>    train_dir, <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30446;&#27161;&#30446;&#37636;&#36335;&#24465;</span>
<span class="linenr"> 49: </span>    target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>), <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35519;&#25972;&#25152;&#26377;&#22294;&#20687;&#22823;&#23567;&#25104; 150&#215;150 </span>
<span class="linenr"> 50: </span>    batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr"> 51: </span>    class_mode=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22240;&#28858;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109; binary_crossentropy &#20316;&#28858;&#25613;&#22833;&#20998;&#25976;&#65292;&#25152;                       &#20197;&#38656;&#35201;&#20108;&#20803;&#27161;&#31844;</span>
<span class="linenr"> 52: </span>
<span class="linenr"> 53: </span>  <span style="color: #dcaeea;">validation_generator</span> = test_datagen.flow_from_directory(
<span class="linenr"> 54: </span>    validation_dir,
<span class="linenr"> 55: </span>    target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),
<span class="linenr"> 56: </span>    batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr"> 57: </span>    class_mode=<span style="color: #98be65;">'binary'</span>)
<span class="linenr"> 58: </span>
<span class="linenr"> 59: </span>
<span class="linenr"> 60: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 61: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 62: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr"> 63: </span>
<span class="linenr"> 64: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model&#36996;&#26159;&#35201;&#21152;&#24460;&#38754;&#30340;layer?</span>
<span class="linenr"> 65: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr"> 66: </span>  model.add(conv_base)        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#21367;&#31309;&#22522;&#24213;&#35222;&#28858;&#23652;&#21152;&#20837; Sequential &#27169;&#22411;&#20013;</span>
<span class="linenr"> 67: </span>  model.add(layers.Flatten()) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25892;&#24179;</span>
<span class="linenr"> 68: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">256</span>, activation=<span style="color: #98be65;">'relu'</span>)) 
<span class="linenr"> 69: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>)) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22686;&#21152;&#23494;&#38598;&#23652;&#20998;&#39006;&#22120;</span>
<span class="linenr"> 70: </span>
<span class="linenr"> 71: </span>
<span class="linenr"> 72: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24494;&#35519;</span>
<span class="linenr"> 73: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32232;&#35695;&#27169;&#22411;</span>
<span class="linenr"> 74: </span>  model.<span style="color: #c678dd;">compile</span>(
<span class="linenr"> 75: </span>      loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr"> 76: </span>      optimizer=optimizers.RMSprop(lr=1e-<span style="color: #da8548; font-weight: bold;">5</span>),
<span class="linenr"> 77: </span>      metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr"> 78: </span>
<span class="linenr"> 79: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;&#27169;&#22411;</span>
<span class="linenr"> 80: </span>  <span style="color: #dcaeea;">history</span> = model.fit_generator(   
<span class="linenr"> 81: </span>      train_generator,
<span class="linenr"> 82: </span>      steps_per_epoch=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr"> 83: </span>      epochs=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr"> 84: </span>      validation_data=validation_generator,
<span class="linenr"> 85: </span>      validation_steps=<span style="color: #da8548; font-weight: bold;">50</span>)
<span class="linenr"> 86: </span>
<span class="linenr"> 87: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32362;&#35069;model&#30340;&#25613;&#22833;&#29575;&#33287;&#31934;&#30906;&#29575;</span>
<span class="linenr"> 88: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 89: </span>
<span class="linenr"> 90: </span>  <span style="color: #dcaeea;">acc</span> = history.history[<span style="color: #98be65;">'acc'</span>]
<span class="linenr"> 91: </span>  <span style="color: #dcaeea;">val_acc</span> = history.history[<span style="color: #98be65;">'val_acc'</span>]
<span class="linenr"> 92: </span>  <span style="color: #dcaeea;">loss</span> = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr"> 93: </span>  <span style="color: #dcaeea;">val_loss</span> = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr"> 94: </span>
<span class="linenr"> 95: </span>  <span style="color: #dcaeea;">epochs</span> = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(acc) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 96: </span>  plt.clf()
<span class="linenr"> 97: </span>  plt.plot(epochs, acc, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr"> 98: </span>  plt.plot(epochs, val_acc, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr"> 99: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">100: </span>  plt.legend()
<span class="linenr">101: </span>  plt.plot()
<span class="linenr">102: </span>  plt.savefig(<span style="color: #98be65;">"FineTune-acc-1.png"</span>)
<span class="linenr">103: </span>  plt.figure()
<span class="linenr">104: </span>
<span class="linenr">105: </span>  plt.clf()
<span class="linenr">106: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">107: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">108: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">109: </span>  plt.legend()
<span class="linenr">110: </span>  plt.plot()
<span class="linenr">111: </span>  plt.savefig(<span style="color: #98be65;">"FineTune-loss-1.png"</span>)
</pre>
</div>

<pre class="example">
Model: "vgg16"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 150, 150, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
100/100 [==============================] - 602s 6s/step - loss: 0.0200 - acc: 0.9935 - val_loss: 0.0559 - val_acc: 0.9380
</pre>


<div id="org38fb752" class="figure">
<p><img src="images/FineTune-acc-1.png" alt="FineTune-acc-1.png" /><br />
</p>
<p><span class="figure-number">Figure 79: </span>VGG16 Fine Tune Acc</p>
</div>


<div id="org63225cc" class="figure">
<p><img src="images/FineTune-loss-1.png" alt="FineTune-loss-1.png" /><br />
</p>
<p><span class="figure-number">Figure 80: </span>VGG16 Fine Tune Loss</p>
</div>

<p>
微調的訓練準確率來到 99%，驗證準確率也有 94%，這是使用 2000 個訓練樣本就達到的結果。<br />
</p>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-org097008e" class="outline-2">
<h2 id="org097008e"><span class="section-number-2">12</span> 視覺化呈現 CNN 的學習內容</h2>
<div class="outline-text-2" id="text-12">
<p>
CNN 學習的表示法非常適合以視覺化呈現，因為它們大部份就是視覺概念的表示法(represnetations of visual concepts)，幾種常用的視得化技術如下：<br />
</p>
<ul class="org-ul">
<li>視覺化中間層 convnet 的輸出(中間啟動函數)：有助於理解 convnet 是如何一層一層的轉化資料，以及對過濾器(filter)的含義。<br /></li>
<li>視覺化 CNN 過濾器：用於準確理解 CNN 中每個過濾器所要接受的視覺 patter 或概念中<br /></li>
<li>視覺化類別激活熱圖(heatmaps of class activation): 有助於了解影像的哪些部份被識別為某個類別，藉以定位影像中的物件。<br /></li>
</ul>
</div>

<div id="outline-container-orgf0bd8ff" class="outline-3">
<h3 id="orgf0bd8ff"><span class="section-number-3">12.1</span> 中間層輸出視覺化</h3>
<div class="outline-text-3" id="text-12-1">
<p>
這部份工作主要是在給定輸入影像後，顯示 convnet 各個卷積層和池化層輸出的特徵層。主要是讓我們能看到在 convnet 的學習過程中，輸入資料是如何經由逐層分解到不同的過濾器，雖然輸入資料為三個維度(width, height, channel)，但其實每個 channel 會針對相對獨立的 feature 進行編碼，所以此處是將每個 channel 的內容獨立繪製成 2D 圖形秀出，此即響應圖(比喻為將吐司切片。<br />
</p>

<p>
以下先載入之前儲存好的 model，取一張測試集中的照片(未經訓練過)，秀出原始內容，然後萃取出特徵圖，因為我們只要看一張圖，所以要建新一個新的 Keras model。<br />
</p>

<div class="org-src-container">
<pre class="src src-python" id="org65bfd9a"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35712;&#20986;&#20043;&#21069;&#23384;&#30340;model</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> load_model
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">model</span> = load_model(<span style="color: #98be65;">'cats_and_dogs_small_data_augmentation.h5'</span>)
<span class="linenr"> 4: </span>  model.summary()
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38928;&#34389;&#29702;&#21934;&#24373;&#29031;&#29255;</span>
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">img_path</span> = <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small/test/cats/cat.1556.jpg'</span>
<span class="linenr"> 8: </span>  <span style="color: #51afef;">from</span> keras.preprocessing <span style="color: #51afef;">import</span> image
<span class="linenr"> 9: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #dcaeea;">img</span> = image.load_img(img_path, target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr">12: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20197;&#19979;&#23559;&#36889;&#24373;&#29031;&#29255;&#38928;&#34389;&#29702;&#25104;4D&#24373;&#37327;&#65292;&#23559;&#20540;&#38480;&#21046;&#21040;0~1&#38291;</span>
<span class="linenr">13: </span>  <span style="color: #dcaeea;">img_tensor</span> = image.img_to_array(img)
<span class="linenr">14: </span>  <span style="color: #dcaeea;">img_tensor</span> = np.expand_dims(img_tensor, axis = <span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">15: </span>  <span style="color: #dcaeea;">img_tensor</span> /= <span style="color: #da8548; font-weight: bold;">255</span>.
<span class="linenr">16: </span>  <span style="color: #51afef;">print</span>(img_tensor.shape)
<span class="linenr">17: </span>
<span class="linenr">18: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#39023;&#31034;&#21407;&#22294;</span>
<span class="linenr">19: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">20: </span>  plt.imshow(img_tensor[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">21: </span>  plt.plot()
<span class="linenr">22: </span>  plt.savefig(<span style="color: #98be65;">"origin.cat.png"</span>)
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#29992;&#19968;&#20491;&#36664;&#20837;&#24373;&#37327;&#21644;&#19968;&#20491;&#36664;&#20986;&#24373;&#37327;list&#20358;&#24314;&#19968;&#20491;&#26032;model</span>
<span class="linenr">25: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">26: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#33795;&#21462;model&#30340;&#21069;8&#23652;&#36664;&#20986;&#24373;&#37327;</span>
<span class="linenr">27: </span>  <span style="color: #dcaeea;">layer_outputs</span> = [layer.output <span style="color: #51afef;">for</span> layer <span style="color: #51afef;">in</span> model.layers[:<span style="color: #da8548; font-weight: bold;">8</span>]]
<span class="linenr">28: </span>  <span style="color: #51afef;">for</span> op <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">layer_outputs</span>:
<span class="linenr">29: </span>      <span style="color: #51afef;">print</span>(op)
<span class="linenr">30: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#22312;&#32102;&#23450;&#36664;&#20837;&#24373;&#37327;&#30340;&#26781;&#20214;&#19979;&#65292;&#24314;&#31435;&#26371;&#29986;&#29983;&#36889;&#20123;&#36664;&#20986;&#30340;model</span>
<span class="linenr">31: </span>  activation_model = models.Model(inputs=model.<span style="color: #c678dd;">input</span>, outputs=layer_outputs)
<span class="linenr">32: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#30070;&#39221;&#20837;&#19968;&#24373;&#24433;&#20687;&#24460;&#65292;&#27492;model&#26371;&#36664;&#20986;&#21407;model&#20013;&#21069;8&#23652;&#21855;&#21205;&#20989;&#25976;&#30340;&#20540;</span>
<span class="linenr">33: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#25925;&#27492;&#34389;&#26371;&#20659;&#22238;&#19968;&#20491;&#21547;8&#20491;&#36664;&#20986;&#24373;&#37327;&#30340;list&#65292;&#21363;8&#20491;layer&#30340;&#21855;&#21205;&#20989;&#25976;&#36664;&#20986;&#20540;</span>
<span class="linenr">34: </span>  <span style="color: #dcaeea;">activations</span> = activation_model.predict(img_tensor)
<span class="linenr">35: </span>  <span style="color: #51afef;">print</span>(<span style="color: #c678dd;">len</span>(activations))
<span class="linenr">36: </span>
<span class="linenr">37: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20363;&#22914;&#65292;&#22312;&#39221;&#20837;&#24433;&#20687;&#24460;&#65292;&#31532;&#19968;&#20491;&#21367;&#31309;&#23652;(index=0)&#30340;&#21855;&#21205;&#20989;&#25976;&#24373;&#37327;&#28858;</span>
<span class="linenr">38: </span>  <span style="color: #dcaeea;">first_layer_activation</span> = activations[<span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr">39: </span>  <span style="color: #51afef;">print</span>(first_layer_activation.shape)
<span class="linenr">40: </span>
<span class="linenr">41: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#32362;&#20986;&#31532;4&#20491;channel&#30340;&#38911;&#25033;&#22294;</span>
<span class="linenr">42: </span>  plt.matshow(first_layer_activation[<span style="color: #da8548; font-weight: bold;">0</span>, :, :, <span style="color: #da8548; font-weight: bold;">4</span>], cmap=<span style="color: #98be65;">'viridis'</span>)
<span class="linenr">43: </span>  plt.plot()
<span class="linenr">44: </span>  plt.savefig(<span style="color: #98be65;">"origin_channel_4_map.png"</span>)
<span class="linenr">45: </span>
<span class="linenr">46: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#32362;&#20986;&#31532;4&#20491;channel&#30340;&#38911;&#25033;&#22294;</span>
<span class="linenr">47: </span>  plt.matshow(first_layer_activation[<span style="color: #da8548; font-weight: bold;">0</span>, :, :, <span style="color: #da8548; font-weight: bold;">7</span>], cmap=<span style="color: #98be65;">'viridis'</span>)
<span class="linenr">48: </span>  plt.plot()
<span class="linenr">49: </span>  plt.savefig(<span style="color: #98be65;">"origin_channel_7_map.png"</span>)
</pre>
</div>

<pre class="example">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 6272)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 6272)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               3211776   
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513       
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
(1, 150, 150, 3)
Tensor("conv2d_1/Relu:0", shape=(?, 148, 148, 32), dtype=float32)
Tensor("max_pooling2d_1/MaxPool:0", shape=(?, 74, 74, 32), dtype=float32)
Tensor("conv2d_2/Relu:0", shape=(?, 72, 72, 64), dtype=float32)
Tensor("max_pooling2d_2/MaxPool:0", shape=(?, 36, 36, 64), dtype=float32)
Tensor("conv2d_3/Relu:0", shape=(?, 34, 34, 128), dtype=float32)
Tensor("max_pooling2d_3/MaxPool:0", shape=(?, 17, 17, 128), dtype=float32)
Tensor("conv2d_4/Relu:0", shape=(?, 15, 15, 128), dtype=float32)
Tensor("max_pooling2d_4/MaxPool:0", shape=(?, 7, 7, 128), dtype=float32)
8
(1, 148, 148, 32)
</pre>

<p>
由上述結果可看出 firtst_layer_activation 的輸出為一個 148&times;148 的特徵圖，共有 32 個 channel，即原 model 中第一層的 conv2d 的輸出內容。其中第 1556 張原圖如下：<br />
</p>


<div id="orgd7afed8" class="figure">
<p><img src="images/origin.cat.png" alt="origin.cat.png" /><br />
</p>
<p><span class="figure-number">Figure 81: </span>Origin cat</p>
</div>

<p>
其中第 4 個 channel 似乎是編碼成一個邊緣特徵偵測器，如圖<a href="#org6ef486d">82</a>。<br />
</p>


<div id="org6ef486d" class="figure">
<p><img src="images/origin_channel_4_map.png" alt="origin_channel_4_map.png" /><br />
</p>
<p><span class="figure-number">Figure 82: </span>Effect of channel 4</p>
</div>

<p>
而第 7 個 channel 看起來則像個亮綠點偵測器，可以用來編碼貓眼特徵。<br />
</p>


<div id="orgc2e5417" class="figure">
<p><img src="images/origin_channel_7_map.png" alt="origin_channel_7_map.png" /><br />
</p>
<p><span class="figure-number">Figure 83: </span>Effect of channel 7</p>
</div>

<p>
如果想看出神經網路中所有啟動函數輸出的完整圖形、每個 channel 的響應圖，其相對應程式碼如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35712;&#20986;&#20043;&#21069;&#23384;&#30340;model</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> load_model
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">model</span> = load_model(<span style="color: #98be65;">'cats_and_dogs_small_data_augmentation.h5'</span>)
<span class="linenr"> 4: </span>  model.summary()
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38928;&#34389;&#29702;&#21934;&#24373;&#29031;&#29255;</span>
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">img_path</span> = <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small/test/cats/cat.1556.jpg'</span>
<span class="linenr"> 8: </span>  <span style="color: #51afef;">from</span> keras.preprocessing <span style="color: #51afef;">import</span> image
<span class="linenr"> 9: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #dcaeea;">img</span> = image.load_img(img_path, target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr">12: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20197;&#19979;&#23559;&#36889;&#24373;&#29031;&#29255;&#38928;&#34389;&#29702;&#25104;4D&#24373;&#37327;&#65292;&#23559;&#20540;&#38480;&#21046;&#21040;0~1&#38291;</span>
<span class="linenr">13: </span>  <span style="color: #dcaeea;">img_tensor</span> = image.img_to_array(img)
<span class="linenr">14: </span>  <span style="color: #dcaeea;">img_tensor</span> = np.expand_dims(img_tensor, axis = <span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">15: </span>  <span style="color: #dcaeea;">img_tensor</span> /= <span style="color: #da8548; font-weight: bold;">255</span>.
<span class="linenr">16: </span>  <span style="color: #51afef;">print</span>(img_tensor.shape)
<span class="linenr">17: </span>
<span class="linenr">18: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#39023;&#31034;&#21407;&#22294;</span>
<span class="linenr">19: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">20: </span>  plt.imshow(img_tensor[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">21: </span>  plt.plot()
<span class="linenr">22: </span>  plt.savefig(<span style="color: #98be65;">"origin.cat.png"</span>)
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#29992;&#19968;&#20491;&#36664;&#20837;&#24373;&#37327;&#21644;&#19968;&#20491;&#36664;&#20986;&#24373;&#37327;list&#20358;&#24314;&#19968;&#20491;&#26032;model</span>
<span class="linenr">25: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">26: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#33795;&#21462;model&#30340;&#21069;8&#23652;&#36664;&#20986;&#24373;&#37327;</span>
<span class="linenr">27: </span>  <span style="color: #dcaeea;">layer_outputs</span> = [layer.output <span style="color: #51afef;">for</span> layer <span style="color: #51afef;">in</span> model.layers[:<span style="color: #da8548; font-weight: bold;">8</span>]]
<span class="linenr">28: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#22312;&#32102;&#23450;&#36664;&#20837;&#24373;&#37327;&#30340;&#26781;&#20214;&#19979;&#65292;&#24314;&#31435;&#26371;&#29986;&#29983;&#36889;&#20123;&#36664;&#20986;&#30340;model</span>
<span class="linenr">29: </span>  <span style="color: #dcaeea;">activation_model</span> = models.Model(inputs=model.<span style="color: #c678dd;">input</span>, outputs=layer_outputs)
<span class="linenr">30: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#30070;&#39221;&#20837;&#19968;&#24373;&#24433;&#20687;&#24460;&#65292;&#27492;model&#26371;&#36664;&#20986;&#21407;model&#20013;&#21069;8&#23652;&#21855;&#21205;&#20989;&#25976;&#30340;&#20540;</span>
<span class="linenr">31: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#25925;&#27492;&#34389;&#26371;&#20659;&#22238;&#19968;&#20491;&#21547;8&#20491;&#36664;&#20986;&#24373;&#37327;&#30340;list&#65292;&#21363;8&#20491;layer&#30340;&#21855;&#21205;&#20989;&#25976;&#36664;&#20986;&#20540;</span>
<span class="linenr">32: </span>  <span style="color: #dcaeea;">activations</span> = activation_model.predict(img_tensor)
<span class="linenr">33: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20363;&#22914;&#65292;&#22312;&#39221;&#20837;&#24433;&#20687;&#24460;&#65292;&#31532;&#19968;&#20491;&#21367;&#31309;&#23652;(index=0)&#30340;&#21855;&#21205;&#20989;&#25976;&#24373;&#37327;&#28858;</span>
<span class="linenr">34: </span>  <span style="color: #dcaeea;">first_layer_activation</span> = activations[<span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr">35: </span>
<span class="linenr">36: </span>  <span style="color: #5B6268;">###</span><span style="color: #5B6268;">=========================</span>
<span class="linenr">37: </span>  <span style="color: #dcaeea;">layer_names</span> = []
<span class="linenr">38: </span>
<span class="linenr">39: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21462;&#24471;&#21508;&#23652;&#30340;&#21517;&#23383;&#65292;&#36889;&#27171;&#25165;&#21487;&#20197;&#25104;&#28858;&#22294;&#34920;&#30340;&#19968;&#37096;&#20998;</span>
<span class="linenr">40: </span>  <span style="color: #51afef;">for</span> layer <span style="color: #51afef;">in</span> model.<span style="color: #dcaeea;">layers</span>[:<span style="color: #da8548; font-weight: bold;">8</span>]:
<span class="linenr">41: </span>      layer_names.append(layer.name)
<span class="linenr">42: </span>
<span class="linenr">43: </span>  images_per_row = <span style="color: #da8548; font-weight: bold;">16</span>
<span class="linenr">44: </span>
<span class="linenr">45: </span>  <span style="color: #51afef;">for</span> layer_name, layer_activation <span style="color: #51afef;">in</span> <span style="color: #c678dd;">zip</span>(layer_names, activations):
<span class="linenr">46: </span>      <span style="color: #dcaeea;">n_features</span> = layer_activation.shape[-<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr">47: </span>      <span style="color: #dcaeea;">size</span> = layer_activation.shape[<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr">48: </span>
<span class="linenr">49: </span>      <span style="color: #dcaeea;">n_cols</span> = n_features // images_per_row
<span class="linenr">50: </span>      <span style="color: #dcaeea;">display_grid</span> = np.zeros((size * n_cols, images_per_row * size))
<span class="linenr">51: </span>
<span class="linenr">52: </span>      <span style="color: #51afef;">for</span> col <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(n_cols):
<span class="linenr">53: </span>          <span style="color: #51afef;">for</span> row <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(images_per_row):
<span class="linenr">54: </span>              <span style="color: #dcaeea;">channel_image</span> = layer_activation[<span style="color: #da8548; font-weight: bold;">0</span>, :, :, col * images_per_row + row]
<span class="linenr">55: </span>              <span style="color: #dcaeea;">channel_image</span> -= channel_image.mean()
<span class="linenr">56: </span>              <span style="color: #dcaeea;">channel_image</span> /= channel_image.std()
<span class="linenr">57: </span>              <span style="color: #dcaeea;">channel_image</span> *= <span style="color: #da8548; font-weight: bold;">64</span>
<span class="linenr">58: </span>              <span style="color: #dcaeea;">channel_image</span> += <span style="color: #da8548; font-weight: bold;">128</span>
<span class="linenr">59: </span>              <span style="color: #dcaeea;">channel_image</span> = np.clip(channel_image, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">255</span>).astype(<span style="color: #98be65;">'uint8'</span>)
<span class="linenr">60: </span>              <span style="color: #dcaeea;">display_grid</span>[col * size : (col + <span style="color: #da8548; font-weight: bold;">1</span>) * size,
<span class="linenr">61: </span>                           row * size : (row + <span style="color: #da8548; font-weight: bold;">1</span>) * size] = channel_image
<span class="linenr">62: </span>
<span class="linenr">63: </span>      <span style="color: #dcaeea;">scale</span> = <span style="color: #da8548; font-weight: bold;">1</span>. / size
<span class="linenr">64: </span>      plt.figure(figsize=(scale * display_grid.shape[<span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">65: </span>      scale * display_grid.shape[<span style="color: #da8548; font-weight: bold;">0</span>]))
<span class="linenr">66: </span>      plt.title(layer_name)
<span class="linenr">67: </span>      plt.grid(<span style="color: #a9a1e1;">False</span>)
<span class="linenr">68: </span>      plt.imshow(display_grid, aspect=<span style="color: #98be65;">'auto'</span>, cmap=<span style="color: #98be65;">'viridis'</span>)
<span class="linenr">69: </span>      plt.plot()
<span class="linenr">70: </span>      plt.savefig(layer_name+<span style="color: #98be65;">".png"</span>)
<span class="linenr">71: </span>      plt.clf()
</pre>
</div>

<pre class="example">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 6272)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 6272)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               3211776   
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513       
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
(1, 150, 150, 3)
</pre>


<div id="org50350ab" class="figure">
<p><img src="images/conv2d_1.png" alt="conv2d_1.png" /><br />
</p>
<p><span class="figure-number">Figure 84: </span>conv2d_1.png</p>
</div>


<div id="org9c74ba0" class="figure">
<p><img src="images/max_pooling2d_1.png" alt="max_pooling2d_1.png" /><br />
</p>
<p><span class="figure-number">Figure 85: </span>max_pooling2d_1.png</p>
</div>


<div id="orge5fa4b8" class="figure">
<p><img src="images/conv2d_2.png" alt="conv2d_2.png" /><br />
</p>
<p><span class="figure-number">Figure 86: </span>conv2d_2.png</p>
</div>


<div id="org27bae72" class="figure">
<p><img src="images/max_pooling2d_2.png" alt="max_pooling2d_2.png" /><br />
</p>
<p><span class="figure-number">Figure 87: </span>max_pooling2d_2.png</p>
</div>


<div id="orgb5eff3d" class="figure">
<p><img src="images/conv2d_3.png" alt="conv2d_3.png" /><br />
</p>
<p><span class="figure-number">Figure 88: </span>conv2d_3.png</p>
</div>


<div id="org747fb94" class="figure">
<p><img src="images/max_pooling2d_3.png" alt="max_pooling2d_3.png" /><br />
</p>
<p><span class="figure-number">Figure 89: </span>max_pooling2d_3.png</p>
</div>


<div id="orgcdbd68d" class="figure">
<p><img src="images/conv2d_4.png" alt="conv2d_4.png" /><br />
</p>
<p><span class="figure-number">Figure 90: </span>conv2d_4.png</p>
</div>


<div id="org996f35e" class="figure">
<p><img src="images/max_pooling2d_4.png" alt="max_pooling2d_4.png" /><br />
</p>
<p><span class="figure-number">Figure 91: </span>max_pooling2d_4.png</p>
</div>

<p>
由上圖可知，隨層數越來越高，啟動函數的輸出變得越來越抽象，視覺上也越來越難解釋，model 開始編碼出更高階的概念。此外，啟動函數輸出的稀疏性也隨著層數的深度而增加，在第一層中，所有的過濾器都被輸入影響所驅動(都有值)，但接下來就有越來越多的 filter 的值是空的(全黑)，這表示在這些層的輸入影像中已經找不到過濾器要編碼的圖案 pattern 了。<br />
</p>

<p>
上述示例也證明了深度神經網路所學習到的表示法有一個重要特性：各層萃取的特徵隨著層的𣶶度而變的越來越抽象，越高階的啟動函數越不會帶有關於特定輸入的資訊，卻具備更多關於目標的資訊（此例中指的是貓或狗）。這和人或動物感知世界的方式很像：在觀察一個場景幾秒中後閉眼，我們可以記得場景中有哪些抽象事物，但不會記得每個物體的特殊外觀，因為大腦也會將事物抽象化。<br />
</p>
</div>
</div>

<div id="outline-container-org797b1ff" class="outline-3">
<h3 id="org797b1ff"><span class="section-number-3">12.2</span> 視覺化 convnet 的 filter</h3>
<div class="outline-text-3" id="text-12-2">
<p>
另一種視覺化的方法：convnet 是去看各 filter 要過濾的視覺化圖案(visual pattern)。我們可以先餵給 convnet 一張空白的影像，然後將梯度下降法套用到 convnet 上，一直到所指定的層、所指定的 filter 對輸入影像的響應達到最大化，如此所得到的輸入影像就是讓該 filter 產生最大化響的影像，也就是 filter 的長相。方法如下：<br />
</p>

<ol class="org-ol">
<li>先建立一個損失函數，讓 convnet 指定層中指定的濾波器的啟動函數輸出最大化<br /></li>
<li>使用隨機梯度下降(SGD)來調整輸入影像像素值，以便最大化這個啟動函數輸出值<br /></li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> backend <span style="color: #51afef;">as</span> K <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#36617;&#20837;Keras&#30340;&#24460;&#31471;&#65292;&#29992;&#20358;&#25805;&#20316;&#24373;&#37327;(&#21462;&#24179;&#22343;)</span>
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">model</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,
<span class="linenr"> 5: </span>                include_top=<span style="color: #a9a1e1;">False</span>)
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">layer_name</span> = <span style="color: #98be65;">'block3_conv1'</span>
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">filter_index</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">layer_output</span> = model.get_layer(layer_name).output 
<span class="linenr">11: </span>  <span style="color: #dcaeea;">loss</span> = K.mean(layer_output[:, :, :, filter_index]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23450;&#32681;&#25613;&#22833;&#20989;&#25976;&#24373;&#37327;, &#20854;&#28858;&#23652;&#36664;&#20986;&#24373;&#37327;&#25976;&#20540;&#21462;&#24179;&#22343;</span>
<span class="linenr">12: </span>
<span class="linenr">13: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#24471;&#21040;&#25613;&#22833;&#20989;&#25976;&#24373;&#37327;&#24460;&#65292;&#25509;&#33879;&#20351;&#29992;backend&#27169;&#32068;gradients()&#20989;&#25976;&#20358;&#21462;&#24471;&#25613;&#22833;&#20989;&#25976;&#24373;&#37327;&#30456;&#23565;&#26044;model&#36664;&#20837;&#30340;&#26799;&#24230;</span>
<span class="linenr">14: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">gradients() &#26371;&#20659;&#22238;&#19968;&#20491;&#30001;&#24373;&#37327;&#32068;&#25104;&#30340;list, &#22312;&#26412;&#20363;&#20013;, list &#30340;&#22823;&#23567;&#28858; 1, &#22240;&#27492;, &#21482;&#21462;&#20986;&#20854;&#31532; 0 &#20491;&#20803;&#32032;, &#21363; grads &#26159; 1 &#20491;&#26799;&#24230;&#24373;&#37327;</span>
<span class="linenr">15: </span>  <span style="color: #dcaeea;">grads</span> = K.gradients(loss, model.<span style="color: #c678dd;">input</span>)[<span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr">16: </span>
<span class="linenr">17: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#28858;&#30906;&#20445;&#26799;&#24230;&#19979;&#38477;&#36942;&#31243;&#38918;&#21033;&#65292;&#27492;&#34389;&#25226;&#26799;&#24230;&#24373;&#37327;&#38500;&#20197;&#20854;L2 norm&#65292;&#36889;&#26159;&#28858;&#20102;&#35731;&#26356;&#26032;&#24133;&#24230;&#38480;&#21046;&#22312;&#35215;&#31684;&#30340;&#31684;&#22285;&#20839;</span>
<span class="linenr">18: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22312;&#20570;&#38500;&#27861;&#20043;&#21069;&#20808;&#21152;&#19978; 1e-5 &#20197;&#36991;&#20813;&#24847;&#22806;&#22320;&#38500;&#20197; 0</span>
<span class="linenr">19: </span>  <span style="color: #dcaeea;">grads</span> /= (K.sqrt(K.mean(K.square(grads))) + 1e-<span style="color: #da8548; font-weight: bold;">5</span>)
<span class="linenr">20: </span>
<span class="linenr">21: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23450;&#32681;&#19968;&#20491;Keras&#24460;&#31471;&#31243;&#24335;&#20358;&#35336;&#31639;&#25613;&#22833;&#24373;&#37327;&#21644;&#26799;&#24230;&#24373;&#37327;&#30340;&#25976;&#20540;:iterate</span>
<span class="linenr">22: </span>  iterate = K.function([model.<span style="color: #c678dd;">input</span>], [loss, grads]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23450;&#32681;&#19968;&#20491; Keras &#24460;&#31471;&#20989;&#24335;</span>
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">25: </span>  <span style="color: #dcaeea;">loss_value</span>, <span style="color: #dcaeea;">grads_value</span> = iterate([np.zeros((<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))])
<span class="linenr">26: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#36664;&#20986;&#25613;&#22833;&#24373;&#37327;&#33287;&#26799;&#24230;&#24373;&#37327;</span>
<span class="linenr">27: </span>
<span class="linenr">28: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23450;&#32681;&#19968;&#20491;&#36852;&#22280;&#20358;&#36914;&#34892;SGD</span>
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24478;&#24118;&#26377;&#38620;&#35338;&#30340;&#28784;&#38542;&#22294;&#20687;&#38283;&#22987;</span>
<span class="linenr">30: </span>  <span style="color: #dcaeea;">input_img_data</span> = np.random.random((<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>)) * <span style="color: #da8548; font-weight: bold;">20</span> + <span style="color: #da8548; font-weight: bold;">128</span>. <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1...</span>
<span class="linenr">31: </span>
<span class="linenr">32: </span>  <span style="color: #dcaeea;">step</span> = <span style="color: #da8548; font-weight: bold;">1</span>. <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27599;&#20491;&#26799;&#24230;&#26356;&#26032;&#30340;&#22823;&#23567;</span>
<span class="linenr">33: </span>  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">40</span>): <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22519;&#34892;&#26799;&#24230;&#19978;&#21319; 40 &#27493;</span>
<span class="linenr">34: </span>    <span style="color: #dcaeea;">loss_value</span>, <span style="color: #dcaeea;">grads_value</span> = iterate([input_img_data]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;&#25613;&#22833;&#20540;&#21644;&#26799;&#24230;&#20540;</span>
<span class="linenr">35: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">2. &#20197;&#26397;&#21521;&#26368;&#22823;&#21270;&#25613;&#22833;&#35519;&#25972;&#36664;&#20837;&#22294;&#20687; (&#20197;&#21069;SGD &#26159;&#29992; -= &#31639;&#31526;, &#29694;&#22312;&#21453;&#36942;&#20358;&#26159;&#29992; += &#31639;&#31526;)</span>
<span class="linenr">36: </span>    <span style="color: #dcaeea;">input_img_data</span> += grads_value * step 
<span class="linenr">37: </span>
<span class="linenr">38: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#24471;&#21040;&#30340;&#22294;&#20687;&#24373;&#37327;&#26159;shape&#28858;(1, 150, 150, 3)&#30340;&#28014;&#40670;&#24373;&#37327;&#65292;&#23565;&#20043;&#36914;&#34892;&#24460;&#34389;&#29702;&#36681;&#28858;&#21487;&#39023;&#31034;&#26684;&#24335;</span>
<span class="linenr">39: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">deprocess_image</span>(x):
<span class="linenr">40: </span>      <span style="color: #dcaeea;">x</span> -= x.mean()
<span class="linenr">41: </span>      <span style="color: #dcaeea;">x</span> /= (x.std() + 1e-<span style="color: #da8548; font-weight: bold;">5</span>)             <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1. &#24373;&#37327;&#27491;&#35215;&#21270;&#65306;&#20197; 0 &#28858;&#20013;&#24515;, &#30906;&#20445; std &#28858; 0.1 </span>
<span class="linenr">42: </span>      <span style="color: #dcaeea;">x</span> *= <span style="color: #da8548; font-weight: bold;">0.1</span>
<span class="linenr">43: </span>      <span style="color: #dcaeea;">x</span> += <span style="color: #da8548; font-weight: bold;">0.5</span>
<span class="linenr">44: </span>      <span style="color: #dcaeea;">x</span> = np.clip(x, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20462;&#27491;&#25104; [0, 1], &#21363; 0-1 &#20043;&#38291; </span>
<span class="linenr">45: </span>      <span style="color: #dcaeea;">x</span> *= <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">46: </span>      <span style="color: #dcaeea;">x</span> = np.clip(x, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">255</span>).astype(<span style="color: #98be65;">'uint8'</span>)        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">2.&#36681;&#25563;&#25104; RGB &#38499;&#21015;</span>
<span class="linenr">47: </span>      <span style="color: #51afef;">return</span> x
<span class="linenr">48: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">49: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23559;&#23652;&#30340;&#21517;&#31281;&#21644;&#36942;&#28670;&#22120;&#32034;&#24341;&#20316;&#28858;&#36664;&#20837;&#21443;&#25976;&#65292;&#22238;&#20659;&#26377;&#25928;&#24433;&#20687;&#24373;&#37327;</span>
<span class="linenr">50: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">generate_pattern</span>(layer_name, filter_index, size=<span style="color: #da8548; font-weight: bold;">150</span>):
<span class="linenr">51: </span>    <span style="color: #dcaeea;">layer_output</span> = model.get_layer(layer_name).output <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21462;&#24471;&#25351;&#23450;&#23652;&#30340;&#36664;&#20986;&#24373;&#37327;</span>
<span class="linenr">52: </span>    <span style="color: #dcaeea;">loss</span> = K.mean(layer_output[:, :, :, filter_index]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1. &#21462;&#24471;&#25351;&#23450;&#36942;&#28670;&#22120;&#30340;&#36664;&#20986;&#24373;&#37327;, &#20006;&#20197;&#26368;&#22823;&#21270;&#27492;&#24373;&#37327;&#30340;&#22343;&#20540;&#20570;&#28858;&#25613;&#22833;</span>
<span class="linenr">53: </span>    <span style="color: #dcaeea;">grads</span> = K.gradients(loss, model.<span style="color: #c678dd;">input</span>)[<span style="color: #da8548; font-weight: bold;">0</span>] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26681;&#25818;&#27492;&#25613;&#22833;&#35336;&#31639;&#36664;&#20837;&#24433;&#20687;&#30340;&#26799;&#24230;</span>
<span class="linenr">54: </span>    <span style="color: #dcaeea;">grads</span> /= (K.sqrt(K.mean(K.square(grads))) + 1e-<span style="color: #da8548; font-weight: bold;">5</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27161;&#28310;&#21270;&#25216;&#24039;&#65306;&#26799;&#24230;&#27161;&#28310;&#21270;</span>
<span class="linenr">55: </span>    <span style="color: #dcaeea;">iterate</span> = K.function([model.<span style="color: #c678dd;">input</span>], [loss, grads]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">2.&#24314;&#31435; Keras function &#20358;&#37341;&#23565;&#32102;&#23450;&#30340;&#36664;&#20837;&#24433;&#20687;&#22238;&#20659;&#25613;&#22833;&#21644;&#26799;&#24230;</span>
<span class="linenr">56: </span>    <span style="color: #dcaeea;">input_img_data</span> = np.random.random((<span style="color: #da8548; font-weight: bold;">1</span>, size, size, <span style="color: #da8548; font-weight: bold;">3</span>)) * <span style="color: #da8548; font-weight: bold;">20</span> + <span style="color: #da8548; font-weight: bold;">128</span>. <span style="color: #5B6268;"># </span><span style="color: #5B6268;">3. &#24478;&#24118;&#26377;&#38620;&#35338;&#30340;&#28784;&#38542;&#24433;&#20687;&#38283;&#22987;</span>
<span class="linenr">57: </span>
<span class="linenr">58: </span>    <span style="color: #dcaeea;">step</span> = <span style="color: #da8548; font-weight: bold;">1</span>.
<span class="linenr">59: </span>    <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">40</span>): <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22519;&#34892;&#26799;&#24230;&#19978;&#21319; 40 &#27493;</span>
<span class="linenr">60: </span>      <span style="color: #dcaeea;">loss_value</span>, <span style="color: #dcaeea;">grads_value</span> = iterate([input_img_data]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">4. &#37341;&#23565;&#32102;&#23450;&#30340;&#36664;&#20837;&#24433;&#20687;&#22238;&#20659;&#25613;&#22833;&#21644;&#26799;&#24230;</span>
<span class="linenr">61: </span>      <span style="color: #dcaeea;">input_img_data</span> += grads_value * step
<span class="linenr">62: </span>    <span style="color: #dcaeea;">img</span> = input_img_data[<span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr">63: </span>    <span style="color: #51afef;">return</span> deprocess_image(img)   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36914;&#34892;&#22294;&#20687;&#24460;&#34389;&#29702;&#24460;&#22238;&#20659;</span>
<span class="linenr">64: </span>
<span class="linenr">65: </span>  plt.imshow(generate_pattern(<span style="color: #98be65;">'block3_conv1'</span>, <span style="color: #da8548; font-weight: bold;">0</span>)) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25105;&#20497;&#20358;&#30475;&#30475; block3_conv1 &#23652;&#20013;&#30340;&#36942;&#28670;&#22120; 0 &#30340;&#29305;&#24501;&#22294;</span>
<span class="linenr">66: </span>  plt.plot()
<span class="linenr">67: </span>  plt.savefig(<span style="color: #98be65;">"block3_conv1_filter_feature.png"</span>)
<span class="linenr">68: </span>
<span class="linenr">69: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29986;&#29983;&#19968;&#23652;&#20013;&#25152;&#26377;&#36942;&#28670;&#22120;&#38911;&#25033;&#22294;</span>
<span class="linenr">70: </span>  <span style="color: #51afef;">for</span> layer_name <span style="color: #51afef;">in</span> [<span style="color: #98be65;">'block1_conv1'</span>, <span style="color: #98be65;">'block2_conv1'</span>, <span style="color: #98be65;">'block3_conv1'</span>, <span style="color: #98be65;">'block4_conv1'</span>]:
<span class="linenr">71: </span>      <span style="color: #dcaeea;">size</span> = <span style="color: #da8548; font-weight: bold;">64</span>
<span class="linenr">72: </span>      <span style="color: #dcaeea;">margin</span> = <span style="color: #da8548; font-weight: bold;">5</span>
<span class="linenr">73: </span>
<span class="linenr">74: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1. &#29992;&#26044;&#20786;&#23384;&#32080;&#26524;&#30340;&#31354;(&#40657;&#33394;)&#24433;&#20687;</span>
<span class="linenr">75: </span>      <span style="color: #dcaeea;">results</span> = np.zeros((<span style="color: #da8548; font-weight: bold;">8</span> * size + <span style="color: #da8548; font-weight: bold;">7</span> * margin, <span style="color: #da8548; font-weight: bold;">8</span> * size + <span style="color: #da8548; font-weight: bold;">7</span> * margin, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr">76: </span>
<span class="linenr">77: </span>      <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">8</span>):  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#8592; &#36845;&#20195;&#29986;&#29983;&#32178;&#26684;&#30340;&#34892;</span>
<span class="linenr">78: </span>          <span style="color: #51afef;">for</span> j <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">8</span>):  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#8592;&#36845;&#20195;&#29986;&#29983;&#32178;&#26684;&#30340;&#21015;</span>
<span class="linenr">79: </span>              <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22312; layer_name &#20013;&#29986;&#29983;&#36942;&#28670;&#22120; i +(j * 8) &#30340; pattern</span>
<span class="linenr">80: </span>              <span style="color: #dcaeea;">filter_img</span> = generate_pattern(layer_name, i + (j * <span style="color: #da8548; font-weight: bold;">8</span>), size=size)
<span class="linenr">81: </span>
<span class="linenr">82: </span>              <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#32080;&#26524;&#25918;&#22312;&#32080;&#26524;&#32178;&#26684;&#30340;&#26041;&#24418;(i, j)&#20013;</span>
<span class="linenr">83: </span>              <span style="color: #dcaeea;">horizontal_start</span> = i * size + i * margin
<span class="linenr">84: </span>              <span style="color: #dcaeea;">horizontal_end</span> = horizontal_start + size
<span class="linenr">85: </span>              <span style="color: #dcaeea;">vertical_start</span> = j * size + j * margin
<span class="linenr">86: </span>              <span style="color: #dcaeea;">vertical_end</span> = vertical_start + size
<span class="linenr">87: </span>              <span style="color: #dcaeea;">results</span>[horizontal_start: horizontal_end, vertical_start: vertical_end, :] = filter_img
<span class="linenr">88: </span>
<span class="linenr">89: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#32178;&#26684;&#32080;&#26524;</span>
<span class="linenr">90: </span>      plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">20</span>, <span style="color: #da8548; font-weight: bold;">20</span>))
<span class="linenr">91: </span>      plt.imshow(results)
<span class="linenr">92: </span>      plt.plot()
<span class="linenr">93: </span>      plt.savefig(layer_name+<span style="color: #98be65;">"_pattern.png"</span>)
</pre>
</div>


<div id="org31d871d" class="figure">
<p><img src="images/block3_conv1_filter_feature.png" alt="block3_conv1_filter_feature.png" /><br />
</p>
<p><span class="figure-number">Figure 92: </span>block3_conv1 層中的第 0 個 channel 的最大回應 pattern</p>
</div>

<p>
block3_conv1 層中的過濾器 0 似乎對波爾卡圓點圖案有回應，再來就是開始對每一層中的每個 filter/channel，為簡化起見，只每每一層中的前 64 個過濾器，將結果以 8&times;8 網格方式呈現。<br />
</p>

<p>
結果和課本的圖差異很大&#x2026;&#x2026;<br />
</p>


<div id="org35d8307" class="figure">
<p><img src="images/block1_conv1_pattern.png" alt="block1_conv1_pattern.png" /><br />
</p>
<p><span class="figure-number">Figure 93: </span>pattern of block1_conv1</p>
</div>


<div id="org799f40f" class="figure">
<p><img src="images/block1_conv2_pattern.png" alt="block1_conv2_pattern.png" /><br />
</p>
<p><span class="figure-number">Figure 94: </span>pattern of block_conv1</p>
</div>


<div id="orgbae7641" class="figure">
<p><img src="images/block3_conv1_pattern.png" alt="block3_conv1_pattern.png" /><br />
</p>
<p><span class="figure-number">Figure 95: </span>pattern of block3_conv1</p>
</div>


<div id="orgb570587" class="figure">
<p><img src="images/block4_conv1_pattern.png" alt="block4_conv1_pattern.png" /><br />
</p>
<p><span class="figure-number">Figure 96: </span>pattern of block4_conv1</p>
</div>
</div>
</div>

<div id="outline-container-org5c5f0b5" class="outline-3">
<h3 id="org5c5f0b5"><span class="section-number-3">12.3</span> 視覺化類別激活熱圖 heatmap of class activation</h3>
<div class="outline-text-3" id="text-12-3">
<p>
CMA(class activation map)可以用來理解影像中的哪些部份會讓 convnet 做出最終分類的決策，這有助於 convnet 決策過程的偵錯。CAM 主要是針對輸入影像產生類別激活熱圖(heatmap of class)，這是一個 2D 的分數網格圖，針對輸入影像的每個位置(網格)進行計算，然後指出每個位置相對於目前類別的重要性。我們可以使用&ldquo;Graid-CAM: Visual Explanations from Deep Networks via Gradient-based Localization&rdquo;這篇論文提到的方法，即，給定影像，取得卷積層的輸出特徵，以&ldquo;這個類別對每個 channel 的梯度值&rdquo;對這個特徵圖中的每個 channel 做加權。進而產生「某張圖片激活某類別的強度」的 2D 分數網格圖。做法如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.applications.vgg16 <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 2: </span> 
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">model</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35531;&#27880;&#24847;, &#22312;&#38914;&#37096;&#21253;&#21547;&#20102;&#23494;&#38598;&#36899;&#25509;&#30340;&#20998;&#39006;&#22120; (&#38928;&#35373; include_top=True)</span>
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38928;&#20808;&#34389;&#29702; VGG16 &#30340;&#36664;&#20837;&#24433;&#20687;</span>
<span class="linenr"> 6: </span>  <span style="color: #51afef;">from</span> keras.preprocessing <span style="color: #51afef;">import</span> image
<span class="linenr"> 7: </span>  <span style="color: #51afef;">from</span> keras.applications.vgg16 <span style="color: #51afef;">import</span> preprocess_input, decode_predictions
<span class="linenr"> 8: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">img_path</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/african_elephants.jpg'</span>
<span class="linenr">11: </span>
<span class="linenr">12: </span>  <span style="color: #dcaeea;">img</span> = image.load_img(img_path, target_size=(<span style="color: #da8548; font-weight: bold;">224</span>, <span style="color: #da8548; font-weight: bold;">224</span>))
<span class="linenr">13: </span>  <span style="color: #51afef;">print</span>(<span style="color: #c678dd;">type</span>(img))  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30446;&#21069;&#22294;&#29255;&#28858; &lt;class 'PIL.Image.Image'&gt; &#29289;&#20214;</span>
<span class="linenr">14: </span>  <span style="color: #51afef;">print</span>(img.size)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21487;&#20197;&#29992; size &#23660;&#24615;&#26597;&#30475;&#23610;&#23544; -&gt; (224, 224)</span>
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #dcaeea;">x</span> = image.img_to_array(img)   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559; PIL &#29289;&#20214;&#36681;&#28858; float32 &#30340; Numpy &#38499;&#21015;</span>
<span class="linenr">17: </span>  <span style="color: #51afef;">print</span>(x.shape)                <span style="color: #5B6268;"># </span><span style="color: #5B6268;">shape=(224, 224, 3)</span>
<span class="linenr">18: </span>
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559; x &#38499;&#21015; (&#21487;&#35222;&#28858;&#24373;&#37327;) &#22686;&#21152;&#19968;&#20491;&#25209;&#27425;&#36600;, shape=(1, 224, 224, 3)</span>
<span class="linenr">21: </span>  <span style="color: #dcaeea;">x</span> = np.expand_dims(x, axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">22: </span>  <span style="color: #51afef;">print</span>(x.shape)
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #dcaeea;">x</span> = preprocess_input(x) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38928;&#34389;&#29702;&#25209;&#27425;&#37327; (&#36889;&#26371;&#23565;&#27599;&#19968; channel &#20570;&#38991;&#33394;&#20540;&#27491;&#35215;&#21270;)</span>
<span class="linenr">25: </span>
<span class="linenr">26: </span>
<span class="linenr">27: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20351;&#29992; VGG &#31070;&#32147;&#32178;&#36335;&#38928;&#28204;&#22294;&#29255;&#39006;&#21029;</span>
<span class="linenr">28: </span>  <span style="color: #dcaeea;">preds</span> = model.predict(x)
<span class="linenr">29: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#38928;&#28204;&#32080;&#26524;:'</span>, decode_predictions(preds, top=<span style="color: #da8548; font-weight: bold;">3</span>)[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">30: </span>
<span class="linenr">31: </span>  np.argmax(preds[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">32: </span>
<span class="linenr">33: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35373;&#23450; Gard-CAM &#28436;&#31639;&#27861;</span>
<span class="linenr">34: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> backend <span style="color: #51afef;">as</span> K
<span class="linenr">35: </span>
<span class="linenr">36: </span>  <span style="color: #dcaeea;">african_elephant_output</span> = model.output[:, <span style="color: #da8548; font-weight: bold;">386</span>] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#8592; &#38928;&#28204;&#21521;&#37327;&#20013;&#30340; "&#38750;&#27954;&#35937;" &#38917;&#30446;</span>
<span class="linenr">37: </span>
<span class="linenr">38: </span>  <span style="color: #dcaeea;">last_conv_layer</span> = model.get_layer(<span style="color: #98be65;">'block5_conv3'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">block5_conv3 &#23652;&#30340;&#36664;&#20986;&#29305;&#24501;&#22294;, &#20854;&#28858; VGG16 &#20013;&#30340;&#26368;&#24460;&#19968;&#20491;&#21367;&#31309;&#23652;</span>
<span class="linenr">39: </span>
<span class="linenr">40: </span>  <span style="color: #dcaeea;">grads</span> = K.gradients(african_elephant_output, last_conv_layer.output)[<span style="color: #da8548; font-weight: bold;">0</span>] <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">block5_conv3 &#30340;&#36664;&#20986;&#29305;&#24501;&#22294;&#20013;&#38364;&#26044; "&#38750;&#27954;&#35937;" &#39006;&#21029;&#30340;&#26799;&#24230;</span>
<span class="linenr">41: </span>
<span class="linenr">42: </span>  <span style="color: #dcaeea;">pooled_grads</span> = K.mean(grads, axis=(<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>)) <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">&#36681;&#25563;&#25104;&#21521;&#37327; shape = (512, ), &#20854;&#20013;&#27599;&#20491;&#38917;&#30446;&#26159;&#29305;&#23450;&#29305;&#24501;&#22294; channel &#30340;&#26799;&#24230;&#24179;&#22343;&#24375;&#24230;(&#20540;)</span>
<span class="linenr">43: </span>
<span class="linenr">44: </span>  <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">&#32102;&#23450;&#36664;&#20837;&#24433;&#20687;&#30340;&#26781;&#20214;&#19979;, &#35731;&#25105;&#20497;&#21487;&#20197;&#23384;&#21462;&#21083;&#21083;&#23450;&#32681;&#30340;&#25976;&#20540;&#65306;pooled_grads &#21644; block5_conv3 &#30340;&#36664;&#20986;&#29305;&#24501;&#22294;</span>
<span class="linenr">45: </span>  <span style="color: #dcaeea;">iterate</span> = K.function([model.<span style="color: #c678dd;">input</span>], 
<span class="linenr">46: </span>      [pooled_grads, last_conv_layer.output[<span style="color: #da8548; font-weight: bold;">0</span>]])
<span class="linenr">47: </span>
<span class="linenr">48: </span>  <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">&#23565;&#26044;&#32102;&#23450;&#30340;&#20841;&#38587;&#22823;&#35937;&#27171;&#26412;&#24433;&#20687;, &#29986;&#29983;&#36889;&#20841;&#20491;&#37327;&#20540;, &#20197; Numpy &#38499;&#21015;&#21576;&#29694;</span>
<span class="linenr">49: </span>  <span style="color: #dcaeea;">pooled_grads_value</span>, <span style="color: #dcaeea;">conv_layer_output_value</span> = iterate([x])
<span class="linenr">50: </span>
<span class="linenr">51: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#29305;&#24501;&#22294;&#38499;&#21015;&#20013;&#30340;&#27599;&#20491; channel &#33287; "&#22823;&#35937;" &#39006;&#21029;&#30456;&#38364;&#30340; "&#27492; channel &#30340;&#37325;&#35201;&#31243;&#24230;" &#30456;&#20056;</span>
<span class="linenr">52: </span>  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">512</span>):
<span class="linenr">53: </span>      <span style="color: #dcaeea;">conv_layer_output_value</span>[:, :, i] *= pooled_grads_value[i]
<span class="linenr">54: </span>
<span class="linenr">55: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29305;&#24501;&#22294;&#30340;&#36328; channel &#24179;&#22343;&#20540;&#26159;&#39006;&#21029;&#28608;&#27963;&#20989;&#25976;&#36664;&#20986;&#30340;&#29105;&#22294;</span>
<span class="linenr">56: </span>  <span style="color: #dcaeea;">heatmap</span> = np.mean(conv_layer_output_value, axis=-<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">57: </span>
<span class="linenr">58: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#29105;&#22294;&#24460;&#26399;&#34389;&#29702;</span>
<span class="linenr">59: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">60: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">61: </span>
<span class="linenr">62: </span>  <span style="color: #dcaeea;">heatmap</span> = np.maximum(heatmap, <span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">63: </span>  <span style="color: #dcaeea;">heatmap</span> /= np.<span style="color: #c678dd;">max</span>(heatmap)
<span class="linenr">64: </span>  plt.matshow(heatmap)
<span class="linenr">65: </span>  plt.plot()
<span class="linenr">66: </span>  plot.savefig(<span style="color: #98be65;">"heapmapOfClassActivation.png"</span>)
<span class="linenr">67: </span>
<span class="linenr">68: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#29105;&#22294;&#33287;&#21407;&#22987;&#24433;&#20687;&#30090;&#21152;&#22312;&#19968;&#36215;</span>
<span class="linenr">69: </span>  <span style="color: #51afef;">import</span> cv2
<span class="linenr">70: </span>
<span class="linenr">71: </span>  <span style="color: #dcaeea;">img</span> = cv2.imread(img_path)
<span class="linenr">72: </span>
<span class="linenr">73: </span>  <span style="color: #dcaeea;">heatmap</span> = cv2.resize(heatmap, (img.shape[<span style="color: #da8548; font-weight: bold;">1</span>], img.shape[<span style="color: #da8548; font-weight: bold;">0</span>]))
<span class="linenr">74: </span>
<span class="linenr">75: </span>  <span style="color: #dcaeea;">heatmap</span> = np.uint8(<span style="color: #da8548; font-weight: bold;">255</span> * heatmap)
<span class="linenr">76: </span>
<span class="linenr">77: </span>  <span style="color: #dcaeea;">heatmap</span> = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
<span class="linenr">78: </span>
<span class="linenr">79: </span>  <span style="color: #dcaeea;">superimposed_img</span> = heatmap * <span style="color: #da8548; font-weight: bold;">0.4</span> + img <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36889;&#35041; 0.4 &#26159;&#29105;&#22294;&#24375;&#24230;&#22240;&#23376;</span>
<span class="linenr">80: </span>
<span class="linenr">81: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#26159;&#21542;&#20786;&#23384;&#25104;&#21151;:'</span>, cv2.imwrite(<span style="color: #98be65;">'elephant_cam.jpg'</span>, superimposed_img))
<span class="linenr">82: </span>
</pre>
</div>


<div id="org5af4e57" class="figure">
<p><img src="images/heapmapOfClassActivation.png" alt="heapmapOfClassActivation.png" /><br />
</p>
<p><span class="figure-number">Figure 97: </span>Heapmap of class activation</p>
</div>


<div id="org9dd4817" class="figure">
<p><img src="images/elephant_cam-1.jpg" alt="elephant_cam-1.jpg" /><br />
</p>
<p><span class="figure-number">Figure 98: </span>將激活熱圖與影像叠加</p>
</div>

<p>
上圖的視覺化技術回答了兩個重要問題：<br />
</p>
<ul class="org-ul">
<li>為什麼神經網路認為這個影像裡有非洲象?<br /></li>
<li>非洲象位於影像中的哪個位置?<br /></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org615672f" class="outline-2">
<h2 id="org615672f"><span class="section-number-2">13</span> MLP 神經網路模型實作：以 Keras 為實作工具</h2>
<div class="outline-text-2" id="text-13">
</div>
<div id="outline-container-orge12ed1e" class="outline-3">
<h3 id="orge12ed1e"><span class="section-number-3">13.1</span> 簡介</h3>
<div class="outline-text-3" id="text-13-1">
<p>
Keras 是 Python 的深度學習框架，提供一種便利的方式來定義和訓練幾秬所有類型的深度學習模型。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org92d469d"></a>優點<br />
<div class="outline-text-4" id="text-13-1-1">
<ul class="org-ul">
<li>相同的程式碼可在 CPU 或 GPU 上執行<br /></li>
<li>內建程式庫支擾了卷積神經網路(用於電腦視覺)、循環神經網路(用於序列資料處理)，以及二者的任何組合。<br /></li>
<li>Keras 可以使用最少的程式碼，花最少的時間，就能建立深度學習模型，並進行培訓、評做準確率；相對的，如果使用 TensorFlow，則需要更多程式碼，花費更多時間。<br /></li>
<li>採用寬鬆的 MIT 授權條款，所以可以自由使用在商業專案上。<br /></li>
</ul>

<p>
Keras 是一個 model-level 模型級的深度學習程式庫，Keras 只處理模型的建立、訓練、預測等功能。深度學習程式庫的運作（如張量運算），Keras 必須配合使用「後端引擎」(backend Engine)進行運算。目前 Keras 提供了兩種 backend engine：Theano 與 TensorFlow。其基本架構如下圖所示：<br />
</p>

<div class="org-src-container">
<pre class="src src-ditaa">  +----------------------------+   
  |            Keras           |
  +----------------------------+   
  | TensorFlow/Theano/CNTK/... |
  +------------+---------------+   
  | CUDA/cuDNN |  BLAS, Eigen  |
  +------------+---------------+   
  |     GPU    |     CPU       |
  +------------+---------------+
</pre>
</div>

<div id="orgd6e6271" class="figure">
<p><img src="images/KerasArch.png" alt="KerasArch.png" width="360" /><br />
</p>
<p><span class="figure-number">Figure 99: </span>深度學習軟硬體架構</p>
</div>

<p>
由圖<a href="#orgd6e6271">99</a>可看出，Keras 並未被綁定在特定張量程式庫中，而是改以模組方式處理，目前可用的後端引擎有 Montreal 大學 MILA 實驗室的 Theano、Google 的 TensorFlow、Microsoft 的 CNTK&#x2026;等，這些後端引擎在應用不同硬體(CPU/GPU)時則會採用不同的低階程式庫(CUDA/Eigen)。<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-orga9b8ded" class="outline-3">
<h3 id="orga9b8ded"><span class="section-number-3">13.2</span> Keras 程式設計模式</h3>
<div class="outline-text-3" id="text-13-2">
<p>
Keras 的開發流程大致如下：<br />
</p>
<ol class="org-ol">
<li>定義問題並建立資料集<br /></li>
<li>選擇一種評量成功的準則(metrics)<br /></li>
<li>決定驗證(validation)程序<br /></li>
<li>準備資料：定義訓練資料：即 input tensor 和 target tensors(label tensors)<br /></li>
<li>開發出優於基準(baseline)的模型：定義神經網路模型的 layers，以便將 input tensor 對應到預測值<br /></li>
<li>選擇 loss function, optimizer 和監控的評量準則(metrics)來建立學習過程<br /></li>
<li>呼叫模型中的 fit()方法來迭代訓練資料<br /></li>
<li>擴大規模：開發一個過度適配的模型<br /></li>
<li>常規化模型並調整參數<br /></li>
</ol>
</div>

<ol class="org-ol">
<li><a id="org3c1150c"></a>定義問題並建立資料集<br />
<div class="outline-text-4" id="text-13-2-1">
<p>
進行模型建構之初，我們首先要評估的是：<br />
</p>
<ul class="org-ul">
<li>輸入資料是什麼？想要預測什麼？有什麼樣的訓練資料，就只能學習預測該類問題。例如，手上只有電影評論和情緒標註資料，就只能學習對電影評論的情緒分類。<br /></li>
<li>面臨什麼樣的問題？是二元分類？多類別分類？純量迴歸？向量迴歸？多類別多標籤？分群？生成式學習？增強式學習？不同的問題類型會引導我們如何選擇模型架構與損失函數。<br /></li>
</ul>

<p>
在確認上述兩項問題後，我們是基於以下兩個假設來進行模型的建立：<br />
</p>

<ul class="org-ul">
<li>假設機器可以根據給定的輸入資料預測結果<br /></li>
<li>假設手上的資料能提供足夠的資訊，讓機器能學習到輸入與輸出間的關係<br /></li>
</ul>

<p>
在真正建構出一個可用模型之前，上述兩個假設依然只是假設，必須經過驗證後才能確定成立與否。重點是：並非所有的問題都能透過模型來解決，例如：試圖以最近的歷史價格來預測股票市場的走勢就很難成功，因為光是參考歷史價格並不足以提供預測股價所需資訊。<br />
</p>

<p>
另一種要特別留意的問題類型為非平穩問題(nonstationary problems)，例如分析服裝的消售/推薦，這當中存在的最大問題在於人們購買的衣服種類會隨季節而變化，所以服裝購買在幾個月內是非平穩現象，建立的模型內容會隨著時間而變化。在這種情況下，解決方法有：<br />
</p>

<ul class="org-ul">
<li>不斷以最近的資料重新訓練模型，或<br /></li>
<li>在相對平穩的時間區間(具有規律的週期間)收集資料，以購買衣服為例，應該以年為單位進行資料收集才足以補捉到季節變化的規律。<br /></li>
</ul>

<p>
最後，切記：機器學習只能用於學習訓練資料中已存在的模式，也就是只能認出以前見過的模式。通常我們所謂以過去的資料預測未來，是假設未來的行為在過去曾發生過，但實際情況則未必如此。<br />
</p>
</div>
</li>

<li><a id="org714a174"></a>選擇一種評量成功的準則(metrics)<br />
<div class="outline-text-4" id="text-13-2-2">
<p>
選好評量成功的準則，才有選擇損失函數的依據。在 Keras 中，所謂選擇評量準則，就是在 compile 時選擇適當的 metrics 參數。大概的選擇原則如下：<br />
</p>

<ul class="org-ul">
<li>二元分類問題：accuracy 和 ROC AUC(area under the receiver operating characteristic curve)為兩種常用的度量。<br /></li>
<li>類別不均(class-imbalanced)問題：使用 precision 和 recall 來做度量。<br /></li>
<li>排名問題或多標籤問題：使用平均精度<br /></li>
<li>少問的問題：自行定義指標<br /></li>
</ul>
</div>
</li>

<li><a id="org2137244"></a>決定驗證(validation)程序<br />
<div class="outline-text-4" id="text-13-2-3">
<p>
一旦決定目標，就要決定驗證學習進度的方法，三種常見的驗證方法如下所述，但在大多數情況下，第一個方法就有不錯的效能。<br />
</p>

<ul class="org-ul">
<li>Simple hold-out: 資料量大時適用<br /></li>
<li>K-fold cross validation: 樣本資料不夠多時用<br /></li>
<li>Iterated K-fold validation with shuffling: 資料量非常少時用<br /></li>
</ul>
</div>
</li>

<li><a id="orge566851"></a>準備資料<br />
<div class="outline-text-4" id="text-13-2-4">
<p>
一旦知道要訓練什麼、要優化什麼、以及如評估效能，就可以著手準備建構模型，但首先要把資料整理成可以輸入神經網路的格式（張量），以監督式學習而言，其輸入的訓練資料會有以下兩類：即 input tensor 和 target tensors(label tensors)<br />
</p>
</div>
</li>

<li><a id="org1276dab"></a>開發優於基準(baseline)的模型<br />
<div class="outline-text-4" id="text-13-2-5">
<p>
此階段的目標在於實現統計功效(statistical power)，以 MNIST 資料集為例，任何準確度大於 0.1 的模型都可以說具有統計功效的（因為一共有 10 個答案類鞏）；而在 IMDB 範例中，只要準確度大於 0.5 即算。雖然 baseline 是一個很低的標準，但我們不見得都能實現這個目標，如果在嚐試過多個合理架構後模型表現仍無法優於隨機基準能力，則很有可能問題出在輸入資料，也許輸入資料沒有所需答案。<br />
</p>

<p>
如果一切順利，則接下來我們要做出三個關鍵選擇來建構第一個模型：<br />
</p>

<ul class="org-ul">
<li>選擇最後一層的啟動函數：這將為神經網路建立輸出的形式。例如，IMDB 分類最後使用 sigmoid 分成兩個值、MNIST 則以 softmax 分為 10 類。<br /></li>
<li>損失函數：要配合問題類型，如 IMDB 使用 binary_crossentropy、迴歸則使用 mse。<br /></li>
<li>優化器設定：大多數情況下，rmsprop 可做為預設選項搭配預設學習率<br /></li>
</ul>

<p>
下表為選擇啟動函數與損失函數的參考<br />
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">問題類型</th>
<th scope="col" class="org-left">輸出層啟動函數</th>
<th scope="col" class="org-left">損失函數</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Binary classification</td>
<td class="org-left">Sigmoid</td>
<td class="org-left">binary_crossentropy</td>
</tr>

<tr>
<td class="org-left">Multiclass, single-label classification</td>
<td class="org-left">softmax</td>
<td class="org-left">categorical_crossentropy</td>
</tr>

<tr>
<td class="org-left">Multiclass, multi-label classification</td>
<td class="org-left">sigmoid</td>
<td class="org-left">binary_crossentropy</td>
</tr>

<tr>
<td class="org-left">Regression to arbitrary values</td>
<td class="org-left">None</td>
<td class="org-left">mse</td>
</tr>

<tr>
<td class="org-left">Regression to values between 0 and 1</td>
<td class="org-left">sigmoid</td>
<td class="org-left">mse or binary_crossentropy</td>
</tr>
</tbody>
</table>
</div>
</li>

<li><a id="orgd589e49"></a>擴大規模：開發一個過度適配的模型<br />
<div class="outline-text-4" id="text-13-2-6">
<p>
一旦成功建構了一個超越 baseline 的模型，問題就變成：這個模型夠不夠強大？有沒有足夠的 layer 和參數來正確模擬手上的問題？例如，只有兩個 units 的單隱藏層也許有辨識 MNIST 的統計功效，但不足以很好的解決該問題。而機器學習就是在最佳化和普適性之間做取捨，理想的模型是介於 underfitting 和 overfitting 的交界、介於模型太小(undercapacity)和模型太大(overcapacity)之間，要找出這個位置，勢必要先越過它再回來。所以，要搞清楚需要多大的模型，就要開發一個太大的模型，有幾種方法可以達到這點：<br />
</p>

<ul class="org-ul">
<li>加入更多的 layer<br /></li>
<li>增加每一層的 capacity<br /></li>
<li>訓練更多的週期<br /></li>
</ul>
</div>
</li>

<li><a id="org563cf9a"></a>常規化模型並調整參數<br />
<div class="outline-text-4" id="text-13-2-7">
<p>
這裡會花掉最多時間：要反覆修改模型、訓練模型、評估驗證資料，然後再次修改，以下有幾種做法：<br />
</p>

<ul class="org-ul">
<li>加入 dropout<br /></li>
<li>嘗試不同架構：新增或刪除 layer<br /></li>
<li>添加 L1 或 L2 regularization，或同時使用<br /></li>
<li>嘗試使用不同的超參數，如每一曾的 units 數或優化器的學習率<br /></li>
<li>著重於特徵工程，如加入新特徵、刪除似乎沒用的特徵<br /></li>
</ul>

<p>
一旦開發出令人滿意的模型配置，就可以在所有可用資料(訓練和驗證)上訓練最終產出的模型，並在測試集上最後一次評估它。<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org692aeea" class="outline-3">
<h3 id="org692aeea"><span class="section-number-3">13.3</span> 基本流程</h3>
<div class="outline-text-3" id="text-13-3">
<p>
在 Keras 定義 model 有兩種方法：<br />
</p>

<ul class="org-ul">
<li>Sequential class: 適用於線性堆叠的模型<br /></li>
<li>functional API: 適用任何有向無環的神經網路架構<br /></li>
</ul>

<p>
以下為建立 sequential model 的例子：<br />
</p>

<ol class="org-ol">
<li>建立模型<br /></li>
</ol>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">3: </span>
<span class="linenr">4: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26032;&#22686;&#19968;&#20491;&#36664;&#20837;&#28858;874&#32173;&#12289;&#36664;&#20986;&#28858;32&#32173;&#30340;Dense layer </span>
<span class="linenr">6: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">32</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">784</span>,)))
<span class="linenr">7: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25509;&#25976;&#20358;&#33258;&#19978;&#23652;32&#32173;&#30340;&#36664;&#20837;&#65292;&#36664;&#20986;&#19968;&#20491;10&#32173;&#30340;&#36039;&#26009;</span>
<span class="linenr">8: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">10</span>, activation=<span style="color: #98be65;">'softmax'</span>))
</pre>
</div>

<p>
若使用 API 來定義相同的模型，其語法如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">3: </span>
<span class="linenr">4: </span>  <span style="color: #dcaeea;">input_tensor</span> = layers.Input(shape=(<span style="color: #da8548; font-weight: bold;">784</span>,))
<span class="linenr">5: </span>  <span style="color: #dcaeea;">x</span> = layers.Dense(<span style="color: #da8548; font-weight: bold;">32</span>, activation=<span style="color: #98be65;">'relu'</span>)(input_tensor)
<span class="linenr">6: </span>  <span style="color: #dcaeea;">output_tensor</span> = layers.Dense(<span style="color: #da8548; font-weight: bold;">10</span>, activation=<span style="color: #98be65;">'softmax'</span>)(x)
<span class="linenr">7: </span>  <span style="color: #dcaeea;">model</span> = models.Model(inputs=input_tensor, outputs=output_tensor)
<span class="linenr">8: </span>
</pre>
</div>

<ol class="org-ol">
<li>一旦建立好模型架構，則無論是使用 Sequential 或 API，其餘步驟均相同。神經網路是在編譯(model.compile)時建立的，我們可以在其中指定使用的 optimizer 和 loss function，以及訓練期間監看的評量準則(metrics)，典型的範例如下：<br /></li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr">2: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=optimizers.RMSprop(lr=<span style="color: #da8548; font-weight: bold;">0.001</span>), loss=<span style="color: #98be65;">'mse'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
</pre>
</div>

<ol class="org-ol">
<li>最後，整個學習程序經由 fit()將輸入資料以 Numpy 陣列的形式傳給模型：<br /></li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  model.fit(input_tensor, target_tensor, batch_size=<span style="color: #da8548; font-weight: bold;">128</span>, ephchs=<span style="color: #da8548; font-weight: bold;">10</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-org5b87db5" class="outline-3">
<h3 id="org5b87db5"><span class="section-number-3">13.4</span> 以 Keras 實作 MNist 手寫數字辨識資料集</h3>
<div class="outline-text-3" id="text-13-4">
</div>
<ol class="org-ol">
<li><a id="org992da63"></a>讀入資料與預處理<br />
<div class="outline-text-4" id="text-13-4-1">
<p>
MNist 手寫數字辨識資料集是由 Yann LeCun 所蒐集，他也是 CNN 的創始人。MNist 資料集共有訓練資料集 60000 筆、測試資料集 10000 筆，每筆資料都由一 28*28 的 image 以及相對應的 label 組成。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #98be65;">'''###1. &#19979;&#36617;MNist&#36039;&#26009;###'''</span>
<span class="linenr"> 2: </span>  <span style="color: #98be65;">'''1.1 &#28377;&#20837;Keras&#21450;&#30456;&#38364;&#25152;&#38656;&#36039;&#28304;'''</span>
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pf
<span class="linenr"> 5: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35201;&#23559;table&#36681;&#28858;one-hot encoding</span>
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #98be65;">'''1.2 &#21295;&#20837;Keras&#27169;&#32068;&#20197;&#19979;&#36617;MNist&#36039;&#26009;&#38598;'''</span>
<span class="linenr">10: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr">11: </span>  <span style="color: #98be65;">'''1.3 &#35712;&#21462;MNist&#36039;&#26009;&#38598;'''</span>
<span class="linenr">12: </span>  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
<span class="linenr">13: </span>
<span class="linenr">14: </span>  <span style="color: #83898d;">'''1.4 &#26597;&#30475;MNist&#36039;&#26009;&#38598;&#31558;&#25976;'''</span>
<span class="linenr">15: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'4. &#26597;&#30475;MNist&#36039;&#26009;&#38598;&#31558;&#25976;'</span>)
<span class="linenr">16: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'train data='</span>, <span style="color: #c678dd;">len</span>(x_train_image))
<span class="linenr">17: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">' test data='</span>, <span style="color: #c678dd;">len</span>(x_test_image))
<span class="linenr">18: </span>
<span class="linenr">19: </span>  <span style="color: #98be65;">'''###2.  &#26597;&#30475;&#35347;&#32244;&#36039;&#26009;'''</span>
<span class="linenr">20: </span>  <span style="color: #98be65;">'''2.1 &#36664;&#20986;&#35347;&#32244;&#36039;&#26009;&#26684;&#24335;'''</span>
<span class="linenr">21: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'2.1 &#26597;&#30475;&#35347;&#32244;&#36039;&#26009;&#26684;&#24335;'</span>)
<span class="linenr">22: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'train image='</span>, x_train_image.shape)
<span class="linenr">23: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">' test image='</span>, y_train_label.shape)
<span class="linenr">24: </span>
<span class="linenr">25: </span>  <span style="color: #98be65;">'''2.2 &#23450;&#32681;plot_image&#20989;&#25976;&#39023;&#31034;&#25976;&#23383;&#24433;&#20687;'''</span>
<span class="linenr">26: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">27: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_image</span>(imgname, image):
<span class="linenr">28: </span>      <span style="color: #dcaeea;">fig</span> = plt.gcf() <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23450;&#35373;&#22294;&#24418;&#22823;&#23567;</span>
<span class="linenr">29: </span>      fig.set_size_inches(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">30: </span>      plt.imshow(image, cmap=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">cmap&#21443;&#25976;&#35373;&#23450;&#28858;binary&#20197;&#40657;&#30333;&#28784;&#38542;&#39023;&#31034;</span>
<span class="linenr">31: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show() #for jupyter or colab</span>
<span class="linenr">32: </span>      plt.plot()
<span class="linenr">33: </span>      plt.savefig(imgname+<span style="color: #98be65;">".png"</span>)
<span class="linenr">34: </span>
<span class="linenr">35: </span>  <span style="color: #98be65;">'''2.3 &#22519;&#34892;plot_image&#20989;&#25976;&#26597;&#30475;&#31532;0&#31558;&#25976;&#23383;&#24433;&#20687;&#21450;&#23565;&#25033;label'''</span>
<span class="linenr">36: </span>  plot_image(<span style="color: #98be65;">"Keras-mnist-0"</span>, x_train_image[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">37: </span>  <span style="color: #51afef;">print</span>(y_train_label[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">38: </span>
<span class="linenr">39: </span>  <span style="color: #98be65;">'''###3. &#26597;&#30475;&#22810;&#31558;&#36039;&#26009;'''</span>
<span class="linenr">40: </span>  <span style="color: #98be65;">'''3.1 &#24314;&#31435;plot_images_labels_prediction()&#20989;&#24335;'''</span>
<span class="linenr">41: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_images_labels_prediction</span>(imgname, images, labels, prediction, idx, num = <span style="color: #da8548; font-weight: bold;">10</span>):
<span class="linenr">42: </span>      <span style="color: #dcaeea;">fig</span> = plt.gcf()
<span class="linenr">43: </span>      fig.set_size_inches(<span style="color: #da8548; font-weight: bold;">8</span>,<span style="color: #da8548; font-weight: bold;">4</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35373;&#23450;&#39023;&#31034;&#22294;&#24418;&#22823;&#23567;</span>
<span class="linenr">44: </span>      <span style="color: #51afef;">if</span> num&gt;<span style="color: #da8548; font-weight: bold;">25</span>: <span style="color: #dcaeea;">num</span>=<span style="color: #da8548; font-weight: bold;">25</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#36039;&#26009;&#31558;&#25976;&#19978;&#38480;</span>
<span class="linenr">45: </span>      <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">0</span>, num):
<span class="linenr">46: </span>          <span style="color: #dcaeea;">ax</span>=plt.subplot(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">1</span>+i) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27492;&#34389;&#39023;&#31034;10&#31558;&#22294;&#24418;&#65292;2*5&#20491;</span>
<span class="linenr">47: </span>          ax.imshow(images[idx], cmap=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr">48: </span>          <span style="color: #dcaeea;">title</span>= <span style="color: #98be65;">"label="</span> +<span style="color: #c678dd;">str</span>(labels[idx]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21152;&#20837;&#23376;&#22294;&#24418;title</span>
<span class="linenr">49: </span>          <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(prediction)&gt;<span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr">50: </span>              <span style="color: #dcaeea;">title</span>+=<span style="color: #98be65;">",predict="</span>+<span style="color: #c678dd;">str</span>(prediction[idx]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27161;&#38988;title&#21152;&#20837;&#38928;&#28204;&#32080;&#26524;</span>
<span class="linenr">51: </span>          ax.set_title(title,fontsize=<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">52: </span>          ax.set_xticks([]);ax.set_yticks([]) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#19981;&#39023;&#31034;&#21051;&#24230;</span>
<span class="linenr">53: </span>          <span style="color: #dcaeea;">idx</span>+=<span style="color: #da8548; font-weight: bold;">1</span> 
<span class="linenr">54: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">55: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.plot()</span>
<span class="linenr">56: </span>      plt.savefig(imgname+<span style="color: #98be65;">".png"</span>)
<span class="linenr">57: </span>
<span class="linenr">58: </span>  <span style="color: #98be65;">'''3.2 &#22519;&#34892;plot_images_labels_prediction&#20989;&#25976;&#26597;&#30475;&#22810;&#31558;images&#21450;labels'''</span>
<span class="linenr">59: </span>  plot_images_labels_prediction(<span style="color: #98be65;">"Keras-mnist-1"</span>,x_train_image,y_train_label,[],<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">60: </span>
<span class="linenr">61: </span>  <span style="color: #98be65;">'''###4. &#22810;&#23652;&#24863;&#30693;&#22120;(Multilayer perception, MLP)&#27169;&#22411;&#36039;&#26009;&#38928;&#34389;&#29702;'''</span>
<span class="linenr">62: </span>  <span style="color: #98be65;">'''4.1 &#20197;reshape&#36681;&#25563;image&#30697;&#38499;'''</span>
<span class="linenr">63: </span>  <span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">64: </span>  <span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">65: </span>  <span style="color: #51afef;">print</span>(x_Train.shape)
<span class="linenr">66: </span>  <span style="color: #51afef;">print</span>(x_Test.shape)
<span class="linenr">67: </span>  <span style="color: #98be65;">'''4.2 &#23559;&#24433;&#20687;&#20043;&#25976;&#23383;&#30697;&#38499;&#27491;&#35215;&#21270;'''</span>
<span class="linenr">68: </span>  <span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">69: </span>  <span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">70: </span>
<span class="linenr">71: </span>  <span style="color: #83898d;">'''4.3 &#21407;&#22987;label&#27396;&#20301;'''</span>
<span class="linenr">72: </span>  <span style="color: #51afef;">print</span>(y_train_label[:<span style="color: #da8548; font-weight: bold;">5</span>]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36664;&#20986;&#21069;5&#31558;</span>
<span class="linenr">73: </span>  <span style="color: #98be65;">'''4.4 &#36914;&#34892;One-hot encoding'''</span>
<span class="linenr">74: </span>  <span style="color: #dcaeea;">y_TrainOneHot</span> = np_utils.to_categorical(y_train_label)
<span class="linenr">75: </span>  <span style="color: #dcaeea;">y_TestOneHot</span> = np_utils.to_categorical(y_test_label)
<span class="linenr">76: </span>  <span style="color: #83898d;">'''4.5 &#36681;&#25563;&#24460;&#20043;label&#27396;&#20301;'''</span>
<span class="linenr">77: </span>  <span style="color: #51afef;">print</span>(y_TrainOneHot[:<span style="color: #da8548; font-weight: bold;">5</span>]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36664;&#20986;&#21069;5&#31558;</span>
<span class="linenr">78: </span>
<span class="linenr">79: </span>  <span style="color: #98be65;">'''###5. Keras MLP'''</span>
</pre>
</div>

<pre class="example">
4. 查看MNist資料集筆數
train data= 60000
 test data= 10000
2.1 查看訓練資料格式
train image= (60000, 28, 28)
 test image= (60000,)
5
(60000, 784)
(10000, 784)
[5 0 4 1 9]
[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
</pre>


<div id="org079d446" class="figure">
<p><img src="images/Keras-mnist-0.png" alt="Keras-mnist-0.png" /><br />
</p>
<p><span class="figure-number">Figure 100: </span>MNist 第一筆資料影像</p>
</div>


<div id="org3e144ce" class="figure">
<p><img src="images/Keras-mnist-1.png" alt="Keras-mnist-1.png" /><br />
</p>
<p><span class="figure-number">Figure 101: </span>MNist 前十筆資料影像</p>
</div>
</div>
</li>

<li><a id="org43bcc60"></a>Keras MLP 辨識 MNist<br />
<ol class="org-ol">
<li><a id="org1948090"></a>多層感知器模型<br />
<div class="outline-text-5" id="text-13-4-2-1">
<p>
MNist 的初始模型分為輸入、隠藏及輸出三層, 輸入層有 784 個輸入神經元(\(x_1,x_2,...,x_{784}\))，接收被 reshape 為一維矩陣的手寫圖片(28*28)；隠藏層內部有 256 個神經元，隱藏層的層數與每層的神經元各數在神經網路的建構中主要取決於設計者；輸出層共有 10 個神經元，代表預測的結果(0~9)。<br />
</p>

<div id="org2867fd7" class="figure">
<p><img src="images/MLP-2.jpg" alt="MLP-2.jpg" /><br />
</p>
<p><span class="figure-number">Figure 102: </span>MNist MLP 模型</p>
</div>
</div>
</li>

<li><a id="org72d5e42"></a>多層感知器的訓練與預測<br />
<div class="outline-text-5" id="text-13-4-2-2">
<p>
多層感知器模型建立後，必須先訓練模型才能夠進行預測(辨識)手寫數字。而這裡所謂的訓練模型，對於神經網路而言，就是學習。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org9edb1fd"></a>訓練(Traning)<br />
<div class="outline-text-6" id="text-13-4-2-2-1">
<p>
MNist 的資料訓練集共 60000 筆，經資料預處理後會產生 features(數字特徵集)與 label(數字的真實值)，然後將這些資料輸入 MLP 模型進行訓練，訓練完成後的模型才能進行預測。<br />
</p>
</div>
</li>
<li><a id="orgf84b7b6"></a>預測(Predict)<br />
<div class="outline-text-6" id="text-13-4-2-2-2">
<p>
將測試資料集匯入訓練完成的 MLP 模型，最後產生預測結果(此例中為 0~9 的數字)。<br />
</p>
</div>
</li>
</ol>
</li>
</ol>
</li>

<li><a id="org73de076"></a>MLP 模型旳建立步驟<br />
<ol class="org-ol">
<li><a id="org9ad627c"></a>進行資料預處理(preprocess)<br />
<ol class="org-ol">
<li><a id="orgba68be5"></a>匯入所需模組<br />
<div class="outline-text-6" id="text-13-4-3-1-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr">2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
<span class="linenr">3: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
</pre>
</div>
</div>
</li>
<li><a id="org26897db"></a>讀取 mnist 資料<br />
<div class="outline-text-6" id="text-13-4-3-1-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr">2: </span>  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
</pre>
</div>
</div>
</li>
<li><a id="org43ef0f9"></a>利用 reshape 轉換影像特徵值(features)<br />
<div class="outline-text-6" id="text-13-4-3-1-3">
<p>
#+BEGIN_SRC python -r -n :results output :exports both :eval no<br />
  x_Train = x_train_image.reshape(60000, 784).astype(&rsquo;float32&rsquo;)<br />
  x_Test = x_test_image.reshape(10000, 784).astype(&rsquo;float32&rsquo;)<br />
  #+END_SRC***** 建立模型<br />
</p>
</div>
</li>
<li><a id="org6718c8c"></a>將 feature 標準化<br />
<div class="outline-text-6" id="text-13-4-3-1-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">2: </span>  <span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
</pre>
</div>
</div>
</li>
<li><a id="org04ec2ec"></a>以 one-hot encoding 轉換數字真實值(label)<br />
<div class="outline-text-6" id="text-13-4-3-1-5">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">y_TrainOneHot</span> = np_utils.to_categorical(y_train_label)
<span class="linenr">2: </span>  <span style="color: #dcaeea;">y_TestOneHot</span> = np_utils.to_categorical(y_test_label)
</pre>
</div>
</div>
</li>
</ol>
</li>
<li><a id="org14076cc"></a>建立模型<br />
<ol class="org-ol">
<li><a id="org31170c2"></a>匯入所需模組<br />
<div class="outline-text-6" id="text-13-4-3-2-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">2: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense
</pre>
</div>
<p>
在 Keras 在 Keras 中有兩類主要的模型：Sequential 順序模型 和 使用函數式 API 的 Model 類模型。<br />
</p>
</div>
</li>
<li><a id="orgf2e9183"></a>建立 Sequential 模型<br />
<div class="outline-text-6" id="text-13-4-3-2-2">
<p>
建立一個線性堆叠模型，後續再使用 model.add()方法將各神經網路層加入模型中即可。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">model</span> = Sequential()
</pre>
</div>
</div>
</li>
<li><a id="orgc54e1d0"></a>建立「輸入層」與「隠藏層」<br />
<div class="outline-text-6" id="text-13-4-3-2-3">
<p>
Dense 神經網路層的特色：所有的上一層與下一層的神經元都完全連接。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#24120;&#24907;&#20998;&#20296;&#30340;&#20098;&#25976;&#21021;&#22987;&#21270;weight&#21644;bias</span>
<span class="linenr">2: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">256</span>, input_dim=<span style="color: #da8548; font-weight: bold;">784</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
</pre>
</div>
</div>
</li>
<li><a id="org2cce9b3"></a>建立「輸出層」<br />
<div class="outline-text-6" id="text-13-4-3-2-4">
<p>
10 個神經元分別對應 0~9 的答案，softmax 可以將神經元的輸出結果轉換為預測每一個數字的機率。建立這裡的 Dense 網路層時無需設定 input_data，因為 Keras 會自動依照上一層的 units 神經元個數(256)來設定這一層的 input_dim 神經元個數。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#24120;&#24907;&#20998;&#20296;&#30340;&#20098;&#25976;&#21021;&#22987;&#21270;weight&#21644;bias</span>
<span class="linenr">2: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'softmax'</span>))
</pre>
</div>
</div>
</li>
<li><a id="org320eec7"></a>查看模型摘要<br />
<div class="outline-text-6" id="text-13-4-3-2-5">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">print</span>(model.summary())
</pre>
</div>
</div>
</li>
<li><a id="orge4263c4"></a>執行結果<br />
<div class="outline-text-6" id="text-13-4-3-2-6">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
<span class="linenr"> 3: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr"> 4: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 5: </span>  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">y_TrainOneHot</span> = np_utils.to_categorical(y_train_label)
<span class="linenr">11: </span>  <span style="color: #dcaeea;">y_TestOneHot</span> = np_utils.to_categorical(y_test_label)
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">13: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr">14: </span>  <span style="color: #dcaeea;">model</span> = Sequential()
<span class="linenr">15: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">256</span>, input_dim=<span style="color: #da8548; font-weight: bold;">784</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">16: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">17: </span>  <span style="color: #51afef;">print</span>(model.summary())
</pre>
</div>

<pre class="example">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 256)               200960    
_________________________________________________________________
dense_2 (Dense)              (None, 10)                2570      
=================================================================
Total params: 203,530
Trainable params: 203,530
Non-trainable params: 0
_________________________________________________________________
None
</pre>

<p>
以上每一層 Param 稱為超參數(Hyper-Parameters)，計算方式為：Param=(上一層神經元數量)\(\times\)(本層的神經元數量)\(+\)(本層的神經元數量)。其中：<br />
</p>
<ul class="org-ul">
<li>隠藏層的 Param 為 200960，即 784(輸入層神經元數量)\(\times\)256(隠藏層神經元數量)+256(隠藏層神經元數量)=200960<br /></li>
<li>輸出層的 Param 為 2570，即 256(隠藏層神經元數量)\(\times\)10(輸出層神經元數量)+10(輸出層神經元數量)=2570<br /></li>
</ul>
</div>
</li>
</ol>
</li>

<li><a id="org1189d39"></a>進行訓練<br />
<div class="outline-text-5" id="text-13-4-3-3">
<p>
模型建立後，即可利用 Back Propagation 來進行訓練，其步驟如下：<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org372ccd0"></a>定義訓練方式<sup><a id="fnr.13" class="footref" href="#fn.13">13</a></sup><br />
<div class="outline-text-6" id="text-13-4-3-3-1">
<p>
在訓練模型前，我們必須使用 compile 方式，設定訓練模式<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>, optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
</pre>
</div>
<p>
model.compile()接收三個參數：<br />
</p>

<ul class="org-ul">
<li>優化器 optimizer。它可以是現有優化器的字符串標識符，如 rmsprop 或 adagrad，也可以是 Optimizer 類的實例。詳見：optimizers。<br /></li>
<li>損失函數 loss，模型試圖最小化的目標函數。它可以是現有損失函數的字符串標識符，如 categorical_crossentropy 或 mse，也可以是一個目標函數。詳見：losses。<br /></li>
<li>評估標準 metrics。對於任何分類問題，你都希望將其設置為 metrics = [&rsquo;accuracy&rsquo;]。評估標準可以是現有的標準的字符串標識符，也可以是自定義的評估標準函數。<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22810;&#20998;&#39006;&#21839;&#38988;</span>
<span class="linenr"> 2: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr"> 3: </span>                loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span class="linenr"> 4: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20108;&#20998;&#39006;&#21839;&#38988;</span>
<span class="linenr"> 7: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr"> 8: </span>                loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr"> 9: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22343;&#26041;&#35492;&#24046;&#22238;&#27512;&#21839;&#38988;</span>
<span class="linenr">12: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr">13: </span>                loss=<span style="color: #98be65;">'mse'</span>)
<span class="linenr">14: </span>
<span class="linenr">15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#33258;&#23450;&#32681;&#35413;&#20272;&#27161;&#28310;&#20989;&#25976;</span>
<span class="linenr">16: </span>  <span style="color: #51afef;">import</span> keras.backend <span style="color: #51afef;">as</span> K
<span class="linenr">17: </span>
<span class="linenr">18: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">mean_pred</span>(y_true, y_pred):
<span class="linenr">19: </span>      <span style="color: #51afef;">return</span> K.mean(y_pred)
<span class="linenr">20: </span>
<span class="linenr">21: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr">22: </span>                loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">23: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>, mean_pred])
<span class="linenr">24: </span>
</pre>
</div>
</div>
</li>

<li><a id="org9b4c383"></a>開始訓練<br />
<div class="outline-text-6" id="text-13-4-3-3-2">
<p>
x,y 分別為輸入之訓練參數資料，split=0.2 表示該批資料的 80%作為訓練用、20%作為驗證用，共執行 10 次訓練週期、verbose=2 則表示要顯示訓練過程。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">train_history</span> = model.fit(x=x_Train_normalize, y=y_Train_OneHot,
<span class="linenr">2: </span>                            validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>, epochs=<span style="color: #da8548; font-weight: bold;">10</span>, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)
</pre>
</div>

<p>
fit 完整語法如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  fit(<span style="color: #51afef;">self</span>, x=<span style="color: #a9a1e1;">None</span>, y=<span style="color: #a9a1e1;">None</span>, batch_size=<span style="color: #a9a1e1;">None</span>,
<span class="linenr">2: </span>      epochs=<span style="color: #da8548; font-weight: bold;">1</span>, verbose=<span style="color: #da8548; font-weight: bold;">1</span>, callbacks=<span style="color: #a9a1e1;">None</span>,
<span class="linenr">3: </span>      validation_split=<span style="color: #da8548; font-weight: bold;">0.0</span>, validation_data=<span style="color: #a9a1e1;">None</span>,
<span class="linenr">4: </span>      shuffle=<span style="color: #a9a1e1;">True</span>, class_weight=<span style="color: #a9a1e1;">None</span>, sample_weight=<span style="color: #a9a1e1;">None</span>,
<span class="linenr">5: </span>      initial_epoch=<span style="color: #da8548; font-weight: bold;">0</span>, steps_per_epoch=<span style="color: #a9a1e1;">None</span>, validation_steps=<span style="color: #a9a1e1;">None</span>)
</pre>
</div>
<p>
對應參數分別為：<sup><a id="fnr.14" class="footref" href="#fn.14">14</a></sup><br />
</p>
<ul class="org-ul">
<li>x：輸入數據。如果模型只有一個輸入，那麼 x 的類型是 numpy array，如果模型有多個輸入，那麼 x 的類型應當為 list，list 的元素是對應於各個輸入的 numpy array。如果模型的每個輸入都有名字，則可以傳入一個字典，將輸入名與其輸入數據對應起來。<br /></li>
<li>y：標籤，numpy array。如果模型有多個輸出，可以傳入一個 numpy array 的 list。如果模型的輸出擁有名字，則可以傳入一個字典，將輸出名與其標籤對應起來。<br /></li>
<li>batch_size：整數，指定進行梯度下降時每個 batch 包含的樣本數。訓練時一個 batch 的樣本會被計算一次梯度下降，使目標函數優化一步。<br /></li>
<li>epochs：整數，訓練終止時的 epoch 值，訓練將在達到該 epoch 值時停止，當沒有設置 initial_epoch 時，它就是訓練的總輪數，否則訓練的總輪數為 epochs - inital_epoch<br /></li>
<li>verbose：日誌顯示，0為不在標準輸出流輸出日誌信息，1為輸出進度條記錄，2為每個 epoch 輸出一行記錄<br /></li>
<li>callbacks：list，其中的元素是 keras.callbacks.Callback 的對象。這個 list 中的回調函數將會在訓練過程中的適當時機被調用，參考回調函數<br /></li>
<li>validation_split：0~1 之間的浮點數，用來指定訓練集的一定比例數據作為驗證集。驗證集將不參與訓練，並在每個 epoch 結束後測試的模型的指標，如損失函數、精確度等。注意，validation_split 的劃分在 shuffle 之後，因此如果你的數據本身是有序的，需要先手工打亂再指定 validation_split，否則可能會出現驗證集樣本不均勻。<br /></li>
<li>validation_data：形式為（X，y）或（X，y，sample_weights）的 tuple，是指定的驗證集。此參數將覆蓋 validation_spilt。<br /></li>
<li>shuffle：布爾值，表示是否在訓練過程中每個 epoch 前隨機打亂輸入樣本的順序。<br /></li>
<li>class_weight：字典，將不同的類別映射為不同的權值，該參數用來在訓練過程中調整損失函數（只能用於訓練）。該參數在處理非平衡的訓練數據（某些類的訓練樣本數很少）時，可以使得損失函數對樣本數不足的數據更加關注。<br /></li>
<li>sample_weight：權值的 numpy array，用於在訓練時調整損失函數（僅用於訓練）。可以傳遞一個 1D 的與樣本等長的向量用於對樣本進行 1 對 1 的加權，或者在面對時序數據時，傳遞一個的形式為（samples，sequence_length）的矩陣來為每個時間步上的樣本賦不同的權。這種情況下請確定在編譯模型時添加了 sample_weight_mode=&rsquo;temporal&rsquo;。<br /></li>
<li>initial_epoch: 從該參數指定的 epoch 開始訓練，在繼續之前的訓練時有用。<br /></li>
<li>steps_per_epoch: 一個 epoch 包含的步數（每一步是一個 batch 的數據送入），當使用如 TensorFlow 數據 Tensor 之類的輸入張量進行訓練時，預設的 None 代表自動分割，即數據集樣本數/batch 樣本數。<br /></li>
<li>validation_steps: 僅當 steps_per_epoch 被指定時有用，在驗證集上的 step 總數。<br /></li>
</ul>
</div>
</li>

<li><a id="org24b35b5"></a>建立、顯示訓練過程：show_train_history<br />
<div class="outline-text-6" id="text-13-4-3-3-3">
<p>
上述過程包括 accuracy 及 loss 均儲存於 train_history 變數中，可以下列程式碼將其轉變為圖表：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">2: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(train_history, train, validation):
<span class="linenr">3: </span>      plt.plot(train_history.history[train])
<span class="linenr">4: </span>      plt.plot(train_history.history[validation])
<span class="linenr">5: </span>      plt.title(<span style="color: #98be65;">'Train History'</span>)
<span class="linenr">6: </span>      plt.ylabel(train)
<span class="linenr">7: </span>      plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
<span class="linenr">8: </span>      plt.legend([<span style="color: #98be65;">'train'</span>,<span style="color: #98be65;">'validation'</span>], toc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">9: </span>      plt.show()
</pre>
</div>
</div>
</li>

<li><a id="org66c5c8c"></a>畫出 accuracy 執行結果<br />
<div class="outline-text-6" id="text-13-4-3-3-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  show_train_history(train_history,<span style="color: #98be65;">'train_acc'</span>,<span style="color: #98be65;">'val_acc'</span>)
</pre>
</div>
</div>
</li>

<li><a id="org5a35882"></a>完整執行結果<br />
<div class="outline-text-6" id="text-13-4-3-3-5">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> warnings
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  warnings.filterwarnings(<span style="color: #98be65;">'ignore'</span>)
<span class="linenr"> 5: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 6: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
<span class="linenr"> 7: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr"> 8: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 9: </span>  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
<span class="linenr">10: </span>  <span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">11: </span>  <span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">12: </span>  <span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">13: </span>  <span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">14: </span>  <span style="color: #dcaeea;">y_TrainOneHot</span> = np_utils.to_categorical(y_train_label)
<span class="linenr">15: </span>  <span style="color: #dcaeea;">y_TestOneHot</span> = np_utils.to_categorical(y_test_label)
<span class="linenr">16: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">17: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr">18: </span>  <span style="color: #dcaeea;">model</span> = Sequential()
<span class="linenr">19: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">256</span>, input_dim=<span style="color: #da8548; font-weight: bold;">784</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">20: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">21: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">===&#36914;&#34892;&#35347;&#32244;===</span>
<span class="linenr">22: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>, optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">23: </span>  <span style="color: #dcaeea;">train_history</span> = model.fit(x=x_Train_normalize, y=y_TrainOneHot,
<span class="linenr">24: </span>                                validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>, epochs=<span style="color: #da8548; font-weight: bold;">20</span>, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">25: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">26: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(train_history, train, validation):
<span class="linenr">27: </span>      plt.plot(train_history.history[train])
<span class="linenr">28: </span>      plt.plot(train_history.history[validation])
<span class="linenr">29: </span>      plt.title(<span style="color: #98be65;">'Train History'</span>)
<span class="linenr">30: </span>      plt.ylabel(train)
<span class="linenr">31: </span>      plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
<span class="linenr">32: </span>      plt.legend([<span style="color: #98be65;">'train'</span>,<span style="color: #98be65;">'validation'</span>], loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">33: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show() # for jupyter notebook</span>
<span class="linenr">34: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#20462;&#25913;for console run</span>
<span class="linenr">35: </span>      <span style="color: #dcaeea;">img</span> = plt.plot()
<span class="linenr">36: </span>      <span style="color: #51afef;">return</span> img
<span class="linenr">37: </span>
<span class="linenr">38: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;acc</span>
<span class="linenr">39: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;val_accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;val_acc</span>
<span class="linenr">40: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">show_train_history(train_history, 'accuracy', 'val_accuracy')</span>
<span class="linenr">41: </span>  <span style="color: #dcaeea;">img</span> = show_train_history(train_history, <span style="color: #98be65;">'accuracy'</span>, <span style="color: #98be65;">'val_accuracy'</span>)
<span class="linenr">42: </span>  plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-1.png"</span>)
<span class="linenr">43: </span>  <span style="color: #dcaeea;">img</span> = show_train_history(train_history, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>)
<span class="linenr">44: </span>  plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-2.png"</span>) <span style="color: #5B6268;">#</span>
<span class="linenr">45: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#28204;&#35430;&#36039;&#26009;&#35413;&#20272;&#31934;&#30906;&#29575;</span>
<span class="linenr">46: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">save</span>
<span class="linenr">47: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'test before save model: '</span>, model.predict(x_Test[<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">5</span>]))
<span class="linenr">48: </span>
<span class="linenr">49: </span>  scores = model.evaluate(x_Test_normalize, y_TestOneHot)
<span class="linenr">50: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'accuracy'</span>,scores)
<span class="linenr">51: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'accuracy'</span>,scores[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">52: </span>
<span class="linenr">53: </span>  model.save(<span style="color: #98be65;">'Keras_MNist_model.h5'</span>)   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">HDF5 file, you have to pip3 install h5py if don't have it</span>
<span class="linenr">54: </span>  <span style="color: #51afef;">del</span> model  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">deletes the existing model</span>
</pre>
</div>

<pre class="example">
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
 - 9s - loss: 0.2697 - accuracy: 0.9225 - val_loss: 0.1326 - val_accuracy: 0.9611
Epoch 2/20
 - 8s - loss: 0.1075 - accuracy: 0.9683 - val_loss: 0.1038 - val_accuracy: 0.9694
Epoch 3/20
 - 8s - loss: 0.0710 - accuracy: 0.9777 - val_loss: 0.0929 - val_accuracy: 0.9726
Epoch 4/20
 - 8s - loss: 0.0513 - accuracy: 0.9843 - val_loss: 0.0826 - val_accuracy: 0.9762
Epoch 5/20
 - 8s - loss: 0.0377 - accuracy: 0.9882 - val_loss: 0.0786 - val_accuracy: 0.9757
Epoch 6/20
 - 8s - loss: 0.0262 - accuracy: 0.9920 - val_loss: 0.0790 - val_accuracy: 0.9787
Epoch 7/20
 - 8s - loss: 0.0202 - accuracy: 0.9938 - val_loss: 0.0863 - val_accuracy: 0.9770
Epoch 8/20
 - 8s - loss: 0.0161 - accuracy: 0.9948 - val_loss: 0.0873 - val_accuracy: 0.9779
Epoch 9/20
 - 8s - loss: 0.0138 - accuracy: 0.9957 - val_loss: 0.0984 - val_accuracy: 0.9753
Epoch 10/20
 - 8s - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.1103 - val_accuracy: 0.9750
Epoch 11/20
 - 8s - loss: 0.0114 - accuracy: 0.9960 - val_loss: 0.0920 - val_accuracy: 0.9787
Epoch 12/20
 - 8s - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.1012 - val_accuracy: 0.9777
Epoch 13/20
 - 8s - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.1182 - val_accuracy: 0.9758
Epoch 14/20
 - 8s - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.0993 - val_accuracy: 0.9799
Epoch 15/20
 - 8s - loss: 0.0072 - accuracy: 0.9976 - val_loss: 0.1104 - val_accuracy: 0.9786
Epoch 16/20
 - 8s - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.1036 - val_accuracy: 0.9797
Epoch 17/20
 - 8s - loss: 0.0061 - accuracy: 0.9984 - val_loss: 0.1288 - val_accuracy: 0.9767
Epoch 18/20
 - 8s - loss: 0.0061 - accuracy: 0.9981 - val_loss: 0.1279 - val_accuracy: 0.9773
Epoch 19/20
 - 9s - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.1226 - val_accuracy: 0.9763
Epoch 20/20
 - 9s - loss: 0.0044 - accuracy: 0.9984 - val_loss: 0.1365 - val_accuracy: 0.9772
test before save model:  [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]

   32/10000 [..............................] - ETA: 0s
 1664/10000 [===&gt;..........................] - ETA: 0s
 3168/10000 [========&gt;.....................] - ETA: 0s
 4832/10000 [=============&gt;................] - ETA: 0s
 6432/10000 [==================&gt;...........] - ETA: 0s
 8032/10000 [=======================&gt;......] - ETA: 0s
 9760/10000 [============================&gt;.] - ETA: 0s
10000/10000 [==============================] - 0s 31us/step
accuracy [0.11840327789002267, 0.9767000079154968]
accuracy 0.9767000079154968
</pre>


<div id="orgc23aa02" class="figure">
<p><img src="images/Keras-MNist-Train-1.png" alt="Keras-MNist-Train-1.png" /><br />
</p>
<p><span class="figure-number">Figure 103: </span>Keras Mnist Model 訓練#1: accuracy</p>
</div>

<p>
圖<a href="#orgc23aa02">103</a>為執行 10 次週期後的預測精確度變化，可以看出以下現象：<br />
</p>
<ol class="org-ol">
<li>訓練與驗證的精確率均隨訓練週期增加而提高<br /></li>
<li>訓練精確度較驗證精確度高，原因是用來評估訓練精確率的資料已在訓練階段用過了；而用來評做驗證精確率的資料則否；這就類似，考試時考學過的練習題，學生得分較高。<br /></li>
<li>驗證精確率雖然低，但較符合現實情況，即，考試時考學生沒有做過的題目。<br /></li>
<li>如果訓練精確率持續增高，但驗證精率卻無法提升，可能是出現過度擬合(overfitting)的現象。,<br /></li>
</ol>

<div id="orgd3fee45" class="figure">
<p><img src="images/Keras-MNist-Train-2.png" alt="Keras-MNist-Train-2.png" /><br />
</p>
<p><span class="figure-number">Figure 104: </span>Keras Mnist Model 訓練#1: lost function</p>
</div>

<p>
由圖<a href="#orgd3fee45">104</a>亦可看出，訓練誤差與驗證誤差亦隨週期增加而降低，且訓練襄差最終低於驗證誤差。<br />
</p>

<p>
訓練完成後，再以測試資料來評估模型準確率。<br />
</p>
</div>
</li>
</ol>
</li>

<li><a id="orgc6d45ba"></a>進行預測<br />
<div class="outline-text-5" id="text-13-4-3-4">
<p>
模型在訓練、驗證、測試後，即可以此訓練完之模型進行預測，預測方式如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 4: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> load_model
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 9: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Define func</span>
<span class="linenr">10: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_images_labels_prediction</span>(imgname, images, labels, prediction, idx, num = <span style="color: #da8548; font-weight: bold;">10</span>):
<span class="linenr">11: </span>        <span style="color: #dcaeea;">fig</span> = plt.gcf()
<span class="linenr">12: </span>        fig.set_size_inches(<span style="color: #da8548; font-weight: bold;">8</span>,<span style="color: #da8548; font-weight: bold;">4</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35373;&#23450;&#39023;&#31034;&#22294;&#24418;&#22823;&#23567;</span>
<span class="linenr">13: </span>        <span style="color: #51afef;">if</span> num&gt;<span style="color: #da8548; font-weight: bold;">25</span>: <span style="color: #dcaeea;">num</span>=<span style="color: #da8548; font-weight: bold;">25</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#36039;&#26009;&#31558;&#25976;&#19978;&#38480;</span>
<span class="linenr">14: </span>        <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">0</span>, num):
<span class="linenr">15: </span>            <span style="color: #dcaeea;">ax</span>=plt.subplot(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">1</span>+i) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27492;&#34389;&#39023;&#31034;10&#31558;&#22294;&#24418;&#65292;2*5&#20491;</span>
<span class="linenr">16: </span>            ax.imshow(images[idx], cmap=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr">17: </span>            <span style="color: #dcaeea;">title</span>= <span style="color: #98be65;">"label="</span> +<span style="color: #c678dd;">str</span>(labels[idx]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21152;&#20837;&#23376;&#22294;&#24418;title</span>
<span class="linenr">18: </span>            <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(prediction)&gt;<span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr">19: </span>                <span style="color: #dcaeea;">title</span>+=<span style="color: #98be65;">",predict="</span>+<span style="color: #c678dd;">str</span>(prediction[idx]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27161;&#38988;title&#21152;&#20837;&#38928;&#28204;&#32080;&#26524;</span>
<span class="linenr">20: </span>            ax.set_title(title,fontsize=<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">21: </span>            ax.set_xticks([]);ax.set_yticks([]) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#19981;&#39023;&#31034;&#21051;&#24230;</span>
<span class="linenr">22: </span>            <span style="color: #dcaeea;">idx</span>+=<span style="color: #da8548; font-weight: bold;">1</span> 
<span class="linenr">23: </span>        <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">24: </span>        <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.plot()</span>
<span class="linenr">25: </span>        plt.savefig(imgname+<span style="color: #98be65;">".png"</span>)
<span class="linenr">26: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36617;&#20837;&#20786;&#23384;&#20043;&#27169;&#22411;</span>
<span class="linenr">27: </span>  <span style="color: #dcaeea;">model</span> = load_model(<span style="color: #98be65;">'Keras_MNist_model.h5'</span>)
<span class="linenr">28: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;340-349&#20849;10&#31558;&#36039;&#26009;</span>
<span class="linenr">29: </span>  <span style="color: #dcaeea;">prediction</span> = model.predict_classes(x_Test)
<span class="linenr">30: </span>  plot_images_labels_prediction(<span style="color: #98be65;">'Keras-MNist-Train-3'</span>,x_test_image, y_test_label,
<span class="linenr">31: </span>                                prediction, idx=<span style="color: #da8548; font-weight: bold;">340</span>)
<span class="linenr">32: </span>
</pre>
</div>
<p>
<img src="images/Keras-MNist-Train-3.png" alt="Keras-MNist-Train-3.png" /><br />
上述程式碼將訓練好後儲存的模型取出，拿 10 筆記錄去預測，發現第一筆有誤（真實值 label 為 5、預測值為 3）。<br />
</p>
</div>
</li>

<li><a id="orgc9912ec"></a>顯示混淆矩陣(confusion matrix)<br />
<div class="outline-text-5" id="text-13-4-3-5">
<p>
若想進一步得知哪些數字容易被混淆，可以使用混淆矩陣(confustion matrix, 也稱為誤差矩陣 error matrix)。實務上可以利用 pandas crosstab 來建立，程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 4: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> load_model
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #dcaeea;">model</span> = load_model(<span style="color: #98be65;">'Keras_MNist_model.h5'</span>)
<span class="linenr">12: </span>  <span style="color: #dcaeea;">prediction</span> = model.predict_classes(x_Test)
<span class="linenr">13: </span>
<span class="linenr">14: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">15: </span>  <span style="color: #dcaeea;">confuse</span> = pd.crosstab(y_test_label, prediction, rownames=[<span style="color: #98be65;">'label'</span>], colnames=[<span style="color: #98be65;">'predict'</span>])
<span class="linenr">16: </span>  <span style="color: #51afef;">print</span>(confuse)
</pre>
</div>

<pre class="example">
  predict    0     1     2    3    4    5    6     7    8    9
  label                                                       
  0        972     1     2    0    0    0    2     1    2    0
  1          0  1126     3    1    0    0    2     1    2    0
  2          7     1  1005    4    2    0    2     7    4    0
  3          1     0     1  999    0    3    0     3    2    1
  4          2     0     3    1  958    1    3     3    1   10
  5          5     0     1   29    1  842    5     3    5    1
  6          7     3     1    1    3    2  941     0    0    0
  7          2     3     5    3    1    0    0  1009    1    4
  8         10     0     4   10    2    1    3     5  938    1
  9          5     5     0    4    9    0    0    11    0  975
</pre>

<p>
由輸出結果可以得知，5很容易被誤判為 3(共 29 次)。若想進一步得知到底有那幾張圖為上述狀況，則可以加入限制條件，程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 4: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> load_model
<span class="linenr"> 5: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">model</span> = load_model(<span style="color: #98be65;">'Keras_MNist_model.h5'</span>)
<span class="linenr">11: </span>  <span style="color: #dcaeea;">prediction</span> = model.predict_classes(x_Test)
<span class="linenr">12: </span>  <span style="color: #dcaeea;">df</span> = pd.DataFrame({<span style="color: #98be65;">'label'</span>: y_test_label, <span style="color: #98be65;">'predict'</span>:prediction})
<span class="linenr">13: </span>  <span style="color: #51afef;">print</span>(df[(df.label==<span style="color: #da8548; font-weight: bold;">5</span>)&amp;(df.predict==<span style="color: #da8548; font-weight: bold;">3</span>)])
<span class="linenr">14: </span>
<span class="linenr">15: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_image</span>(imgname, image):
<span class="linenr">16: </span>      <span style="color: #dcaeea;">fig</span> = plt.gcf() <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23450;&#35373;&#22294;&#24418;&#22823;&#23567;</span>
<span class="linenr">17: </span>      fig.set_size_inches(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">18: </span>      plt.imshow(image, cmap=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">cmap&#21443;&#25976;&#35373;&#23450;&#28858;binary&#20197;&#40657;&#30333;&#28784;&#38542;&#39023;&#31034;</span>
<span class="linenr">19: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show() #for jupyter or colab</span>
<span class="linenr">20: </span>      plt.plot()
<span class="linenr">21: </span>      plt.savefig(imgname+<span style="color: #98be65;">".png"</span>)
<span class="linenr">22: </span>
<span class="linenr">23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#31532;340&#31558;&#36039;&#26009;</span>
<span class="linenr">24: </span>  plot_image(<span style="color: #98be65;">'Keras-MNist-Train-4'</span>, x_test_image[<span style="color: #da8548; font-weight: bold;">340</span>])
<span class="linenr">25: </span>
</pre>
</div>

<pre class="example">
      label  predict
340       5        3
1003      5        3
1082      5        3
1393      5        3
2035      5        3
2291      5        3
2369      5        3
2526      5        3
2597      5        3
2604      5        3
2810      5        3
2970      5        3
3117      5        3
3414      5        3
4255      5        3
4271      5        3
4360      5        3
5874      5        3
5891      5        3
5913      5        3
5937      5        3
5972      5        3
5982      5        3
5985      5        3
6028      5        3
6042      5        3
6043      5        3
6598      5        3
9982      5        3
</pre>

<div id="orgf5f6e58" class="figure">
<p><img src="images/Keras-MNist-Train-4.png" alt="Keras-MNist-Train-4.png" /><br />
</p>
<p><span class="figure-number">Figure 105: </span>Keras Mnist Model 訓練#1: error sample</p>
</div>
</div>
</li>
</ol>
</li>
</ol>
</div>

<div id="outline-container-org33a7c3a" class="outline-3">
<h3 id="org33a7c3a"><span class="section-number-3">13.5</span> 強化 MLP 辨識 solution #1: 增加隠藏層神經元數</h3>
<div class="outline-text-3" id="text-13-5">
<p>
為了增加 MLP 的準確率，其中一種方法可以增加隠藏層的神經元數至 1000 個，程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
<span class="linenr"> 3: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr"> 4: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 5: </span>  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">y_TrainOneHot</span> = np_utils.to_categorical(y_train_label)
<span class="linenr">11: </span>  <span style="color: #dcaeea;">y_TestOneHot</span> = np_utils.to_categorical(y_test_label)
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">13: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr">14: </span>  <span style="color: #dcaeea;">model</span> = Sequential()
<span class="linenr">15: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#22686;&#21152;&#31070;&#32147;&#20803;&#25976;</span>
<span class="linenr">16: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">1000</span>, input_dim=<span style="color: #da8548; font-weight: bold;">784</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">17: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">18: </span>  <span style="color: #51afef;">print</span>(model.summary())
<span class="linenr">19: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#26597;&#30475;&#35347;&#32244;&#36942;&#31243;&#21450;&#32080;&#26524;</span>
<span class="linenr">20: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>, optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">21: </span>  <span style="color: #dcaeea;">train_history</span> = model.fit(x=x_Train_normalize, y=y_TrainOneHot,
<span class="linenr">22: </span>                                validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>, epochs=<span style="color: #da8548; font-weight: bold;">10</span>, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">23: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">24: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(train_history, train, validation):
<span class="linenr">25: </span>      plt.plot(train_history.history[train])
<span class="linenr">26: </span>      plt.plot(train_history.history[validation])
<span class="linenr">27: </span>      plt.title(<span style="color: #98be65;">'Train History'</span>)
<span class="linenr">28: </span>      plt.ylabel(train)
<span class="linenr">29: </span>      plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
<span class="linenr">30: </span>      plt.legend([<span style="color: #98be65;">'train'</span>,<span style="color: #98be65;">'validation'</span>], loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">31: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show() # for jupyter notebook</span>
<span class="linenr">32: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#20462;&#25913;for console run</span>
<span class="linenr">33: </span>      <span style="color: #dcaeea;">img</span> = plt.plot()
<span class="linenr">34: </span>      <span style="color: #51afef;">return</span> img
<span class="linenr">35: </span>
<span class="linenr">36: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;acc</span>
<span class="linenr">37: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;val_accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;val_acc</span>
<span class="linenr">38: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">show_train_history(train_history, 'accuracy', 'val_accuracy')</span>
<span class="linenr">39: </span>  <span style="color: #dcaeea;">img</span> = show_train_history(train_history, <span style="color: #98be65;">'accuracy'</span>, <span style="color: #98be65;">'val_accuracy'</span>)
<span class="linenr">40: </span>  plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-5.png"</span>)
<span class="linenr">41: </span>  <span style="color: #dcaeea;">img</span> = show_train_history(train_history, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>)
<span class="linenr">42: </span>  plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-6.png"</span>) <span style="color: #5B6268;">#</span>
<span class="linenr">43: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#28204;&#35430;&#36039;&#26009;&#35413;&#20272;&#31934;&#30906;&#29575;</span>
<span class="linenr">44: </span>  <span style="color: #dcaeea;">scores</span> = model.evaluate(x_Test_normalize, y_TestOneHot)
<span class="linenr">45: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'accuracy'</span>,scores[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">46: </span>
<span class="linenr">47: </span>  model.save(<span style="color: #98be65;">'Keras_MNist_model-1.h5'</span>) 
<span class="linenr">48: </span>  <span style="color: #51afef;">del</span> model  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">deletes the existing model</span>
</pre>
</div>

<pre class="example">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 1000)              785000    
_________________________________________________________________
dense_2 (Dense)              (None, 10)                10010     
=================================================================
Total params: 795,010
Trainable params: 795,010
Non-trainable params: 0
_________________________________________________________________
None
Train on 48000 samples, validate on 12000 samples
Epoch 1/10
 - 22s - loss: 0.2047 - accuracy: 0.9385 - val_loss: 0.1026 - val_accuracy: 0.9691
Epoch 2/10
 - 22s - loss: 0.0797 - accuracy: 0.9754 - val_loss: 0.0866 - val_accuracy: 0.9734
Epoch 3/10
 - 23s - loss: 0.0504 - accuracy: 0.9833 - val_loss: 0.0917 - val_accuracy: 0.9737
Epoch 4/10
 - 22s - loss: 0.0356 - accuracy: 0.9888 - val_loss: 0.0820 - val_accuracy: 0.9772
Epoch 5/10
 - 23s - loss: 0.0259 - accuracy: 0.9914 - val_loss: 0.0763 - val_accuracy: 0.9785
Epoch 6/10
 - 22s - loss: 0.0201 - accuracy: 0.9931 - val_loss: 0.0898 - val_accuracy: 0.9792
Epoch 7/10
 - 23s - loss: 0.0171 - accuracy: 0.9942 - val_loss: 0.0934 - val_accuracy: 0.9789
Epoch 8/10
 - 22s - loss: 0.0148 - accuracy: 0.9950 - val_loss: 0.0965 - val_accuracy: 0.9790
Epoch 9/10
 - 22s - loss: 0.0148 - accuracy: 0.9951 - val_loss: 0.1181 - val_accuracy: 0.9771
Epoch 10/10
 - 22s - loss: 0.0117 - accuracy: 0.9961 - val_loss: 0.1280 - val_accuracy: 0.9741

   32/10000 [..............................] - ETA: 0s
 1120/10000 [==&gt;...........................] - ETA: 0s
 2112/10000 [=====&gt;........................] - ETA: 0s
 3008/10000 [========&gt;.....................] - ETA: 0s
 4000/10000 [===========&gt;..................] - ETA: 0s
 5152/10000 [==============&gt;...............] - ETA: 0s
 6208/10000 [=================&gt;............] - ETA: 0s
 7296/10000 [====================&gt;.........] - ETA: 0s
 8448/10000 [========================&gt;.....] - ETA: 0s
 9568/10000 [===========================&gt;..] - ETA: 0s
10000/10000 [==============================] - 0s 48us/step
accuracy 0.9760000109672546
</pre>


<div id="org0853ccd" class="figure">
<p><img src="images/Keras-MNist-Train-5.png" alt="Keras-MNist-Train-5.png" /><br />
</p>
<p><span class="figure-number">Figure 106: </span>Keras Mnist Model 訓練#2: accuracy</p>
</div>

<div id="orge6a5a07" class="figure">
<p><img src="images/Keras-MNist-Train-6.png" alt="Keras-MNist-Train-6.png" /><br />
</p>
<p><span class="figure-number">Figure 107: </span>Keras Mnist Model 訓練#2: loss information</p>
</div>

<p>
將神經元數增加至 1000 後，精確率由 0.97 提升至 0.98。但測驗準確率仍未提升，可見 overfitting 仍然嚴重。<br />
</p>
</div>
</div>

<div id="outline-container-orgff64d1e" class="outline-3">
<h3 id="orgff64d1e"><span class="section-number-3">13.6</span> 強化 MLP 辨識 solution #2: 加入 DropOut 以避免 overfitting</h3>
<div class="outline-text-3" id="text-13-6">
<p>
為解決 Overfitting 問題，此處再加入 Dropout(0.5)指令，其功能為在每次訓練迭代時，會隨機地在隱藏層中放棄 50%的神經元，以避免 overfitting，程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr"> 5: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 6: </span>  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">11: </span>  <span style="color: #dcaeea;">y_TrainOneHot</span> = np_utils.to_categorical(y_train_label)
<span class="linenr">12: </span>  <span style="color: #dcaeea;">y_TestOneHot</span> = np_utils.to_categorical(y_test_label)
<span class="linenr">13: </span>
<span class="linenr">14: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">15: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr">16: </span>  <span style="color: #dcaeea;">model</span> = Sequential()
<span class="linenr">17: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#22686;&#21152;&#31070;&#32147;&#20803;&#25976;</span>
<span class="linenr">18: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">1000</span>, input_dim=<span style="color: #da8548; font-weight: bold;">784</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">19: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#21152;&#20837;Dropout</span>
<span class="linenr">20: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dropout
<span class="linenr">21: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr">22: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#21152;&#20837;&#36664;&#20986;&#23652;</span>
<span class="linenr">23: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">24: </span>  <span style="color: #51afef;">print</span>(model.summary())
<span class="linenr">25: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#35347;&#32244;&#27169;&#22411;&#12289;&#26597;&#30475;&#32080;&#26524;</span>
<span class="linenr">26: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>, optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">27: </span>  <span style="color: #dcaeea;">train_history</span> = model.fit(x=x_Train_normalize, y=y_TrainOneHot,
<span class="linenr">28: </span>                                validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>, epochs=<span style="color: #da8548; font-weight: bold;">10</span>, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">29: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">30: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(train_history, train, validation):
<span class="linenr">31: </span>      plt.plot(train_history.history[train])
<span class="linenr">32: </span>      plt.plot(train_history.history[validation])
<span class="linenr">33: </span>      plt.title(<span style="color: #98be65;">'Train History'</span>)
<span class="linenr">34: </span>      plt.ylabel(train)
<span class="linenr">35: </span>      plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
<span class="linenr">36: </span>      plt.legend([<span style="color: #98be65;">'train'</span>,<span style="color: #98be65;">'validation'</span>], loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">37: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show() # for jupyter notebook</span>
<span class="linenr">38: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#20462;&#25913;for console run</span>
<span class="linenr">39: </span>      <span style="color: #dcaeea;">img</span> = plt.plot()
<span class="linenr">40: </span>      <span style="color: #51afef;">return</span> img
<span class="linenr">41: </span>
<span class="linenr">42: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;acc</span>
<span class="linenr">43: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;val_accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;val_acc</span>
<span class="linenr">44: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">show_train_history(train_history, 'accuracy', 'val_accuracy')</span>
<span class="linenr">45: </span>  <span style="color: #dcaeea;">img</span> = show_train_history(train_history, <span style="color: #98be65;">'accuracy'</span>, <span style="color: #98be65;">'val_accuracy'</span>)
<span class="linenr">46: </span>  plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-7.png"</span>)
<span class="linenr">47: </span>  <span style="color: #dcaeea;">img</span> = show_train_history(train_history, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>)
<span class="linenr">48: </span>  plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-8.png"</span>) <span style="color: #5B6268;">#</span>
<span class="linenr">49: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#28204;&#35430;&#36039;&#26009;&#35413;&#20272;&#31934;&#30906;&#29575;</span>
<span class="linenr">50: </span>  <span style="color: #dcaeea;">scores</span> = model.evaluate(x_Test_normalize, y_TestOneHot)
<span class="linenr">51: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'accuracy='</span>,scores[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">52: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36914;&#34892;&#38928;&#28204;</span>
<span class="linenr">53: </span>  <span style="color: #dcaeea;">prediction</span>=model.predict_classes(x_Test)
<span class="linenr">54: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36664;&#20986;confuse mqtrix</span>
<span class="linenr">55: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">56: </span>  pd.crosstab(y_test_label,prediction,
<span class="linenr">57: </span>              rownames=[<span style="color: #98be65;">'label'</span>],colnames=[<span style="color: #98be65;">'predict'</span>])
<span class="linenr">58: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20786;&#23384;&#35347;&#32244;&#22909;&#30340;&#27169;&#24335;</span>
<span class="linenr">59: </span>  model.save(<span style="color: #98be65;">'Keras_MNist_model-2.h5'</span>) 
<span class="linenr">60: </span>  <span style="color: #51afef;">del</span> model  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">deletes the existing model</span>
</pre>
</div>

<pre class="example">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 1000)              785000    
_________________________________________________________________
dropout_1 (Dropout)          (None, 1000)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                10010     
=================================================================
Total params: 795,010
Trainable params: 795,010
Non-trainable params: 0
_________________________________________________________________
None
Train on 48000 samples, validate on 12000 samples
Epoch 1/10
 - 25s - loss: 0.2673 - accuracy: 0.9171 - val_loss: 0.1145 - val_accuracy: 0.9659
Epoch 2/10
 - 25s - loss: 0.1319 - accuracy: 0.9581 - val_loss: 0.1046 - val_accuracy: 0.9689
Epoch 3/10
 - 26s - loss: 0.1008 - accuracy: 0.9686 - val_loss: 0.1004 - val_accuracy: 0.9690
Epoch 4/10
 - 26s - loss: 0.0884 - accuracy: 0.9719 - val_loss: 0.0826 - val_accuracy: 0.9763
Epoch 5/10
 - 26s - loss: 0.0764 - accuracy: 0.9752 - val_loss: 0.0858 - val_accuracy: 0.9763
Epoch 6/10
 - 25s - loss: 0.0703 - accuracy: 0.9779 - val_loss: 0.0826 - val_accuracy: 0.9779
Epoch 7/10
 - 25s - loss: 0.0617 - accuracy: 0.9811 - val_loss: 0.0849 - val_accuracy: 0.9772
Epoch 8/10
 - 25s - loss: 0.0597 - accuracy: 0.9817 - val_loss: 0.0816 - val_accuracy: 0.9795
Epoch 9/10
 - 26s - loss: 0.0529 - accuracy: 0.9829 - val_loss: 0.0878 - val_accuracy: 0.9787
Epoch 10/10
 - 28s - loss: 0.0516 - accuracy: 0.9838 - val_loss: 0.0833 - val_accuracy: 0.9805

   32/10000 [..............................] - ETA: 0s
 1088/10000 [==&gt;...........................] - ETA: 0s
 2016/10000 [=====&gt;........................] - ETA: 0s
 2848/10000 [=======&gt;......................] - ETA: 0s
 3904/10000 [==========&gt;...................] - ETA: 0s
 5024/10000 [==============&gt;...............] - ETA: 0s
 6144/10000 [=================&gt;............] - ETA: 0s
 7264/10000 [====================&gt;.........] - ETA: 0s
 8384/10000 [========================&gt;.....] - ETA: 0s
 9472/10000 [===========================&gt;..] - ETA: 0s
10000/10000 [==============================] - 0s 48us/step
accuracy= 0.9815000295639038
</pre>

<p>
由訓練過程可以看出，驗證精確率(0.9805)已接近訓練精確率(0.9838)，可見已改善了 overfitting 問題。<br />
</p>


<div id="orge0e0d55" class="figure">
<p><img src="images/Keras-MNist-Train-7.png" alt="Keras-MNist-Train-7.png" /><br />
</p>
<p><span class="figure-number">Figure 108: </span>Keras Mnist Model 訓練#3: accuracy</p>
</div>

<div id="org0e7e8b4" class="figure">
<p><img src="images/Keras-MNist-Train-8.png" alt="Keras-MNist-Train-8.png" /><br />
</p>
<p><span class="figure-number">Figure 109: </span>Keras Mnist Model 訓練#3: loss information</p>
</div>

<p>
由圖<a href="#orge0e0d55">108</a>也可看出，驗證精確率已隨訓練週期提高，驗證誤差也隨訓練週期降低。<br />
</p>
</div>
</div>

<div id="outline-container-org969585d" class="outline-3">
<h3 id="org969585d"><span class="section-number-3">13.7</span> 強化 MLP 辨識 solution #3: 增加隱藏層層數</h3>
<div class="outline-text-3" id="text-13-7">
<p>
本例 MLP 模型的預測能力仍有改善空間：增加隠藏層層數。以下程式碼將隠藏層數提高至 2 層：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr"> 5: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 6: </span>  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">11: </span>  <span style="color: #dcaeea;">y_TrainOneHot</span> = np_utils.to_categorical(y_train_label)
<span class="linenr">12: </span>  <span style="color: #dcaeea;">y_TestOneHot</span> = np_utils.to_categorical(y_test_label)
<span class="linenr">13: </span>
<span class="linenr">14: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">15: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr">16: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dropout
<span class="linenr">17: </span>
<span class="linenr">18: </span>  <span style="color: #dcaeea;">model</span> = Sequential()
<span class="linenr">19: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#22686;&#21152;&#31532;1&#23652;&#38577;&#34255;&#23652;&#31070;&#32147;&#20803;&#25976;</span>
<span class="linenr">20: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">1000</span>, input_dim=<span style="color: #da8548; font-weight: bold;">784</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">21: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#21152;&#20837;Dropout</span>
<span class="linenr">22: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr">23: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#22686;&#21152;&#31532;2&#23652;&#38577;&#34255;&#23652;</span>
<span class="linenr">24: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">1000</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">25: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#21152;&#20837;&#31532;2&#23652;Dropout</span>
<span class="linenr">26: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr">27: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#21152;&#20837;&#36664;&#20986;&#23652;</span>
<span class="linenr">28: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">29: </span>  <span style="color: #51afef;">print</span>(model.summary())
<span class="linenr">30: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#35347;&#32244;&#27169;&#22411;&#12289;&#26597;&#30475;&#32080;&#26524;</span>
<span class="linenr">31: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>, optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">32: </span>  <span style="color: #dcaeea;">train_history</span> = model.fit(x=x_Train_normalize, y=y_TrainOneHot,
<span class="linenr">33: </span>                                validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>, epochs=<span style="color: #da8548; font-weight: bold;">10</span>, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">34: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">35: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(train_history, train, validation):
<span class="linenr">36: </span>      plt.plot(train_history.history[train])
<span class="linenr">37: </span>      plt.plot(train_history.history[validation])
<span class="linenr">38: </span>      plt.title(<span style="color: #98be65;">'Train History'</span>)
<span class="linenr">39: </span>      plt.ylabel(train)
<span class="linenr">40: </span>      plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
<span class="linenr">41: </span>      plt.legend([<span style="color: #98be65;">'train'</span>,<span style="color: #98be65;">'validation'</span>], loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">42: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show() # for jupyter notebook</span>
<span class="linenr">43: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#20462;&#25913;for console run</span>
<span class="linenr">44: </span>      <span style="color: #dcaeea;">img</span> = plt.plot()
<span class="linenr">45: </span>      <span style="color: #51afef;">return</span> img
<span class="linenr">46: </span>
<span class="linenr">47: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;acc</span>
<span class="linenr">48: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;val_accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;val_acc</span>
<span class="linenr">49: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">show_train_history(train_history, 'accuracy', 'val_accuracy')</span>
<span class="linenr">50: </span>  <span style="color: #dcaeea;">img</span> = show_train_history(train_history, <span style="color: #98be65;">'accuracy'</span>, <span style="color: #98be65;">'val_accuracy'</span>)
<span class="linenr">51: </span>  plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-9.png"</span>)
<span class="linenr">52: </span>  <span style="color: #dcaeea;">img</span> = show_train_history(train_history, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>)
<span class="linenr">53: </span>  plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-a.png"</span>) <span style="color: #5B6268;">#</span>
<span class="linenr">54: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#28204;&#35430;&#36039;&#26009;&#35413;&#20272;&#31934;&#30906;&#29575;</span>
<span class="linenr">55: </span>  <span style="color: #dcaeea;">scores</span> = model.evaluate(x_Test_normalize, y_TestOneHot)
<span class="linenr">56: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'accuracy='</span>,scores[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">57: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36914;&#34892;&#38928;&#28204;</span>
<span class="linenr">58: </span>  <span style="color: #dcaeea;">prediction</span>=model.predict_classes(x_Test)
<span class="linenr">59: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36664;&#20986;confuse mqtrix</span>
<span class="linenr">60: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">61: </span>  pd.crosstab(y_test_label,prediction,
<span class="linenr">62: </span>              rownames=[<span style="color: #98be65;">'label'</span>],colnames=[<span style="color: #98be65;">'predict'</span>])
<span class="linenr">63: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20786;&#23384;&#35347;&#32244;&#22909;&#30340;&#27169;&#24335;</span>
<span class="linenr">64: </span>  model.save(<span style="color: #98be65;">'Keras_MNist_model-3.h5'</span>) 
<span class="linenr">65: </span>  <span style="color: #51afef;">del</span> model  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">deletes the existing model</span>
</pre>
</div>

<pre class="example">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 1000)              785000    
_________________________________________________________________
dropout_1 (Dropout)          (None, 1000)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 1000)              1001000   
_________________________________________________________________
dropout_2 (Dropout)          (None, 1000)              0         
_________________________________________________________________
dense_3 (Dense)              (None, 10)                10010     
=================================================================
Total params: 1,796,010
Trainable params: 1,796,010
Non-trainable params: 0
_________________________________________________________________
None
Train on 48000 samples, validate on 12000 samples
Epoch 1/10
 - 51s - loss: 0.3197 - accuracy: 0.9040 - val_loss: 0.1234 - val_accuracy: 0.9613
Epoch 2/10
 - 53s - loss: 0.1753 - accuracy: 0.9463 - val_loss: 0.1277 - val_accuracy: 0.9635
Epoch 3/10
 - 50s - loss: 0.1493 - accuracy: 0.9557 - val_loss: 0.0965 - val_accuracy: 0.9725
Epoch 4/10
 - 49s - loss: 0.1321 - accuracy: 0.9616 - val_loss: 0.1069 - val_accuracy: 0.9693
Epoch 5/10
 - 50s - loss: 0.1265 - accuracy: 0.9632 - val_loss: 0.1008 - val_accuracy: 0.9714
Epoch 6/10
 - 50s - loss: 0.1204 - accuracy: 0.9658 - val_loss: 0.0802 - val_accuracy: 0.9772
Epoch 7/10
 - 50s - loss: 0.1086 - accuracy: 0.9681 - val_loss: 0.0860 - val_accuracy: 0.9768
Epoch 8/10
 - 50s - loss: 0.1018 - accuracy: 0.9709 - val_loss: 0.0904 - val_accuracy: 0.9755
Epoch 9/10
 - 50s - loss: 0.1055 - accuracy: 0.9708 - val_loss: 0.0878 - val_accuracy: 0.9768
Epoch 10/10
 - 50s - loss: 0.0973 - accuracy: 0.9739 - val_loss: 0.0886 - val_accuracy: 0.9771

   32/10000 [..............................] - ETA: 0s
  768/10000 [=&gt;............................] - ETA: 0s
 1472/10000 [===&gt;..........................] - ETA: 0s
 2016/10000 [=====&gt;........................] - ETA: 0s
 2656/10000 [======&gt;.......................] - ETA: 0s
 3360/10000 [=========&gt;....................] - ETA: 0s
 4032/10000 [===========&gt;..................] - ETA: 0s
 4736/10000 [=============&gt;................] - ETA: 0s
 5472/10000 [===============&gt;..............] - ETA: 0s
 6176/10000 [=================&gt;............] - ETA: 0s
 6912/10000 [===================&gt;..........] - ETA: 0s
 7648/10000 [=====================&gt;........] - ETA: 0s
 8384/10000 [========================&gt;.....] - ETA: 0s
 9120/10000 [==========================&gt;...] - ETA: 0s
 9856/10000 [============================&gt;.] - ETA: 0s
10000/10000 [==============================] - 1s 73us/step
accuracy= 0.9793999791145325
</pre>


<div id="orgeb5c12d" class="figure">
<p><img src="images/Keras-MNist-Train-9.png" alt="Keras-MNist-Train-9.png" /><br />
</p>
<p><span class="figure-number">Figure 110: </span>Keras Mnist Model 訓練#3: accuracy</p>
</div>

<div id="org85faec2" class="figure">
<p><img src="images/Keras-MNist-Train-a.png" alt="Keras-MNist-Train-a.png" /><br />
</p>
<p><span class="figure-number">Figure 111: </span>Keras Mnist Model 訓練#3: loss information</p>
</div>

<p>
由訓練成果判斷，雖然精確率並未提升，但是由圖<a href="#orgeb5c12d">110</a>可以看出驗證精確率已經比訓練精確率高，已確實可以解決 overfitting 的問題。<br />
</p>

<p>
隨著 MLP 模型的改進，雖然精確率可逐步提升，也可藉由加入 Dropout 解決 overfitting 的問題，但 MLP 仍有其極限，如果要進一步提升準確率，就要使用卷積神經網路 CNN (convolutional neural network)。<br />
</p>
</div>
</div>
</div>

<div id="outline-container-org49420cd" class="outline-2">
<h2 id="org49420cd"><span class="section-number-2">14</span> 以 Keras 解決分類問題</h2>
<div class="outline-text-2" id="text-14">
</div>
<div id="outline-container-orgb469cad" class="outline-3">
<h3 id="orgb469cad"><span class="section-number-3">14.1</span> 二元分類：IMDB</h3>
<div class="outline-text-3" id="text-14-1">
<p>
自 IMDB 資料集中取得 50000 個正/負評論，各 25000 個，該資料集已內建於 Keras 中，且資料已先預處理，電影評論內容為由單字構成的 list 結構，例如，若評論內容為&ldquo;In a Wonderful morning&#x2026;&rdquo;，其 list 結構可能為(8, 3, 386, 1969&#x2026;)，每個單字都會依據其出現頻率給定一個編號，編號越小越常見。(與 IMDb 相關的 paper 參見<a href="https://paperswithcode.com/sota/sentiment-analysis-on-imdb">Sentiment Analysis on IMDb / paperswithcode</a><br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> imdb
<span class="linenr">2: </span>  (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr">3: </span>  <span style="color: #51afef;">print</span>(train_data[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">4: </span>  <span style="color: #51afef;">print</span>(train_labels[<span style="color: #da8548; font-weight: bold;">0</span>])
</pre>
</div>

<pre class="example">
[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]
1
</pre>


<p>
如上為第一筆評論的單字代號與評論結果，若要將原始資料的單字代號還原，其程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> imdb
<span id="coderef-imdbLoadData" class="coderef-off"><span class="linenr"> 2: </span>  (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)</span>
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">word_index is a dictionary mapping words to an integer index</span>
<span id="coderef-wordIndex" class="coderef-off"><span class="linenr"> 5: </span>  <span style="color: #dcaeea;">word_index</span> = imdb.get_word_index()</span>
<span class="linenr"> 6: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#23383;&#20856;&#20013;key&#28858;this&#23565;&#25033;&#30340;value:"</span>,word_index[<span style="color: #98be65;">'this'</span>])
<span class="linenr"> 7: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">We reverse it, mapping integer indices to words</span>
<span id="coderef-reverseWordIndex" class="coderef-off"><span class="linenr"> 8: </span>  reverse_word_index = <span style="color: #c678dd;">dict</span>([(value, key) <span style="color: #51afef;">for</span> (key, value) <span style="color: #51afef;">in</span> word_index.items()])</span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#21453;&#36681;&#23383;&#20856;&#20013;key&#28858;11&#25152;&#23565;&#25033;&#21040;&#30340;value:"</span>,reverse_word_index[<span style="color: #da8548; font-weight: bold;">11</span>])
<span class="linenr">10: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#21453;&#36681;&#23383;&#20856;&#20013;key&#28858;1&#25152;&#23565;&#25033;&#21040;&#30340;value:"</span>,reverse_word_index[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">11: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#21453;&#36681;&#23383;&#20856;&#20013;key&#28858;2&#25152;&#23565;&#25033;&#21040;&#30340;value:"</span>,reverse_word_index[<span style="color: #da8548; font-weight: bold;">2</span>])
<span class="linenr">12: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">We decode the review; note that our indices were offset by 3</span>
<span class="linenr">13: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">because 0, 1 and 2 are reserved indices for "padding", "start of sequence", and "unknown".</span>
<span id="coderef-decodedReview" class="coderef-off"><span class="linenr">14: </span>  decoded_review = <span style="color: #98be65;">' '</span>.join([reverse_word_index.get(i - <span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #98be65;">'?'</span>) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> train_data[<span style="color: #da8548; font-weight: bold;">0</span>]])</span>
<span class="linenr">15: </span>  <span style="color: #51afef;">print</span>(decoded_review)
</pre>
</div>

<pre class="example">
字典中key為this對應的value: 11
反轉字典中key為11所對應到的value: this
反轉字典中key為1所對應到的value: the
反轉字典中key為2所對應到的value: and
編號 0的單字: None
編號 1的單字: the
編號 2的單字: and
編號 3的單字: a
編號11的單字: this
? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all
</pre>

<p>
上述程式中第<a href="#coderef-wordIndex" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-wordIndex');" onmouseout="CodeHighlightOff(this, 'coderef-wordIndex');">5</a>行主要負責取得單字(key)的對應數字(value)的字典，再藉由第<a href="#coderef-reverseWordIndex" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-reverseWordIndex');" onmouseout="CodeHighlightOff(this, 'coderef-reverseWordIndex');">8</a>行將(key:value)轉換為(value:key)，最後第<a href="#coderef-decodedReview" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-decodedReview');" onmouseout="CodeHighlightOff(this, 'coderef-decodedReview');">14</a>行將字典中的單字回復至原始評論，程式中(i-3)的原因是第<a href="#coderef-imdbLoadData" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-imdbLoadData');" onmouseout="CodeHighlightOff(this, 'coderef-imdbLoadData');">2</a>的 load 已預留了第 0~2 個位置做特殊用途。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org977b3cd"></a>準備資料<br />
<div class="outline-text-4" id="text-14-1-1">
<p>
由於 IMDB 匯入 train_data 及 test_data 均為 list 型態，要先轉換為 tensor 才能輸入至神經網路，方法有二：<br />
</p>

<ol class="org-ol">
<li>填補資料中每個子 list 內容使其具有相同長度，再轉 shapre。<br /></li>
<li>對每個子 list 做 one-hot 編碼，其程式碼如下：<br /></li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> imdb
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">vectorize_sequences</span>(sequences, dimension=<span style="color: #da8548; font-weight: bold;">10000</span>):
<span class="linenr"> 8: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Create an all-zero matrix of shape (len(sequences), dimension)</span>
<span class="linenr"> 9: </span>      <span style="color: #dcaeea;">results</span> = np.zeros((<span style="color: #c678dd;">len</span>(sequences), dimension))
<span class="linenr">10: </span>      <span style="color: #51afef;">for</span> i, sequence <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(sequences):
<span class="linenr">11: </span>          <span style="color: #dcaeea;">results</span>[i, sequence] = <span style="color: #da8548; font-weight: bold;">1</span>.  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">set specific indices of results[i] to 1s</span>
<span class="linenr">12: </span>      <span style="color: #51afef;">return</span> results
<span class="linenr">13: </span>
<span class="linenr">14: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized training data</span>
<span class="linenr">15: </span>  <span style="color: #dcaeea;">x_train</span> = vectorize_sequences(train_data)
<span class="linenr">16: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized test data</span>
<span class="linenr">17: </span>  <span style="color: #dcaeea;">x_test</span> = vectorize_sequences(test_data)
<span class="linenr">18: </span>
<span class="linenr">19: </span>  <span style="color: #51afef;">print</span>(x_train[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">20: </span>
<span class="linenr">21: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26368;&#24460;&#20877;&#23559;&#27161;&#31844;&#36039;&#26009;&#20063;&#21521;&#37327;&#21270;</span>
<span class="linenr">22: </span>  <span style="color: #dcaeea;">y_train</span> = np.asarray(train_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">23: </span>  <span style="color: #dcaeea;">y_test</span> = np.asarray(test_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">24: </span>
<span class="linenr">25: </span>  <span style="color: #51afef;">print</span>(y_train[<span style="color: #da8548; font-weight: bold;">0</span>])
</pre>
</div>


<pre class="example">
[0. 1. 1. ... 0. 0. 0.]
1.0
</pre>
</div>
</li>

<li><a id="orgd3603e8"></a>建立神經網路<br />
<div class="outline-text-4" id="text-14-1-2">
<p>
由於輸入資料為向量、標籤為純量(1, 0)，對這樣的問題，適合用 relu 啟動函數的全連接層(Dense)堆疊架構：Dense(16, activation=&rsquo;relu&rsquo;)。其中 16 指該層神經元的數量(也可看成該層的寬度)，典型旳寫法為：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">output</span> = relu(dot(W, <span style="color: #c678dd;">input</span>)+b)
</pre>
</div>

<p>
擁有 16 個神經單元表示權重矩陣 W 的 shape 為(input_dimension, 10)，在 W 和 input 做內積後，input 資料會被映射到 16 維的空間上，最後加上 b、套用 relu 運算來產生輸出值。每一層的神經元數越多，可以讓神經網路學習更複雜的資料表示法，但也使計算成本更高。<br />
</p>

<p>
要建構一個 Dense 層堆疊架構，要考慮兩個關鍵：<br />
</p>

<ol class="org-ol">
<li>要用多少層？<br /></li>
<li>每一層要有多少神經元？<br /></li>
</ol>

<p>
此處使用兩個中間層、一個輸出層，如圖<a href="#org6fe30e2">112</a>，一般的神經網路中，介於輸入層和輸出層間的習慣稱為隱藏層(hidden layers)，但 Keras 的輸入層也有隱藏層的特性。圖<a href="#org6fe30e2">112</a>的 hidden layer 以 relu 為啟動函數，輸出層以 sigmoid 啟動函數輸出機率值。<br />
</p>

<div class="org-src-container">
<pre class="src src-ditaa">
        &#36664;&#20837;(&#21521;&#37327;&#21270;&#25991;&#23383;)
            |
            v
  +-------------------+   
  |+-----------------+|
  || Dense(units=16) ||-+
  |+--------+--------+| |
  |         |         | +-&#38577;&#34255;&#23652;
  |         v         | |
  |+-----------------+| |
  || Dense(units=10) ||-+
  |+--------+--------+|
  |         |         |
  |         v         |
  |+-----------------+|-+
  ||  Dense(units=1) || +-&#36664;&#20986;&#23652;
  |+-----------------+|-+
  +-------------------+
</pre>
</div>


<div id="org6fe30e2" class="figure">
<p><img src="images/nn3-6.png" alt="nn3-6.png" width="200" /><br />
</p>
<p><span class="figure-number">Figure 112: </span>IMDB model 架構</p>
</div>

<p>
為何要有 relu 等啟動函數？原因之一是這類函數為非線性函數(如圖<a href="#org87c8118">15</a>)，如果不是線性函數，則 Dense 層的運作就會變成<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">output</span> = dot(W, <span style="color: #c678dd;">input</span>)+b
</pre>
</div>

<p>
也就是說，該層只能學習輸入資料的線性變換，即使輸入資料的維度再多，也只是這些多維空間的所有可能線性變換，如此一來就算加入再多層的運算，最終仍只是在做線性運算，並無助於複雜學習。<br />
</p>

<p>
圖<a href="#org6fe30e2">112</a>的實作程式如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">3: </span>
<span class="linenr">4: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">5: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">6: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">7: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
</pre>
</div>

<p>
建好 model 後，要選擇一個損失函數和一個優化器，由於要處理的是二元分類問題，所以最好用 binary_crossentropy 損失函數，因為 crossentropy 主要就是用來測量機率分佈之間的距離(差異)。其實作如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr">2: </span>                loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">3: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>])
</pre>
</div>

<p>
之所以能將 optimizer 和 loss function 以字串方式經由參數傳給 compoile()，這是因為 rmsprop、binary_crossentropy 和 accuracy 均已事先在 Keras 套件中定義好了，若是要進一步自訂參數(如自訂學習率)，做法如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35519;&#25972;learning rate</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=optimizers.RMSprop(lr=<span style="color: #da8548; font-weight: bold;">0.001</span>),
<span class="linenr"> 5: </span>                loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr"> 6: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;&#21478;&#22806;&#30340;&#35413;&#20272;&#20989;&#25976;</span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> losses
<span class="linenr">10: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> metrics
<span class="linenr">11: </span>
<span class="linenr">12: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=optimizers.RMSprop(lr=<span style="color: #da8548; font-weight: bold;">0.001</span>),
<span class="linenr">13: </span>                loss=losses.binary_crossentropy,
<span class="linenr">14: </span>                metrics=[metrics.binary_accuracy])
<span class="linenr">15: </span>
<span class="linenr">16: </span>
</pre>
</div>
</div>
</li>

<li><a id="orgb7976cb"></a>驗證神經網路的 model<br />
<div class="outline-text-4" id="text-14-1-3">
<p>
為了在訓練期間監控 model 對新資料的準確度，可以從原始訓練資料中分離出 10000 個樣本來建立驗證資料集。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">x_val</span> = x_train[:<span style="color: #da8548; font-weight: bold;">10000</span>] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21069;10000&#20491;&#36039;&#26009;&#28858;&#39511;&#35657;&#38598;</span>
<span class="linenr">2: </span>  <span style="color: #dcaeea;">partial_x_train</span> = x_train[<span style="color: #da8548; font-weight: bold;">10000</span>:] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31532;10000&#20491;&#20197;&#24460;&#28858;&#35347;&#32244;&#38598;</span>
<span class="linenr">3: </span>
<span class="linenr">4: </span>  <span style="color: #dcaeea;">y_val</span> = y_train[:<span style="color: #da8548; font-weight: bold;">10000</span>]
<span class="linenr">5: </span>  <span style="color: #dcaeea;">partial_y_train</span> = y_train[<span style="color: #da8548; font-weight: bold;">10000</span>:]
</pre>
</div>

<p>
接下來才是使用 fit()來訓練模型，進行 20 個訓練週期(epoch，即，把 x_train 和 y_train 張量中的所有訓練樣本進行 20 輪的訓練)，以 512 個小樣本的小批量(batch_size)進行訓練，<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">history</span> = model.fit(partial_x_train,
<span class="linenr">2: </span>                      partial_y_train,
<span class="linenr">3: </span>                      epochs=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">4: </span>                      batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,
<span class="linenr">5: </span>                      validation_data=(x_val, y_val))
</pre>
</div>

<p>
model.fit()會回傳一個 history 物件，這物件本身有一個 history 屬性，為一個包含有關訓練過程中相關數據的字典，這個字期包含有 4 個項目(val_loss, val_acc, loss, acc)，為訓練和驗證時監控的指標。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28310;&#20633;&#36039;&#26009;</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> imdb
<span class="linenr"> 3: </span>  (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 5: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">vectorize_sequences</span>(sequences, dimension=<span style="color: #da8548; font-weight: bold;">10000</span>):
<span class="linenr"> 6: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Create an all-zero matrix of shape (len(sequences), dimension)</span>
<span class="linenr"> 7: </span>      <span style="color: #dcaeea;">results</span> = np.zeros((<span style="color: #c678dd;">len</span>(sequences), dimension))
<span class="linenr"> 8: </span>      <span style="color: #51afef;">for</span> i, sequence <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(sequences):
<span class="linenr"> 9: </span>          <span style="color: #dcaeea;">results</span>[i, sequence] = <span style="color: #da8548; font-weight: bold;">1</span>.  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">set specific indices of results[i] to 1s</span>
<span class="linenr">10: </span>      <span style="color: #51afef;">return</span> results
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized training data</span>
<span class="linenr">12: </span>  <span style="color: #dcaeea;">x_train</span> = vectorize_sequences(train_data)
<span class="linenr">13: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized test data</span>
<span class="linenr">14: </span>  <span style="color: #dcaeea;">x_test</span> = vectorize_sequences(test_data)
<span class="linenr">15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26368;&#24460;&#20877;&#23559;&#27161;&#31844;&#36039;&#26009;&#20063;&#21521;&#37327;&#21270;</span>
<span class="linenr">16: </span>  <span style="color: #dcaeea;">y_train</span> = np.asarray(train_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">17: </span>  <span style="color: #dcaeea;">y_test</span> = np.asarray(test_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">18: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#31435;model</span>
<span class="linenr">19: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">20: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">21: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">22: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">23: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">24: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
<span class="linenr">25: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr">26: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=optimizers.RMSprop(lr=<span style="color: #da8548; font-weight: bold;">0.001</span>),
<span class="linenr">27: </span>                loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">28: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39511;&#35657;&#25976;&#25818;&#38598;</span>
<span class="linenr">30: </span>  <span style="color: #dcaeea;">x_val</span> = x_train[:<span style="color: #da8548; font-weight: bold;">10000</span>] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21069;10000&#20491;&#36039;&#26009;&#28858;&#39511;&#35657;&#38598;</span>
<span class="linenr">31: </span>  <span style="color: #dcaeea;">partial_x_train</span> = x_train[<span style="color: #da8548; font-weight: bold;">10000</span>:] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31532;10000&#20491;&#20197;&#24460;&#28858;&#35347;&#32244;&#38598;</span>
<span class="linenr">32: </span>  <span style="color: #dcaeea;">y_val</span> = y_train[:<span style="color: #da8548; font-weight: bold;">10000</span>]
<span class="linenr">33: </span>  <span style="color: #dcaeea;">partial_y_train</span> = y_train[<span style="color: #da8548; font-weight: bold;">10000</span>:]
<span class="linenr">34: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;model</span>
<span class="linenr">35: </span>  <span style="color: #dcaeea;">history</span> = model.fit(partial_x_train,
<span class="linenr">36: </span>                      partial_y_train,
<span class="linenr">37: </span>                      epochs=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">38: </span>                      batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,
<span class="linenr">39: </span>                      validation_data=(x_val, y_val),
<span class="linenr">40: </span>                      verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">41: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31168;&#20986;history&#26550;&#27083;</span>
<span class="linenr">42: </span>  <span style="color: #dcaeea;">history_dict</span> = history.history
<span class="linenr">43: </span>  <span style="color: #51afef;">print</span>(history_dict.keys())
<span class="linenr">44: </span>
<span class="linenr">45: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr">46: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">47: </span>  <span style="color: #dcaeea;">accuracy</span> = history.history[<span style="color: #98be65;">'accuracy'</span>]
<span class="linenr">48: </span>  <span style="color: #dcaeea;">val_accuracy</span> = history.history[<span style="color: #98be65;">'val_accuracy'</span>]
<span class="linenr">49: </span>  <span style="color: #dcaeea;">loss</span> = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr">50: </span>  <span style="color: #dcaeea;">val_loss</span> = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr">51: </span>  <span style="color: #dcaeea;">epochs</span> = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(accuracy) + <span style="color: #da8548; font-weight: bold;">1</span>)<span style="color: #5B6268;"># </span><span style="color: #5B6268;">"bo" is for "blue dot"</span>
<span class="linenr">52: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">53: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">b is for "solid blue line"</span>
<span class="linenr">54: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">55: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">56: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">57: </span>  plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">58: </span>  plt.legend()
<span class="linenr">59: </span>  plt.plot()
<span class="linenr">60: </span>  plt.savefig(<span style="color: #98be65;">"imdb-Keras-1.png"</span>)
<span class="linenr">61: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()plt.clf()   # clear figure</span>
<span class="linenr">62: </span>
<span class="linenr">63: </span>  plt.clf()
<span class="linenr">64: </span>  <span style="color: #dcaeea;">acc_values</span> = history_dict[<span style="color: #98be65;">'accuracy'</span>]
<span class="linenr">65: </span>  <span style="color: #dcaeea;">val_acc_values</span> = history_dict[<span style="color: #98be65;">'val_accuracy'</span>]
<span class="linenr">66: </span>  plt.plot(epochs, accuracy, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr">67: </span>  plt.plot(epochs, val_accuracy, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr">68: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">69: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">70: </span>  plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">71: </span>  plt.legend()
<span class="linenr">72: </span>  plt.plot()
<span class="linenr">73: </span>  plt.savefig(<span style="color: #98be65;">"imdb-Keras-2.png"</span>)
<span class="linenr">74: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">75: </span>
</pre>
</div>

<pre class="example">
dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])
</pre>



<div id="org009fedd" class="figure">
<p><img src="images/imdb-Keras-1.png" alt="imdb-Keras-1.png" /><br />
</p>
<p><span class="figure-number">Figure 113: </span>IMDB-Keras-1</p>
</div>


<div id="org5c74199" class="figure">
<p><img src="images/imdb-Keras-2.png" alt="imdb-Keras-2.png" /><br />
</p>
<p><span class="figure-number">Figure 114: </span>IMDB-Keras-2</p>
</div>
</div>
</li>

<li><a id="org6576e9e"></a>優化 model<br />
<div class="outline-text-4" id="text-14-1-4">
<p>
由圖<a href="#org009fedd">113</a>、<a href="#org5c74199">114</a>可以看出，上述 model 雖然在訓練階段的效能不錯，loss function 隨 epoch 下降、accuracy 也隨 epoch 升高，但在驗證階段的表現卻十分不理想，不僅 accuracy 隨 epoch 的增加呈緩降趨勢，loss function 甚至還往上急升。<br />
</p>

<p>
第二版的 model 加入了兩層 layer 以及 dropout 層，其架構如下:<br />
</p>

<div class="org-src-container">
<pre class="src src-ditaa">
        &#36664;&#20837;(&#21521;&#37327;&#21270;&#25991;&#23383;)
            |
            v
  +-------------------+   
  |+-----------------+|
  || Dense(units=16) ||-+
  |+--------+--------+| |
  |         |         | |
  |         v         | |
  |+-----------------+| |
  || Dense(units=64) || |
  |+--------+--------+| |
  |         |         | |
  |         v         | |
  |+-----------------+| |
  ||  Dropout(0.25)  || |
  |+--------+--------+| |
  |         |         | +-&#38577;&#34255;&#23652;
  |         v         | |
  |+-----------------+| |
  || Dense(units=64) || |
  |+--------+--------+| |
  |         |         | |
  |         v         | |
  |+-----------------+| |
  ||  Dropout(0.25)  || |
  |+--------+--------+| |
  |         |         | |
  |         v         | |
  |+-----------------+| |
  || Dense(units=10) ||-+
  |+--------+--------+|
  |         |         |
  |         v         |
  |+-----------------+|-+
  ||  Dense(units=1) || +-&#36664;&#20986;&#23652;
  |+-----------------+|-+
  +-------------------+
</pre>
</div>

<div id="org0c38963" class="figure">
<p><img src="images/nn3-6-2.png" alt="nn3-6-2.png" width="200" /><br />
</p>
<p><span class="figure-number">Figure 115: </span>IMDB model 架構#2</p>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28310;&#20633;&#36039;&#26009;</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> imdb
<span class="linenr"> 3: </span>  (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 5: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">vectorize_sequences</span>(sequences, dimension=<span style="color: #da8548; font-weight: bold;">10000</span>):
<span class="linenr"> 6: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Create an all-zero matrix of shape (len(sequences), dimension)</span>
<span class="linenr"> 7: </span>      <span style="color: #dcaeea;">results</span> = np.zeros((<span style="color: #c678dd;">len</span>(sequences), dimension))
<span class="linenr"> 8: </span>      <span style="color: #51afef;">for</span> i, sequence <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(sequences):
<span class="linenr"> 9: </span>          <span style="color: #dcaeea;">results</span>[i, sequence] = <span style="color: #da8548; font-weight: bold;">1</span>.  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">set specific indices of results[i] to 1s</span>
<span class="linenr">10: </span>      <span style="color: #51afef;">return</span> results
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized training data</span>
<span class="linenr">12: </span>  <span style="color: #dcaeea;">x_train</span> = vectorize_sequences(train_data)
<span class="linenr">13: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized test data</span>
<span class="linenr">14: </span>  <span style="color: #dcaeea;">x_test</span> = vectorize_sequences(test_data)
<span class="linenr">15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26368;&#24460;&#20877;&#23559;&#27161;&#31844;&#36039;&#26009;&#20063;&#21521;&#37327;&#21270;</span>
<span class="linenr">16: </span>  <span style="color: #dcaeea;">y_train</span> = np.asarray(train_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">17: </span>  <span style="color: #dcaeea;">y_test</span> = np.asarray(test_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">18: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#31435;model</span>
<span class="linenr">19: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">20: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">21: </span>
<span class="linenr">22: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">23: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">24: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">25: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.25</span>))
<span class="linenr">26: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">27: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.25</span>))
<span class="linenr">28: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
<span class="linenr">29: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr">30: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> metrics
<span class="linenr">31: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=optimizers.RMSprop(lr=<span style="color: #da8548; font-weight: bold;">0.0001</span>),
<span class="linenr">32: </span>                loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">33: </span>                metrics=[metrics.binary_accuracy])
<span class="linenr">34: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39511;&#35657;&#25976;&#25818;&#38598;</span>
<span class="linenr">35: </span>  <span style="color: #dcaeea;">x_val</span> = x_train[:<span style="color: #da8548; font-weight: bold;">10000</span>] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21069;10000&#20491;&#36039;&#26009;&#28858;&#39511;&#35657;&#38598;</span>
<span class="linenr">36: </span>  <span style="color: #dcaeea;">partial_x_train</span> = x_train[<span style="color: #da8548; font-weight: bold;">10000</span>:] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31532;10000&#20491;&#20197;&#24460;&#28858;&#35347;&#32244;&#38598;</span>
<span class="linenr">37: </span>  <span style="color: #dcaeea;">y_val</span> = y_train[:<span style="color: #da8548; font-weight: bold;">10000</span>]
<span class="linenr">38: </span>  <span style="color: #dcaeea;">partial_y_train</span> = y_train[<span style="color: #da8548; font-weight: bold;">10000</span>:]
<span class="linenr">39: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;model</span>
<span class="linenr">40: </span>  <span style="color: #dcaeea;">history</span> = model.fit(partial_x_train,
<span class="linenr">41: </span>                      partial_y_train,
<span class="linenr">42: </span>                      epochs=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">43: </span>                      batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,
<span class="linenr">44: </span>                      validation_data=(x_val, y_val),
<span class="linenr">45: </span>                      verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">46: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31168;&#20986;history&#26550;&#27083;</span>
<span class="linenr">47: </span>  <span style="color: #dcaeea;">history_dict</span> = history.history
<span class="linenr">48: </span>  <span style="color: #51afef;">print</span>(history_dict.keys())
<span class="linenr">49: </span>
<span class="linenr">50: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36914;&#34892;&#38928;&#28204;</span>
<span class="linenr">51: </span>  <span style="color: #dcaeea;">x</span> = model.predict(x_test)
<span class="linenr">52: </span>  <span style="color: #51afef;">print</span>(x)
<span class="linenr">53: </span>
<span class="linenr">54: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr">55: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">56: </span>  plt.clf()
<span class="linenr">57: </span>  <span style="color: #dcaeea;">binary_accuracy</span> = history.history[<span style="color: #98be65;">'binary_accuracy'</span>]
<span class="linenr">58: </span>  <span style="color: #dcaeea;">val_binary_accuracy</span> = history.history[<span style="color: #98be65;">'val_binary_accuracy'</span>]
<span class="linenr">59: </span>  <span style="color: #dcaeea;">loss</span> = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr">60: </span>  <span style="color: #dcaeea;">val_loss</span> = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr">61: </span>  <span style="color: #dcaeea;">epochs</span> = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(binary_accuracy) + <span style="color: #da8548; font-weight: bold;">1</span>)<span style="color: #5B6268;"># </span><span style="color: #5B6268;">"bo" is for "blue dot"</span>
<span class="linenr">62: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">63: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">b is for "solid blue line"</span>
<span class="linenr">64: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">65: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">66: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">67: </span>  plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">68: </span>  plt.legend()
<span class="linenr">69: </span>  plt.plot()
<span class="linenr">70: </span>  plt.savefig(<span style="color: #98be65;">"imdb-Keras-3.png"</span>)
<span class="linenr">71: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()plt.clf()   # clear figure</span>
<span class="linenr">72: </span>
<span class="linenr">73: </span>  plt.clf()
<span class="linenr">74: </span>  <span style="color: #dcaeea;">acc_values</span> = history_dict[<span style="color: #98be65;">'binary_accuracy'</span>]
<span class="linenr">75: </span>  <span style="color: #dcaeea;">val_acc_values</span> = history_dict[<span style="color: #98be65;">'val_binary_accuracy'</span>]
<span class="linenr">76: </span>  plt.plot(epochs, binary_accuracy, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr">77: </span>  plt.plot(epochs, val_binary_accuracy, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr">78: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">79: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">80: </span>  plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">81: </span>  plt.legend()
<span class="linenr">82: </span>  plt.plot()
<span class="linenr">83: </span>  plt.savefig(<span style="color: #98be65;">"imdb-Keras-4.png"</span>)
<span class="linenr">84: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">85: </span>
</pre>
</div>

<pre class="example">
dict_keys(['val_loss', 'val_binary_accuracy', 'loss', 'binary_accuracy'])
[[0.1434195 ]
 [0.9996901 ]
 [0.98705375]
 ...
 [0.05256996]
 [0.11039814]
 [0.7423996 ]]
</pre>


<p>
[[file:<img src="images/imdb-Keras-3.png" alt="imdb-Keras-3.png" />images/imdb-Keras-1.png]]<br />
</p>

<p>
[[file:<img src="images/imdb-Keras-4.png" alt="imdb-Keras-4.png" />images/imdb-Keras-2.png]]<br />
</p>

<p>
比較上述兩組結果，可以發現優化版的 model 在 loss function 以及 accuracy 的表現都有進步。<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgf858a4a" class="outline-3">
<h3 id="orgf858a4a"><span class="section-number-3">14.2</span> 多類別分類：數位新聞</h3>
<div class="outline-text-3" id="text-14-2">
<p>
目標：將路透社(Reuters)的數位新聞專欄分成 46 個主題，這屬於多類別分類(multiclass classification)問題，每個資料點只會被歸入一個類別；如果每個資料點可能屬於多個類別，則屬於多標籤多類別(multilabel multiclass classification)問題。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org25f5856"></a>資料集<br />
<div class="outline-text-4" id="text-14-2-1">
<p>
和 MNIST、IMDB 一樣，這組由 Reuters 在 1986 年發布的簡短新聞主題資料集也內建在 Keras 中，這個資料集總共分為 46 個不同主題。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> reuters
<span class="linenr">2: </span>  (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr">3: </span>  <span style="color: #51afef;">print</span>(train_data[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">4: </span>  <span style="color: #51afef;">print</span>(train_labels[<span style="color: #da8548; font-weight: bold;">0</span>])
</pre>
</div>

<pre class="example">
[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]
3
</pre>


<p>
將資料向量化有幾種方式：將 label list 轉為整數張量，或是用 one-hot 編碼。以下為使用 pythonh 自訂的編碼程式：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">vectorize_sequences</span>(sequences, dimension=<span style="color: #da8548; font-weight: bold;">10000</span>):
<span class="linenr"> 4: </span>      <span style="color: #dcaeea;">results</span> = np.zeros((<span style="color: #c678dd;">len</span>(sequences), dimension))
<span class="linenr"> 5: </span>      <span style="color: #51afef;">for</span> i, sequence <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(sequences):
<span class="linenr"> 6: </span>          <span style="color: #dcaeea;">results</span>[i, sequence] = <span style="color: #da8548; font-weight: bold;">1</span>.
<span class="linenr"> 7: </span>      <span style="color: #51afef;">return</span> results
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized training data</span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">x_train</span> = vectorize_sequences(train_data)
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized test data</span>
<span class="linenr">12: </span>  <span style="color: #dcaeea;">x_test</span> = vectorize_sequences(test_data)
<span class="linenr">13: </span>
</pre>
</div>

<p>
另外，Keras 也有一個內建的函式可用：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras.utils.np_utils <span style="color: #51afef;">import</span> to_categorical
<span class="linenr">2: </span>
<span class="linenr">3: </span>  <span style="color: #dcaeea;">one_hot_train_labels</span> = to_categorical(train_labels)
<span class="linenr">4: </span>  <span style="color: #dcaeea;">one_hot_test_labels</span> = to_categorical(test_labels)
<span class="linenr">5: </span>
</pre>
</div>
</div>
</li>

<li><a id="org1c7f74f"></a>建立神經網路<br />
<div class="outline-text-4" id="text-14-2-2">
<p>
此次面臨的問題不似 IMDB 只分成兩類，而是共有 46 類，若每個 Dense layer 仍只使用 16 個維度，可能無法學會區分 46 個不同類別，故有需要將維度增加：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">3: </span>
<span class="linenr">4: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">5: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">6: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">7: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">46</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">8: </span>
</pre>
</div>

<p>
另外，輸出層將啟動函數由 sigmoid 改為 softmax，以機率值來顯示預測的類別結果，配合這種情境，最適合的損失函數為 categorical_crossentropy，它可以測量兩個機率分佈間的差距（即神經網路輸出的預測機率分佈與真實分佈間的距離），透過最小化這兩個分佈間的距離來訓練神經網路，讓結果接近答案。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr">2: </span>                loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span class="linenr">3: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">4: </span>
</pre>
</div>
</div>
</li>

<li><a id="orgc203a42"></a>驗證數據集<br />
<div class="outline-text-4" id="text-14-2-3">
<p>
由訓練集鵋出 1000 個樣本來驗證：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">x_val</span> = x_train[:<span style="color: #da8548; font-weight: bold;">1000</span>]
<span class="linenr">2: </span>  <span style="color: #dcaeea;">partial_x_train</span> = x_train[<span style="color: #da8548; font-weight: bold;">1000</span>:]
<span class="linenr">3: </span>
<span class="linenr">4: </span>  <span style="color: #dcaeea;">y_val</span> = one_hot_train_labels[:<span style="color: #da8548; font-weight: bold;">1000</span>]
<span class="linenr">5: </span>  <span style="color: #dcaeea;">partial_y_train</span> = one_hot_train_labels[<span style="color: #da8548; font-weight: bold;">1000</span>:]
</pre>
</div>
</div>
</li>

<li><a id="org84a2d00"></a>完整實作<br />
<div class="outline-text-4" id="text-14-2-4">
<p>
以下為完整的 model 程式碼<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> reuters
<span class="linenr">  2: </span>
<span class="linenr">  3: </span>  (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr">  4: </span>
<span class="linenr">  5: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  6: </span>
<span class="linenr">  7: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">vectorize_sequences</span>(sequences, dimension=<span style="color: #da8548; font-weight: bold;">10000</span>):
<span class="linenr">  8: </span>      <span style="color: #dcaeea;">results</span> = np.zeros((<span style="color: #c678dd;">len</span>(sequences), dimension))
<span class="linenr">  9: </span>      <span style="color: #51afef;">for</span> i, sequence <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(sequences):
<span class="linenr"> 10: </span>          <span style="color: #dcaeea;">results</span>[i, sequence] = <span style="color: #da8548; font-weight: bold;">1</span>.
<span class="linenr"> 11: </span>      <span style="color: #51afef;">return</span> results
<span class="linenr"> 12: </span>
<span class="linenr"> 13: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized training data</span>
<span class="linenr"> 14: </span>  <span style="color: #dcaeea;">x_train</span> = vectorize_sequences(train_data)
<span class="linenr"> 15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized test data</span>
<span class="linenr"> 16: </span>  <span style="color: #dcaeea;">x_test</span> = vectorize_sequences(test_data)
<span class="linenr"> 17: </span>
<span class="linenr"> 18: </span>  <span style="color: #51afef;">from</span> keras.utils.np_utils <span style="color: #51afef;">import</span> to_categorical
<span class="linenr"> 19: </span>
<span class="linenr"> 20: </span>  <span style="color: #dcaeea;">one_hot_train_labels</span> = to_categorical(train_labels)
<span class="linenr"> 21: </span>  <span style="color: #dcaeea;">one_hot_test_labels</span> = to_categorical(test_labels)
<span class="linenr"> 22: </span>
<span class="linenr"> 23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#27083;&#27169;&#22411;</span>
<span class="linenr"> 24: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 25: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 26: </span>
<span class="linenr"> 27: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr"> 28: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr"> 29: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 30: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">46</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr"> 31: </span>
<span class="linenr"> 32: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr"> 33: </span>                loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span id="coderef-metricsName" class="coderef-off"><span class="linenr"> 34: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>])</span>
<span class="linenr"> 35: </span>
<span class="linenr"> 36: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39511;&#35657;</span>
<span class="linenr"> 37: </span>  <span style="color: #dcaeea;">x_val</span> = x_train[:<span style="color: #da8548; font-weight: bold;">1000</span>]
<span class="linenr"> 38: </span>  <span style="color: #dcaeea;">partial_x_train</span> = x_train[<span style="color: #da8548; font-weight: bold;">1000</span>:]
<span class="linenr"> 39: </span>
<span class="linenr"> 40: </span>  <span style="color: #dcaeea;">y_val</span> = one_hot_train_labels[:<span style="color: #da8548; font-weight: bold;">1000</span>]
<span class="linenr"> 41: </span>  <span style="color: #dcaeea;">partial_y_train</span> = one_hot_train_labels[<span style="color: #da8548; font-weight: bold;">1000</span>:]
<span class="linenr"> 42: </span>
<span class="linenr"> 43: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;</span>
<span class="linenr"> 44: </span>  <span style="color: #dcaeea;">history</span> = model.fit(partial_x_train,
<span class="linenr"> 45: </span>                      partial_y_train,
<span class="linenr"> 46: </span>                      epochs=<span style="color: #da8548; font-weight: bold;">9</span>,
<span class="linenr"> 47: </span>                      batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,
<span class="linenr"> 48: </span>                      validation_data=(x_val, y_val),
<span class="linenr"> 49: </span>                      verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 50: </span>
<span class="linenr"> 51: </span>  <span style="color: #dcaeea;">history_dict</span> = history.history
<span class="linenr"> 52: </span>  <span style="color: #51afef;">print</span>(history_dict.keys())
<span class="linenr"> 53: </span>
<span class="linenr"> 54: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35413;&#20272;</span>
<span class="linenr"> 55: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Returns the loss value &amp; metrics values for the model in test mode.</span>
<span id="coderef-modelEvaluate" class="coderef-off"><span class="linenr"> 56: </span>  <span style="color: #dcaeea;">results</span> = model.evaluate(x_test, one_hot_test_labels)</span>
<span class="linenr"> 57: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#35413;&#20272;&#36039;&#26009;&#20839;&#23481;&#65306;"</span>,results)
<span class="linenr"> 58: </span>
<span class="linenr"> 59: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38928;&#28204;</span>
<span class="linenr"> 60: </span>  <span style="color: #dcaeea;">predictions</span> = model.predict(x_test)
<span class="linenr"> 61: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#36039;&#26009;&#26550;&#27083;&#65306;"</span>,predictions[<span style="color: #da8548; font-weight: bold;">0</span>].shape)
<span class="linenr"> 62: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#36039;&#26009;&#20839;&#23481;&#65306;"</span>,predictions[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 63: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#32080;&#26524;:"</span>,np.argmax(predictions[<span style="color: #da8548; font-weight: bold;">0</span>]))
<span class="linenr"> 64: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#31572;&#26696;:"</span>,one_hot_test_labels[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 65: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr"> 66: </span>
<span class="linenr"> 67: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 68: </span>
<span class="linenr"> 69: </span>  loss = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr"> 70: </span>  <span style="color: #dcaeea;">val_loss</span> = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr"> 71: </span>
<span class="linenr"> 72: </span>  <span style="color: #dcaeea;">epochs</span> = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(loss) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 73: </span>
<span class="linenr"> 74: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr"> 75: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr"> 76: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr"> 77: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr"> 78: </span>  plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr"> 79: </span>  plt.axis([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">10</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">3</span>])
<span class="linenr"> 80: </span>  plt.legend()
<span class="linenr"> 81: </span>  plt.plot()
<span class="linenr"> 82: </span>  plt.savefig(<span style="color: #98be65;">"reuters-1.png"</span>)
<span class="linenr"> 83: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr"> 84: </span>
<span class="linenr"> 85: </span>  plt.clf()   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">clear figure</span>
<span class="linenr"> 86: </span>
<span class="linenr"> 87: </span>  <span style="color: #dcaeea;">accuracy</span> = history.history[<span style="color: #98be65;">'accuracy'</span>]
<span class="linenr"> 88: </span>  <span style="color: #dcaeea;">val_accuracy</span> = history.history[<span style="color: #98be65;">'val_accuracy'</span>]
<span class="linenr"> 89: </span>
<span class="linenr"> 90: </span>  plt.plot(epochs, accuracy, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training accuracy'</span>)
<span class="linenr"> 91: </span>  plt.plot(epochs, val_accuracy, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation accuracy'</span>)
<span class="linenr"> 92: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr"> 93: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr"> 94: </span>  plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr"> 95: </span>  plt.axis([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">10</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr"> 96: </span>  plt.legend()
<span class="linenr"> 97: </span>  plt.plot()
<span class="linenr"> 98: </span>  plt.savefig(<span style="color: #98be65;">"reuters-2.png"</span>)
<span class="linenr"> 99: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">100: </span>
</pre>
</div>

<pre class="example">
dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])

  32/2246 [..............................] - ETA: 0s
 672/2246 [=======&gt;......................] - ETA: 0s
1344/2246 [================&gt;.............] - ETA: 0s
2016/2246 [=========================&gt;....] - ETA: 0s
2246/2246 [==============================] - 0s 78us/step
評估資料內容： [0.9810597261783807, 0.7804986834526062]
預測資料架構： (46,)
預測資料內容： [4.7579077e-05 1.2676844e-03 1.5874884e-04 9.6115595e-01 2.2415580e-02
 4.0142340e-06 1.0888425e-04 6.9402384e-05 6.5381191e-04 8.4027524e-05
 1.4560925e-05 1.6368082e-03 9.6688804e-05 4.5832386e-04 2.1395419e-05
 2.1998589e-05 5.2564731e-03 1.6274580e-04 1.8614135e-05 1.5144094e-03
 2.2311162e-03 5.8142754e-04 1.6369991e-05 2.4161035e-04 3.8008704e-05
 1.2996762e-04 9.4583183e-06 1.0127547e-04 1.5613921e-05 2.0752830e-04
 1.2362217e-04 9.5950272e-05 4.5157034e-05 3.6724876e-05 3.9637266e-04
 6.4885942e-05 1.7066645e-04 6.9418798e-05 2.6165835e-05 1.2429565e-04
 1.5218212e-05 7.5062417e-05 1.4183885e-06 6.8154754e-06 2.2027129e-06
 6.0409470e-06]
預測結果: 3
答案: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
</pre>


<div id="org9d66dee" class="figure">
<p><img src="images/reuters-1.png" alt="reuters-1.png" /><br />
</p>
<p><span class="figure-number">Figure 116: </span>Reuters-1</p>
</div>


<div id="orgac498d3" class="figure">
<p><img src="images/reuters-2.png" alt="reuters-2.png" /><br />
</p>
<p><span class="figure-number">Figure 117: </span>Reuters-2</p>
</div>

<p>
程式第<a href="#coderef-modelEvaluate" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-modelEvaluate');" onmouseout="CodeHighlightOff(this, 'coderef-modelEvaluate');">56</a>行傳回的值有兩個，一個是 loss value、一個是在建構 model 時(model.compile)所指定的評估標準 metrics（程式第<a href="#coderef-metricsName" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-metricsName');" onmouseout="CodeHighlightOff(this, 'coderef-metricsName');">34</a>行），在此處指的是 accuracy。上述程式在經由 9 個 epoch 後精準度已近 80%(0.79)。<br />
</p>
</div>
</li>

<li><a id="org2bff694"></a>優化 model<br />
<div class="outline-text-4" id="text-14-2-5">
<p>
上例中的中間層若將神經元數(維度)降到 4，則其驗證準確率會降至 71%，主要原因是因為這樣會壓縮大量資訊到一個低維度的中間層表示空間，雖然神經網路能將大部份必要的資訊塞進這 4 維表示法中，但仍顯不足。若再提升維度、增加層數、加入 Dropout，結果似乎沒有顯著改善，為什麼？<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> reuters
<span class="linenr">  2: </span>
<span class="linenr">  3: </span>  (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr">  4: </span>
<span class="linenr">  5: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  6: </span>
<span class="linenr">  7: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">vectorize_sequences</span>(sequences, dimension=<span style="color: #da8548; font-weight: bold;">10000</span>):
<span class="linenr">  8: </span>      <span style="color: #dcaeea;">results</span> = np.zeros((<span style="color: #c678dd;">len</span>(sequences), dimension))
<span class="linenr">  9: </span>      <span style="color: #51afef;">for</span> i, sequence <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(sequences):
<span class="linenr"> 10: </span>          <span style="color: #dcaeea;">results</span>[i, sequence] = <span style="color: #da8548; font-weight: bold;">1</span>.
<span class="linenr"> 11: </span>      <span style="color: #51afef;">return</span> results
<span class="linenr"> 12: </span>
<span class="linenr"> 13: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized training data</span>
<span class="linenr"> 14: </span>  <span style="color: #dcaeea;">x_train</span> = vectorize_sequences(train_data)
<span class="linenr"> 15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized test data</span>
<span class="linenr"> 16: </span>  <span style="color: #dcaeea;">x_test</span> = vectorize_sequences(test_data)
<span class="linenr"> 17: </span>
<span class="linenr"> 18: </span>  <span style="color: #51afef;">from</span> keras.utils.np_utils <span style="color: #51afef;">import</span> to_categorical
<span class="linenr"> 19: </span>
<span class="linenr"> 20: </span>  <span style="color: #dcaeea;">one_hot_train_labels</span> = to_categorical(train_labels)
<span class="linenr"> 21: </span>  <span style="color: #dcaeea;">one_hot_test_labels</span> = to_categorical(test_labels)
<span class="linenr"> 22: </span>
<span class="linenr"> 23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#27083;&#27169;&#22411;</span>
<span class="linenr"> 24: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 25: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 26: </span>
<span class="linenr"> 27: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr"> 28: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr"> 29: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">128</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 30: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.25</span>))
<span class="linenr"> 31: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">256</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 32: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.3</span>))
<span class="linenr"> 33: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">512</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 34: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr"> 35: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">46</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr"> 36: </span>
<span class="linenr"> 37: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr"> 38: </span>                loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span id="coderef-metricsName" class="coderef-off"><span class="linenr"> 39: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>])</span>
<span class="linenr"> 40: </span>
<span class="linenr"> 41: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39511;&#35657;</span>
<span class="linenr"> 42: </span>  <span style="color: #dcaeea;">x_val</span> = x_train[:<span style="color: #da8548; font-weight: bold;">1000</span>]
<span class="linenr"> 43: </span>  <span style="color: #dcaeea;">partial_x_train</span> = x_train[<span style="color: #da8548; font-weight: bold;">1000</span>:]
<span class="linenr"> 44: </span>
<span class="linenr"> 45: </span>  <span style="color: #dcaeea;">y_val</span> = one_hot_train_labels[:<span style="color: #da8548; font-weight: bold;">1000</span>]
<span class="linenr"> 46: </span>  <span style="color: #dcaeea;">partial_y_train</span> = one_hot_train_labels[<span style="color: #da8548; font-weight: bold;">1000</span>:]
<span class="linenr"> 47: </span>
<span class="linenr"> 48: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;</span>
<span class="linenr"> 49: </span>  <span style="color: #dcaeea;">history</span> = model.fit(partial_x_train,
<span class="linenr"> 50: </span>                      partial_y_train,
<span class="linenr"> 51: </span>                      epochs=<span style="color: #da8548; font-weight: bold;">9</span>,
<span class="linenr"> 52: </span>                      batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,
<span class="linenr"> 53: </span>                      validation_data=(x_val, y_val),
<span class="linenr"> 54: </span>                      verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 55: </span>
<span class="linenr"> 56: </span>  <span style="color: #dcaeea;">history_dict</span> = history.history
<span class="linenr"> 57: </span>  <span style="color: #51afef;">print</span>(history_dict.keys())
<span class="linenr"> 58: </span>
<span class="linenr"> 59: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35413;&#20272;</span>
<span class="linenr"> 60: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Returns the loss value &amp; metrics values for the model in test mode.</span>
<span id="coderef-modelEvaluate" class="coderef-off"><span class="linenr"> 61: </span>  <span style="color: #dcaeea;">results</span> = model.evaluate(x_test, one_hot_test_labels)</span>
<span class="linenr"> 62: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#35413;&#20272;&#36039;&#26009;&#20839;&#23481;&#65306;"</span>,results)
<span class="linenr"> 63: </span>
<span class="linenr"> 64: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38928;&#28204;</span>
<span class="linenr"> 65: </span>  <span style="color: #dcaeea;">predictions</span> = model.predict(x_test)
<span class="linenr"> 66: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#36039;&#26009;&#26550;&#27083;&#65306;"</span>,predictions[<span style="color: #da8548; font-weight: bold;">0</span>].shape)
<span class="linenr"> 67: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#36039;&#26009;&#20839;&#23481;&#65306;"</span>,predictions[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 68: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#32080;&#26524;:"</span>,np.argmax(predictions[<span style="color: #da8548; font-weight: bold;">0</span>]))
<span class="linenr"> 69: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#31572;&#26696;:"</span>,one_hot_test_labels[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 70: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr"> 71: </span>
<span class="linenr"> 72: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 73: </span>
<span class="linenr"> 74: </span>  loss = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr"> 75: </span>  <span style="color: #dcaeea;">val_loss</span> = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr"> 76: </span>
<span class="linenr"> 77: </span>  <span style="color: #dcaeea;">epochs</span> = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(loss) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 78: </span>
<span class="linenr"> 79: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr"> 80: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr"> 81: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr"> 82: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr"> 83: </span>  plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr"> 84: </span>  plt.axis([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">10</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">3</span>])
<span class="linenr"> 85: </span>  plt.legend()
<span class="linenr"> 86: </span>  plt.plot()
<span class="linenr"> 87: </span>  plt.savefig(<span style="color: #98be65;">"reuters-3.png"</span>)
<span class="linenr"> 88: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr"> 89: </span>
<span class="linenr"> 90: </span>  plt.clf()   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">clear figure</span>
<span class="linenr"> 91: </span>
<span class="linenr"> 92: </span>  <span style="color: #dcaeea;">accuracy</span> = history.history[<span style="color: #98be65;">'accuracy'</span>]
<span class="linenr"> 93: </span>  <span style="color: #dcaeea;">val_accuracy</span> = history.history[<span style="color: #98be65;">'val_accuracy'</span>]
<span class="linenr"> 94: </span>
<span class="linenr"> 95: </span>  plt.plot(epochs, accuracy, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training accuracy'</span>)
<span class="linenr"> 96: </span>  plt.plot(epochs, val_accuracy, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation accuracy'</span>)
<span class="linenr"> 97: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr"> 98: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr"> 99: </span>  plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">100: </span>  plt.axis([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">10</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">101: </span>  plt.legend()
<span class="linenr">102: </span>  plt.plot()
<span class="linenr">103: </span>  plt.savefig(<span style="color: #98be65;">"reuters-4.png"</span>)
<span class="linenr">104: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">105: </span>
<span class="linenr">106: </span>
</pre>
</div>

<pre class="example">
dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])

  32/2246 [..............................] - ETA: 0s
 544/2246 [======&gt;.......................] - ETA: 0s
1088/2246 [=============&gt;................] - ETA: 0s
1632/2246 [====================&gt;.........] - ETA: 0s
2208/2246 [============================&gt;.] - ETA: 0s
2246/2246 [==============================] - 0s 96us/step
評估資料內容： [1.3893752790197982, 0.7497773766517639]
預測資料架構： (46,)
預測資料內容： [8.8242497e-11 1.8549613e-07 1.2244985e-12 9.9946600e-01 4.8491010e-04
2.4004774e-11 4.0463274e-08 2.8705716e-09 1.4672127e-06 1.2457425e-12
 2.1608832e-08 1.1945065e-06 1.6438412e-08 6.1330823e-08 5.5952110e-10
 2.0300506e-12 4.4415983e-06 5.0839004e-09 3.8925752e-09 2.0913212e-05
 2.0335670e-05 7.7277225e-09 1.0450782e-12 6.6110601e-08 2.6362378e-11
 2.6260804e-07 2.6264095e-12 4.5255667e-11 4.4987689e-10 2.7449030e-09
 1.0358207e-08 7.0458644e-10 1.4057776e-09 6.6201856e-11 9.8362518e-09
 1.4279193e-11 2.3172060e-08 3.4204664e-10 7.4201589e-10 3.5206096e-08
 2.1588344e-09 2.4565621e-09 1.9249602e-11 5.2338623e-11 4.7235077e-14
 9.4377089e-14]
預測結果: 3
答案: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
</pre>

<p>
[[file:<img src="images/reuters-3.png" alt="reuters-3.png" />images/reuters-1.png]]<br />
</p>
<p>
[[file:<img src="images/reuters-4.png" alt="reuters-4.png" />images/reuters-2.png]]<br />
</p>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-orgd5bf4ba" class="outline-2">
<h2 id="orgd5bf4ba"><span class="section-number-2">15</span> 以 Keras 解決迴歸問題：預測房價: Boston</h2>
<div class="outline-text-2" id="text-15">
<p>
迴歸(regression)與邏輯斯迴歸(logistic regression)不同，後者為分類法，與迴歸無關。本例使用資料集為 1970 年中期 Boston 郊區資料，包含犯罪率、當地財產稅等，用以預測某郊區房價中位數，本例有 506 筆資料，分為 404 個訓練樣本和 102 個測試樣本，但每個 feature 的單位不同，故須先進行資料預調整。<br />
</p>
</div>

<div id="outline-container-org8a480a4" class="outline-3">
<h3 id="org8a480a4"><span class="section-number-3">15.1</span> 準備資料</h3>
<div class="outline-text-3" id="text-15-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> boston_housing
<span class="linenr">2: </span>
<span class="linenr">3: </span>  (train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()
<span class="linenr">4: </span>
<span class="linenr">5: </span>  <span style="color: #51afef;">print</span>(train_data.shape)
<span class="linenr">6: </span>  <span style="color: #51afef;">print</span>(test_data.shape)
</pre>
</div>

<pre class="example">
Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz

 8192/57026 [===&gt;..........................] - ETA: 1s
24576/57026 [===========&gt;..................] - ETA: 0s
40960/57026 [====================&gt;.........] - ETA: 0s
57344/57026 [==============================] - 1s 11us/step
(404, 13)
(102, 13)
</pre>


<p>
由於將不同類型不同單位的數值直接輸入神經網路會有問題，故要先資料進行正規化(normalization)處理，即，減去平均值，除以標準差。需留意的是，正規化時要用訓練資料集來計算 mean 和 std，不能使用測試集的資料。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">mean</span> = train_data.mean(axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">2: </span>  <span style="color: #dcaeea;">train_data</span> -= mean
<span class="linenr">3: </span>  <span style="color: #dcaeea;">std</span> = train_data.std(axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">4: </span>  <span style="color: #dcaeea;">train_data</span> /= std
<span class="linenr">5: </span>
<span class="linenr">6: </span>  <span style="color: #dcaeea;">test_data</span> -= mean
<span class="linenr">7: </span>  <span style="color: #dcaeea;">test_data</span> /= std
</pre>
</div>
</div>
</div>

<div id="outline-container-orgac6afe4" class="outline-3">
<h3 id="orgac6afe4"><span class="section-number-3">15.2</span> 建立神經網路</h3>
<div class="outline-text-3" id="text-15-2">
<p>
由於可用的樣本很少，所以使用一個較小的神經網路，一般來說，訓練資料集越少，過度配適的情況會越嚴重。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">build_model</span>():
<span class="linenr"> 5: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Because we will need to instantiate</span>
<span class="linenr"> 6: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">the same model multiple times,</span>
<span class="linenr"> 7: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">we use a function to construct it.</span>
<span class="linenr"> 8: </span>      <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr"> 9: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>,
<span class="linenr">10: </span>                             input_shape=(train_data.shape[<span style="color: #da8548; font-weight: bold;">1</span>],)))
<span class="linenr">11: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span id="coderef-OneUnitLayer" class="coderef-off"><span class="linenr">12: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>))</span>
<span class="linenr">13: </span>      model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>, loss=<span style="color: #98be65;">'mse'</span>, metrics=[<span style="color: #98be65;">'mae'</span>])
<span class="linenr">14: </span>      <span style="color: #51afef;">return</span> model
<span class="linenr">15: </span>
</pre>
</div>

<p>
這裡以 1 unit 的神經網路結束而且沒有啟動函數(第<a href="#coderef-OneUnitLayer" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-OneUnitLayer');" onmouseout="CodeHighlightOff(this, 'coderef-OneUnitLayer');">12</a>行)，代表為線性轉換，這是純量迴歸的基本設定，會輸出一個浮點數型別的數值(即迴歸值)，如果使用啟動函數，則只會輸出 0~1 間的值。另，mse 也是迴歸常用的損失函數，在評量指標的選擇方面，則採用 mae(mean absolute error，即預測值與目標值間差異的絕對值)。<br />
</p>
</div>
</div>

<div id="outline-container-orge7458a4" class="outline-3">
<h3 id="orge7458a4"><span class="section-number-3">15.3</span> 驗證</h3>
<div class="outline-text-3" id="text-15-3">
<p>
本例中由於資料點少，驗證集也只有 100 筆資料，故驗證分數可能會因驗證資料點或訓練資料點的選用而有很大的變化，因而阻礙評估 model 優劣的可靠性。在這種情況下，最好的方式是選用 K-fold corss validation，做法如圖<a href="#orge1b7cd6">118</a>，原理是將資料拆分為 K 個區域(通常 K=4 或 5)，每次取一個區域做為驗證資料集，最後求 K 次驗證分數的平均值。<br />
</p>


<div id="orge1b7cd6" class="figure">
<p><img src="images/k-fold-validation.png" alt="k-fold-validation.png" /><br />
</p>
<p><span class="figure-number">Figure 118: </span>K-fold 交叉驗證</p>
</div>

<p>
K-fold cross validation 的 python 實作程式碼如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">k</span> = <span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">num_val_samples</span> = <span style="color: #c678dd;">len</span>(train_data) // k
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">num_epochs</span> = <span style="color: #da8548; font-weight: bold;">100</span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">all_scores</span> = []
<span class="linenr"> 7: </span>  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(k):
<span class="linenr"> 8: </span>      <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'processing fold #'</span>, i)
<span class="linenr"> 9: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Prepare the validation data: data from partition # k</span>
<span class="linenr">10: </span>      val_data = train_data[i * num_val_samples: (i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples]
<span class="linenr">11: </span>      <span style="color: #dcaeea;">val_targets</span> = train_targets[i * num_val_samples: (i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples]
<span class="linenr">12: </span>
<span class="linenr">13: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Prepare the training data: data from all other partitions</span>
<span class="linenr">14: </span>      partial_train_data = np.concatenate(
<span class="linenr">15: </span>          [train_data[:i * num_val_samples],
<span class="linenr">16: </span>           train_data[(i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples:]],
<span class="linenr">17: </span>          axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">18: </span>      <span style="color: #dcaeea;">partial_train_targets</span> = np.concatenate(
<span class="linenr">19: </span>          [train_targets[:i * num_val_samples],
<span class="linenr">20: </span>           train_targets[(i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples:]],
<span class="linenr">21: </span>          axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">22: </span>
<span class="linenr">23: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Build the Keras model (already compiled)</span>
<span class="linenr">24: </span>      <span style="color: #dcaeea;">model</span> = build_model()
<span class="linenr">25: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Train the model (in silent mode, verbose=0)</span>
<span class="linenr">26: </span>      model.fit(partial_train_data, partial_train_targets,
<span class="linenr">27: </span>                epochs=num_epochs, batch_size=<span style="color: #da8548; font-weight: bold;">1</span>, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">28: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Evaluate the model on the validation data</span>
<span class="linenr">29: </span>      <span style="color: #dcaeea;">val_mse</span>, <span style="color: #dcaeea;">val_mae</span> = model.evaluate(val_data, val_targets, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">30: </span>      all_scores.append(val_mae)
</pre>
</div>



<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> boston_housing
<span class="linenr"> 2: </span>  (train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()
<span class="linenr"> 3: </span>  <span style="color: #51afef;">print</span>(train_data.shape)
<span class="linenr"> 4: </span>  <span style="color: #51afef;">print</span>(test_data.shape)
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">mean</span> = train_data.mean(axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">train_data</span> -= mean
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">std</span> = train_data.std(axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">train_data</span> /= std
<span class="linenr">10: </span>  <span style="color: #dcaeea;">test_data</span> -= mean
<span class="linenr">11: </span>  <span style="color: #dcaeea;">test_data</span> /= std
<span class="linenr">12: </span>
<span class="linenr">13: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">14: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">15: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">build_model</span>():
<span class="linenr">16: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Because we will need to instantiate</span>
<span class="linenr">17: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">the same model multiple times,</span>
<span class="linenr">18: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">we use a function to construct it.</span>
<span class="linenr">19: </span>      <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">20: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>,
<span class="linenr">21: </span>                             input_shape=(train_data.shape[<span style="color: #da8548; font-weight: bold;">1</span>],)))
<span class="linenr">22: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">23: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr">24: </span>      model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>, loss=<span style="color: #98be65;">'mse'</span>, metrics=[<span style="color: #98be65;">'mae'</span>])
<span class="linenr">25: </span>      <span style="color: #51afef;">return</span> model
<span class="linenr">26: </span>
<span class="linenr">27: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">28: </span>
<span class="linenr">29: </span>  <span style="color: #dcaeea;">k</span> = <span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr">30: </span>  <span style="color: #dcaeea;">num_val_samples</span> = <span style="color: #c678dd;">len</span>(train_data) // k
<span class="linenr">31: </span>  <span style="color: #dcaeea;">num_epochs</span> = <span style="color: #da8548; font-weight: bold;">100</span>
<span class="linenr">32: </span>  <span style="color: #dcaeea;">all_scores</span> = []
<span class="linenr">33: </span>  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(k):
<span class="linenr">34: </span>      <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'processing fold #'</span>, i)
<span class="linenr">35: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Prepare the validation data: data from partition # k</span>
<span class="linenr">36: </span>      val_data = train_data[i * num_val_samples: (i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples]
<span class="linenr">37: </span>      <span style="color: #dcaeea;">val_targets</span> = train_targets[i * num_val_samples: (i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples]
<span class="linenr">38: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Prepare the training data: data from all other partitions</span>
<span class="linenr">39: </span>      partial_train_data = np.concatenate(
<span class="linenr">40: </span>          [train_data[:i * num_val_samples],
<span class="linenr">41: </span>           train_data[(i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples:]],
<span class="linenr">42: </span>          axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">43: </span>      <span style="color: #dcaeea;">partial_train_targets</span> = np.concatenate(
<span class="linenr">44: </span>          [train_targets[:i * num_val_samples],
<span class="linenr">45: </span>           train_targets[(i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples:]],
<span class="linenr">46: </span>          axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">47: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Build the Keras model (already compiled)</span>
<span class="linenr">48: </span>      <span style="color: #dcaeea;">model</span> = build_model()
<span class="linenr">49: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Train the model (in silent mode, verbose=0)</span>
<span class="linenr">50: </span>      model.fit(partial_train_data, partial_train_targets,
<span class="linenr">51: </span>                epochs=num_epochs, batch_size=<span style="color: #da8548; font-weight: bold;">1</span>, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">52: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Evaluate the model on the validation data</span>
<span class="linenr">53: </span>      <span style="color: #dcaeea;">val_mse</span>, <span style="color: #dcaeea;">val_mae</span> = model.evaluate(val_data, val_targets, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">54: </span>      all_scores.append(val_mae)
<span class="linenr">55: </span>
<span class="linenr">56: </span>  <span style="color: #51afef;">print</span>(all_scores)
<span class="linenr">57: </span>  <span style="color: #51afef;">print</span>(np.mean(all_scores))
</pre>
</div>

<pre class="example">
(404, 13)
(102, 13)
processing fold # 0
processing fold # 1
processing fold # 2
processing fold # 3
[1.8689913749694824, 2.581745147705078, 2.9093284606933594, 2.6838433742523193]
2.51097708940506
</pre>


<p>
由上述結果看來，拆成 4 區的驗證分數自 1.87 到 2.91，總平均為 2.51，這個平均值是較為可靠的指標，因為當目標房價的數值很大時，1.87 到 2.91 會變成很大的誤差。<br />
</p>

<p>
可能是因為 MAC 與 Linux 版本的 Anaconda 相容性問題，或是 Keras 版本差異問題，MAC 版與 Linux 下的 history.history 架構略有差異：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Linux with Keras 2.2.5</span>
<span class="linenr">2: </span>dict_keys([<span style="color: #98be65;">'val_loss'</span>, <span style="color: #98be65;">'val_mean_absolute_error'</span>, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'mean_absolute_error'</span>])
<span class="linenr">3: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Mac with Keras 2.3.1</span>
<span class="linenr">4: </span>dict_keys([<span style="color: #98be65;">'val_loss'</span>, <span style="color: #98be65;">'val_mae'</span>, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'mae'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> boston_housing
<span class="linenr"> 2: </span>  (train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()
<span class="linenr"> 3: </span>  <span style="color: #51afef;">print</span>(train_data.shape)
<span class="linenr"> 4: </span>  <span style="color: #51afef;">print</span>(test_data.shape)
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">mean</span> = train_data.mean(axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">train_data</span> -= mean
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">std</span> = train_data.std(axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">train_data</span> /= std
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">test_data</span> -= mean
<span class="linenr">10: </span>  <span style="color: #dcaeea;">test_data</span> /= std
<span class="linenr">11: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">13: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">build_model</span>():
<span class="linenr">14: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Because we will need to instantiate</span>
<span class="linenr">15: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">the same model multiple times,</span>
<span class="linenr">16: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">we use a function to construct it.</span>
<span class="linenr">17: </span>      <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">18: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>,
<span class="linenr">19: </span>                             input_shape=(train_data.shape[<span style="color: #da8548; font-weight: bold;">1</span>],)))
<span class="linenr">20: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">21: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr">22: </span>      model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>, loss=<span style="color: #98be65;">'mse'</span>, metrics=[<span style="color: #98be65;">'mae'</span>])
<span class="linenr">23: </span>      <span style="color: #51afef;">return</span> model
<span class="linenr">24: </span>
<span class="linenr">25: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">26: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> backend <span style="color: #51afef;">as</span> K
<span class="linenr">27: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Some memory clean-up</span>
<span class="linenr">28: </span>  K.clear_session()
<span class="linenr">29: </span>  <span style="color: #dcaeea;">k</span> = <span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr">30: </span>  <span style="color: #dcaeea;">num_val_samples</span> = <span style="color: #c678dd;">len</span>(train_data) // k
<span class="linenr">31: </span>  <span style="color: #dcaeea;">num_epochs</span> = <span style="color: #da8548; font-weight: bold;">500</span>
<span class="linenr">32: </span>  <span style="color: #dcaeea;">all_mae_histories</span> = []
<span class="linenr">33: </span>  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(k):
<span class="linenr">34: </span>      <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'processing fold #'</span>, i)
<span class="linenr">35: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Prepare the validation data: data from partition # k</span>
<span class="linenr">36: </span>      val_data = train_data[i * num_val_samples: (i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples]
<span class="linenr">37: </span>      <span style="color: #dcaeea;">val_targets</span> = train_targets[i * num_val_samples: (i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples]
<span class="linenr">38: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Prepare the training data: data from all other partitions</span>
<span class="linenr">39: </span>      partial_train_data = np.concatenate(
<span class="linenr">40: </span>          [train_data[:i * num_val_samples],
<span class="linenr">41: </span>           train_data[(i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples:]],
<span class="linenr">42: </span>          axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">43: </span>      <span style="color: #dcaeea;">partial_train_targets</span> = np.concatenate(
<span class="linenr">44: </span>          [train_targets[:i * num_val_samples],
<span class="linenr">45: </span>           train_targets[(i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples:]],
<span class="linenr">46: </span>          axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">47: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Build the Keras model (already compiled)</span>
<span class="linenr">48: </span>      <span style="color: #dcaeea;">model</span> = build_model()
<span class="linenr">49: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Train the model (in silent mode, verbose=0)</span>
<span class="linenr">50: </span>      <span style="color: #dcaeea;">history</span> = model.fit(partial_train_data, partial_train_targets,
<span class="linenr">51: </span>                          validation_data=(val_data, val_targets),
<span class="linenr">52: </span>                          epochs=num_epochs, batch_size=<span style="color: #da8548; font-weight: bold;">1</span>, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">53: </span>      <span style="color: #dcaeea;">mae_history</span> = history.history[<span style="color: #98be65;">'val_mae'</span>]
<span class="linenr">54: </span>      all_mae_histories.append(mae_history)
<span class="linenr">55: </span>
<span class="linenr">56: </span>  <span style="color: #dcaeea;">average_mae_history</span> = [np.mean([x[i] <span style="color: #51afef;">for</span> x <span style="color: #51afef;">in</span> all_mae_histories]) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(num_epochs)]
<span class="linenr">57: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">58: </span>  plt.plot(<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(average_mae_history) + <span style="color: #da8548; font-weight: bold;">1</span>), average_mae_history)
<span class="linenr">59: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">60: </span>  plt.ylabel(<span style="color: #98be65;">'Validation MAE'</span>)
<span class="linenr">61: </span>  plt.plot()
<span class="linenr">62: </span>  plt.savefig(<span style="color: #98be65;">"Boston-House-Price.png"</span>)
<span class="linenr">63: </span>
<span class="linenr">64: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25490;&#38500;&#27599;&#36913;&#26399;&#30340;&#21069;10&#20491;&#36039;&#26009;&#40670;</span>
<span class="linenr">65: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">smooth_curve</span>(points, factor=<span style="color: #da8548; font-weight: bold;">0.9</span>):
<span class="linenr">66: </span>    <span style="color: #dcaeea;">smoothed_points</span> = []
<span class="linenr">67: </span>    <span style="color: #51afef;">for</span> point <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">points</span>:
<span class="linenr">68: </span>      <span style="color: #51afef;">if</span> smoothed_points:
<span class="linenr">69: </span>        previous = smoothed_points[-<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr">70: </span>        smoothed_points.append(previous * factor + point * (<span style="color: #da8548; font-weight: bold;">1</span> - factor))
<span class="linenr">71: </span>      <span style="color: #51afef;">else</span>:
<span class="linenr">72: </span>        smoothed_points.append(point)
<span class="linenr">73: </span>    <span style="color: #51afef;">return</span> smoothed_points
<span class="linenr">74: </span>
<span class="linenr">75: </span>  smooth_mae_history = smooth_curve(average_mae_history[<span style="color: #da8548; font-weight: bold;">10</span>:])
<span class="linenr">76: </span>  plt.clf()
<span class="linenr">77: </span>  plt.plot(<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(smooth_mae_history) + <span style="color: #da8548; font-weight: bold;">1</span>), smooth_mae_history)
<span class="linenr">78: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">79: </span>  plt.ylabel(<span style="color: #98be65;">'Validation MAE'</span>)
<span class="linenr">80: </span>  plt.plot()
<span class="linenr">81: </span>  plt.savefig(<span style="color: #98be65;">"Boston-House-Price-ex10.png"</span>)
<span class="linenr">82: </span>
</pre>
</div>

<pre class="example">
(404, 13)
(102, 13)
processing fold # 0
processing fold # 1
processing fold # 2
processing fold # 3
</pre>



<div id="org2ff47c1" class="figure">
<p><img src="images/Boston-House-Price.png" alt="Boston-House-Price.png" /><br />
</p>
<p><span class="figure-number">Figure 119: </span>Boston House Price Training MAE</p>
</div>

<p>
圖<a href="#org2ff47c1">119</a>是由每一訓練週期的平均 MAE 分數所繪出的折線圖，由於單位刻度與 y 軸刻度問題，此圖失去了部份重要細節，經由下列方式進行修正：<br />
</p>
<ul class="org-ul">
<li>省略前 10 個資料點，<br /></li>
<li>把每個資料點替換成前一點的指數移動平均值(exponential moving average, EMA)，讓誤差變平滑。<br /></li>
</ul>

<p>
EMA 常應用於各領域的資料分析中，其核心概念為：現在的資料會被過去的資料所影響，而時間點越近的資料影響越大，反之越小，如股票的漲幅，前 10 年的漲跌與前 10 日的漲跌，自然是後者對未來的影響更大。<br />
</p>

<p>
EMA 的數學函式如下：<br />
</p>

<p>
\( E_t = a \times V_t + (1-a) \times E_{t-1} \)，其中<br />
</p>

<ul class="org-ul">
<li>\(E_t\)為時間點\(t\)的指數移動平均值<br /></li>
<li>\(a\)為平滑係數，通常介於 0 到 1 之間<br /></li>
<li>\(V_t\)為時間點\(t\)的原始數值<br /></li>
<li>\(E_{t-1}\)為時間點\(t-1\)的指數移動平均值<br /></li>
</ul>

<p>
為什麼前例中前 10 筆數據的與其他數據差異如此巨大？我們以前 10 天的資料(一天一筆)來看，第 10 天的 EMA 為：<br />
\( E_{10} = aV_{10} + (1-a)E_9 \)<br />
展開第 9 天的\(E_9\)後<br />
\( E_{10} = aV_{10} + (1-a)[aV_9 + (1-a)E_8] \)<br />
整理後變成<br />
\( E_{10} = a(V_{10} + (1-a)V_9) + (1-a)^{2}E_8 \)<br />
若繼續展開所有天數，將得到<br />
\( E_{10} = a(V_{10} + (1-a)V_9) + (1-a)^{2}E_8+ \dots + (1-a)^{9}V_{1}) + (1-a)^{9}E_1 \)<br />
通常上式的最後一項會因為時間很長而變太小，故可忽略不計，而由此也可看出，\(E_{10}\)的值會被每天的原始資料\((V_{10} \dots V_{1}\))影響，每多一天，原始數值就會多乘(1-a)倍，成指數關係，故時間越久遠的事件，影響越小。<br />
</p>

<p>
[[<img src="images/Boston-House-Price-ex10.png" alt="Boston-House-Price-ex10.png" />]<br />
</p>

<p>
由圖<a href="#orgb975b9f">130</a>是可看出 MAE 在 80 個週期後已停止改善，然後開始往上升，即，過了這點就開始發生過度適配的情況。<br />
</p>
</div>
</div>

<div id="outline-container-orgf03a632" class="outline-3">
<h3 id="orgf03a632"><span class="section-number-3">15.4</span> 小結</h3>
<div class="outline-text-3" id="text-15-4">
<p>
由此範例可知：<br />
</p>
<ul class="org-ul">
<li>進行迴歸分木卜竹一中時，常以 MSE 做為損失函數、以 MAE 做為評估指標(而非 accuracy).<br /></li>
<li>當輸入資料的特徵有不同刻度時，應先將每個特徵進行轉換。<br /></li>
<li>當可用資料很少時，使用 K-fold 驗證來評估模式。<br /></li>
<li>當可用資料很少時，最好使用隠藏層較少(較淺)的小型神經網路，如一個或兩個，以免產生過渡配適。<br /></li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> keras
<span class="linenr">2: </span>  <span style="color: #51afef;">print</span>(keras.__version__)
</pre>
</div>

<pre class="example">
2.3.1
</pre>


<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orge9b4325" class="outline-2">
<h2 id="orge9b4325"><span class="section-number-2">16</span> CNN 卷積神經網路: 以 Keras 為實作工具</h2>
<div class="outline-text-2" id="text-16">
<p>
卷積神經網路(CNN)是由一位計算機科學家楊・勒丘恩(Yann LeCun)所提出，此人在機器學習、計算機視覺、計算機神經科學等領域都有很多貢獻。<br />
</p>

<p>
CNN 也是模仿人類大腦的認知方式，譬如我們辨識一個圖像，會先注意到顏色鮮明的點、線、面，之後將它們構成一個個不同的形狀(眼睛、鼻子、嘴巴&#x2026;)，這種抽象化的過程就是 CNN 演算法建立模型的方式。卷積層(Convolution Layer) 就是由點的比對轉成局部的比對，透過一塊塊的特徵研判，逐步堆疊綜合比對結果，就可以得到比較好的辨識結果，過程如下圖。<sup><a id="fnr.15" class="footref" href="#fn.15">15</a></sup><br />
</p>

<p>
<img src="images/CNN-1.png" alt="CNN-1.png" /><br />
資料來源：<sup><a id="fnr.16" class="footref" href="#fn.16">16</a></sup><br />
</p>
</div>

<div id="outline-container-orgb43fd6e" class="outline-3">
<h3 id="orgb43fd6e"><span class="section-number-3">16.1</span> CNN v.s. MLP</h3>
<div class="outline-text-3" id="text-16-1">
<p>
CNN 與前章所述之 MLP 的主要差異可由下圖看出，即，CNN 在輪入層中多了卷積層與池化層，MLP 有兩個缺點<sup><a id="fnr.17" class="footref" href="#fn.17">17</a></sup>：<br />
</p>
<ol class="org-ol">
<li>需要大量記憶體:在處理 256x256 大小的彩色圖片時，會需要用到 256 * 256 * 3 =196,608 個 Input Neuron，如果中間的隱藏層有 1000 個 Neuron，每個神經元需要一個浮點數的權重值 (8bytes)，那麼總共需要 196,608 * 1000 * 8 = 1.4648GBytes 的記憶體才夠。更何況這還只是個簡單的模型。<br /></li>
<li>多層感知器只針對圖片中每個單一像素去作判斷，完全捨棄重要的影像特徵。人類在判斷所看到的物體時，會從不同部位的特徵先作個別判斷，例如當你看到一架飛機，會先從機翼、機鼻、機艙體等這些特徵，再跟記憶中的印象來判斷是否為一架飛機，甚至再進一步判斷為客機還是戰鬥機。但是多層感知器沒有利用這些特徵，所以在影像的判讀上準確率就沒有接下來要討論的 CNN 來得好。<br /></li>
</ol>

<p>
與 MLP 相比，CNN 多了卷積層與池化層，卷積層用來強調圖案(資料)特徵，池化層則可以縮減取樣，減少 overfitting 問題。二者架構差異如圖<a href="#orgb956ce8">120</a>所示。<br />
</p>

<div id="orgb956ce8" class="figure">
<p><img src="images/CNN-MLP.jpg" alt="CNN-MLP.jpg" /><br />
</p>
<p><span class="figure-number">Figure 120: </span>CNN 與 MLP 之差異</p>
</div>
</div>
</div>

<div id="outline-container-orgf9c2d87" class="outline-3">
<h3 id="orgf9c2d87"><span class="section-number-3">16.2</span> CNN 模型架構</h3>
<div class="outline-text-3" id="text-16-2">
<p>
典型的 CNN 模型架構如圖<a href="#orgd483d70">121</a>所示，主要可分為兩大部份：<br />
</p>

<div id="orgd483d70" class="figure">
<p><img src="images/CNN-arch-1.jpg" alt="CNN-arch-1.jpg" /><br />
</p>
<p><span class="figure-number">Figure 121: </span>CNN 架構圖</p>
</div>
</div>

<ol class="org-ol">
<li><a id="org697fd2e"></a>影像特徵擷取<br />
<div class="outline-text-4" id="text-16-2-1">
<p>
透過卷積層及池化層擷取影像特徵，每個 Conv2D 和 MAsxPooling2D 層的輸出都是 shape 為(height, width, channels)的 3D 張量，隨著神經網路進入更深的層、寬度和高度也隨之縮小。<br />
</p>
</div>
</li>

<li><a id="orgd984030"></a>完全連結神經網路<br />
<div class="outline-text-4" id="text-16-2-2">
<p>
包含平坦層、隠藏層、輸出層所組成的神經網路，由於最後一個 Conv2D 層輸出的 36 個 7*7 必須送到接續的分類器神經網路(也就是 Dense 層)，分類器神經網路能處理的是 1D 向量，但前一層的輸出是 3D 張量，所以要把 3D 張量展平為 1D。<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-orga05e7cf" class="outline-3">
<h3 id="orga05e7cf"><span class="section-number-3">16.3</span> 卷積運算</h3>
<div class="outline-text-3" id="text-16-3">
<p>
以全連接層（Affine 層）來處理神經網路的問題在於：全連接層會忽略資料的「形狀」。例如，假設輸入資料為影像，則通常會包含水平、垂直、色版方向的三維形狀，然而當這些資料輸入全連接層時，三維資料就必須變為平面（一維資料），如前述的 MNist 資料集，輸入為(1, 28, 28)，即，一種顏色、28*28 像素，輸入全連接層後會變成一行的 784 個資料。<br />
</p>

<p>
三維形狀的影像包含了許多重要的空間資料，例如，類似的空間有著相似的象素值、RGB 各色版間有緊密連接的關連性，距離較遠的像素彼此沒有關連&#x2026;等特質，這些特質會在全連接層中被忽略掉。卷積層（Convolution layer）則能維持這些形狀，我們把 CNN 的卷積層輸出入資料稱作特徵圖（input / output feature map），而在卷積層中執行的處理則稱為卷積運算，若以影像處理來比喻卷積運算，則相當於「濾鏡效果」。卷積層的意義即是將原本一個影像經由卷積運算產生多個影像，每個影像均代表不同特徵。卷積運算方式如下：<br />
</p>

<p>
典型的卷積運算如圖<a href="#orgc894382">122</a>，此處特徵圖為(4,4)，濾鏡為(3,3)，輸出為(2,2)，這裡的濾鏡又稱為「核(kernel)」，其初始值由隨機方式產生。此外，在全連接網路層中，除了權重參數（核）之外，還有偏權值，其結果如圖<a href="#orga233c63">123</a>。<br />
</p>
<div class="org-src-container">
<pre class="src src-dot">digraph structs {
  rankdir=LR;
    S[label = <span style="color: #98be65;">"&#8855;"</span> shape=circle size = <span style="color: #98be65;">"1"</span> color=white]
    struct1 [shape = record label=<span style="color: #98be65;">"{1|2|3|0}|{0|1|2|&lt;f1&gt;3}|{3|0|1|2}|{2|3|0|1}"</span>];
  struct2 [shape = record label=<span style="color: #98be65;">"{2|0|1}|{&lt;f21&gt;0|1|&lt;f22&gt;2}|{1|0|2}"</span>];
  struct3 [shape = record label=<span style="color: #98be65;">"{15|16}|{&lt;f3&gt;6|15}"</span>];
  struct1:f1 -&gt; S;
  S -&gt; struct2:f21;
  struct2:f22 -&gt; struct3:f3;
}
</pre>
</div>

<div id="orgc894382" class="figure">
<p><img src="images/CNN-struc1.png" alt="CNN-struc1.png" /><br />
</p>
<p><span class="figure-number">Figure 122: </span>卷積運算</p>
</div>

<div class="org-src-container">
<pre class="src src-dot">digraph structs {
  nodesep=0.2;
  ranksep = 0.3;
  rankdir=LR;
    S1[label = <span style="color: #98be65;">"&#8855;"</span> shape=circle size = <span style="color: #98be65;">"1"</span> color=white]
    S2[label = <span style="color: #98be65;">"+"</span> shape=record size = <span style="color: #98be65;">"1"</span> color=white]
    S3[label = <span style="color: #98be65;">"3"</span> shape=record size = <span style="color: #98be65;">"1"</span> color=white nodesep = 0.2]
    struct1 [shape = record label=<span style="color: #98be65;">"{1|2|3|0}|{0|1|2|&lt;f1&gt;3}|{3|0|1|2}|{2|3|0|1}"</span>];
  struct2 [shape = record label=<span style="color: #98be65;">"{2|0|1}|{&lt;f21&gt;0|1|&lt;f22&gt;2}|{1|0|2}"</span>];
  struct3 [shape = record label=<span style="color: #98be65;">"{15|16}|{ &lt;f31&gt;6|&lt;f32&gt;15}"</span>];
  struct5 [shape = record label=<span style="color: #98be65;">"{18|19}|{&lt;f5&gt; 9|18}"</span>];
  struct1:f1 -&gt; S1;
  S1 -&gt; struct2:f21;
  struct2:f22 -&gt; struct3:f31;
  struct3:f32 -&gt; S2 -&gt; S3 -&gt; struct5:f5;
  
  
}
</pre>
</div>

<div id="orga233c63" class="figure">
<p><img src="images/CNN-struc2.png" alt="CNN-struc2.png" /><br />
</p>
<p><span class="figure-number">Figure 123: </span>卷積運算 2</p>
</div>

<p>
由圖<a href="#orgc894382">122</a>、<a href="#orga233c63">123</a>的卷積運算可以看出，輸入特徵值在經過運算後，其大小會縮小，可以預見的結果是，在經過多層神經網路反複進行卷積運算後，輸出特徵值大小很快就會縮小為 1，而無法再進行卷積運算。為了避免這種情況發生，在進行卷積運算前，可以針對輸入資料周圍補上一圈固定的資料（例如 0），這個動作稱為填補(padding)，如果我們對在卷積運算前對於圖<a href="#orgc894382">122</a>中的輸入特徵值進行寬度 1 的填補，則其結果如圖<a href="#org9b0002e">124</a>所示。<br />
</p>

<div class="org-src-container">
<pre class="src src-dot">digraph structs {
  rankdir=LR;
    S[label = <span style="color: #98be65;">"&#8855;"</span> shape=circle size = <span style="color: #98be65;">"1"</span> color=white]
    struct1 [shape = record label=<span style="color: #98be65;">"{0|0|0|0|0|0}|{0|1|2|3|0|0}|{0|0|1|2|3|&lt;f3&gt;0}|{0|3|0|1|2|0}|{0|2|3|0|1|0}|{0|0|0|0|0|0}"</span>];
  struct2 [shape = record label=<span style="color: #98be65;">"{2|0|1}|{&lt;f21&gt;0|1|&lt;f22&gt;2}|{1|0|2}"</span>];
  struct3 [shape = record label=<span style="color: #98be65;">"{7|12|10|4}|{&lt;f3&gt;4|15|16|10}|{10|6|15|6}|{8|10|4|3}"</span>];
  struct1:f1 -&gt; S;
  S -&gt; struct2:f21;
  struct2:f22 -&gt; struct3:f3;
  }
</pre>
</div>

<div id="org9b0002e" class="figure">
<p><img src="images/CNN-struc3.png" alt="CNN-struc3.png" /><br />
</p>
<p><span class="figure-number">Figure 124: </span>卷積運算 3</p>
</div>

<p>
進行上述卷積運算時，每次自左而右、自上而下，自輸入特徵值取出一個與核大小相同的子矩陣與核進行運算，而每個取出的子矩陣的間隔稱為步幅(stride)，上述範例中的 stide 均為 1，若 stride 設為 2，則一個(7,7)的輸入特徵值與一個(3,3)的核進行運算後，其輸出特徵值大小只剩(3,3)。<br />
</p>

<p>
上述範例中的特徵值均為二維矩陣，亦即，僅能表示水平與垂直方向的單色點陣圖，若輸入中包含色彩資料(RGB)，則輸入特徵圖將升及為三維矩陣（三層二維矩陣，每層表示一種 RGB 值），核的結構也是三維矩陣，但輸出特徵值則為二維矩陣。 以 MNist 資料集為例，將數字 7 的 28*28 影像以隨機產生的 5*5 濾鏡(或稱 filter weight、kernel)對其進行卷積，其結果如圖<a href="#org27f5a84">125</a>所示，這種效果有助於提取輸入影像的不同特徵，例如邊緣、線條&#x2026;等。<br />
</p>

<div id="org27f5a84" class="figure">
<p><img src="CNN-FW-1.jpg" alt="CNN-FW-1.jpg" /><br />
</p>
<p><span class="figure-number">Figure 125: </span>卷積運算 4</p>
</div>

<p>
實際建構模型時，不會只進行單一 kenrel 的卷積，圖<a href="#org6d89b82">126</a>即是隨機產生 16 個 kernel 對輸入影像提取不同特徵。<br />
</p>

<div id="org6d89b82" class="figure">
<p><img src="images/CNN-FW-2.jpg" alt="CNN-FW-2.jpg" /><br />
</p>
<p><span class="figure-number">Figure 126: </span>卷積運算 5</p>
</div>

<p>
密集層和卷積層之間的根本區別在於：密集層會由輸入的特徵空間中學習全域的 pattern，而卷積層則是學習局部的 pattern，以影像辨識為例，convent 會把輸入分解成小小的 2D 窗格，然後從中找出 pattern。這個關鍵特性為 CNN 提供了兩個有趣的特質：<br />
</p>

<ul class="org-ul">
<li>學習到的 pattern 具平移不變性(translation invariant)：在影像的右下角學習到的某個 pattern，CNN 可以在任何位置識別這樣的 pattern(如左上角)，相對的，當密集層連接神經網路看到 pattern 出現在新位置時，就必須重新學習。這使的 CNN 可以更有效率地處理影像資料。<br /></li>
<li>學習到 pattern 的空間層次結構(spatial hierarchies of patterns)：第一層卷積層會學習到諸如邊邊角角的小局部圖案，第二層卷積層則會基於第一層學到的特徵(小圖案)來學習較大的圖案，這使得 CNN 能夠有效地學習越來越複雜和抽象的視覺概念。<br /></li>
</ul>

<p>
CNN 所運算的 3D 張量稱為特徵映射圖(feature maps，簡稱特徵圖)，特徵圖有 2 個空間軸(height, width)以及 1 個色深度軸(depth / channel)，對於 RGB 影像來說，depth 值為 3。卷積運算會從輸入特徵中萃取出各種小區塊 pattern，當它對整張影像都做完萃取之後，就會產生輸出特徵映射圖(output feature map)。輸出特徵映射圖仍是 3D 張量，具有寬度和高度，但此時其深度軸已不再代表 RGB 顏色值，此時它代表過濾器(filter)，每一種 filter 會對輸入資料進行特定面向的編碼、萃取出結果，例如，filter 可以萃取到「在輸入資料中出現一張臉」這種高階抽象的概念，將不是臉的都過濾掉。<br />
</p>
</div>
</div>

<div id="outline-container-orgf13e09f" class="outline-3">
<h3 id="orgf13e09f"><span class="section-number-3">16.4</span> 池化層(Pooling Layer)</h3>
<div class="outline-text-3" id="text-16-4">
<p>
卷積層之間通常會加一個池化層(Pooling Layer)，它是一個壓縮圖片並保留重要資訊的方法，取樣的方法一樣是採滑動視窗，但是通常取最大值(Max-Pooling)，而非加權總和，若滑動視窗大小設為 2，『滑動步長』(Stride) 也為 2，則資料量就降為原本的四分之一，但因為取最大值，它還是保留局部範圍比對的最大可能性。也就是說，池化後的資訊更專注於圖片中是否存在相符的特徵，而非圖片中『哪裡』存在這些特徵，幫助 CNN 判斷圖片中是否包含某項特徵，而不必關心特徵所在的位置，這樣圖像偏移，一樣可以辨識出來。其架構如圖<a href="#org46df240">127</a>所示。<sup><a id="fnr.16.100" class="footref" href="#fn.16">16</a></sup><br />
</p>

<div id="org46df240" class="figure">
<p><img src="images/CNN-Pooling.png" alt="CNN-Pooling.png" /><br />
</p>
<p><span class="figure-number">Figure 127: </span>CNN 之池化層</p>
</div>

<p>
池化層以縮減取樣(downsampling)的方式縮小影像可以帶來以下優點：<br />
</p>

<ol class="org-ol">
<li>減少需要處理的資料點：減少後續運算所需時間。<br /></li>
<li>讓影像位置差異變小：影像要辨識的目標（如 MNist 資料集的數字）可能因為在影像中的位置不同而影響辦識，減小影像可以讓數字的位置差異變小。<br /></li>
<li>參數的數量程計算量下降：同時也能控制 overfitting。<br /></li>
</ol>

<p>
圖<a href="#orgd19ff64">128</a>即為使用 Max-Pool 對 16 個卷積影像進行縮減取樣(downsampling)的效果，將 16 個 28*28 個影像縮小為 16 個 14*14 的影像，但仍保持其特徵。MaxPooling 主要是由輸入特徵圖中做採樣並輸出樣本的最大值，它在概念上類似於卷積層操作，但並不是用卷積核(convolution kernel)張量積的方式來轉換局部區塊，而是經由手動編碼的 max 張量操作進行轉換。與卷積層操作的很大區別是 MaxPooling 通常用 2&times;2 窗格和步長(strides)2 來完成，以便將特徵圖每一軸的採樣減少到原來的 1/2，而卷積層操作通常使用 3&times;3 窗格且不指定步長(即使用預設步長 1)。<br />
</p>


<div id="orgd19ff64" class="figure">
<p><img src="images/CNN-PL-1.jpg" alt="CNN-PL-1.jpg" /><br />
</p>
<p><span class="figure-number">Figure 128: </span>池化效果</p>
</div>

<p>
為什麼要採用這種方式來縮小採樣特徵圖而不用原來的特徵圖尺寸一路執行下去？假設一個有不加入 MaxPooling 而是以全卷積層的模型設計如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">model_no_max_pool</span> = models.Sequential()
<span class="linenr">2: </span>  model_no_max_pool.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">32</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>),
<span class="linenr">3: </span>                        input_shape=(<span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr">4: </span>  model_no_max_pool.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">64</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>),)
<span class="linenr">5: </span>  model_no_max_pool.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">64</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>),)
</pre>
</div>

<p>
上述設計的問題可以從以下兩個面向來看：<sup><a id="fnr.18" class="footref" href="#fn.18">18</a></sup><br />
</p>

<ul class="org-ul">
<li>不利於學習特徵的空間層次結構。當 strides=1，第二層的 3&times;3 窗格要走過 5&times;5 的區域才能包含第一層的 7&times;7 區城，進而生成下一層的完整 3&times;3 區域，依此類推，第三層的 3&times;3 窗格也要走過 5&times;5 的區域才能包含第二層的 5&times;5 區域。也就是說，第三層的 3&times;3 窗格實際上僅包含來自原始輸入的 7&times;7 區城的資訊。相對於初始輸入，由卷積神經網路學習的高階 pattern 仍然進展很小，不足以學習分類數字。我們希望的是：最後一個卷積層的輸出特徵已經能提供有關輸入資料的總體訊息。<br /></li>
<li>最終特徵圖的每個樣本總係數為 22&times;22&times;64=3-976，這是相當巨大的 model，如果要將其展平以在頂部連接大小為 512 的 Dense 層，則該層將會有 1580 萬個參數，這對小 model 而言實在是太大，而且會導致 overfitting。<br /></li>
</ul>

<p>
雖然 MaxPooling 並不是縮小探樣特徵圖的唯一方法（也可以透過卷積層的 strides 來調整，或是使用平均池化而非 MaxPooling）,不過就經驗來看，MaxPooling 往往比這些方案好。主要原因是：特徵通常是源自於空間某些有特色的 pattern，因此要取其最大值才更具訊息性，若採用平均值，則特色就被掩蓋了。<br />
</p>
</div>
</div>

<div id="outline-container-org97149cb" class="outline-3">
<h3 id="org97149cb"><span class="section-number-3">16.5</span> 以 CNN 實作 MNIST</h3>
<div class="outline-text-3" id="text-16-5">
</div>
<ol class="org-ol">
<li><a id="org4ec8d04"></a>資料預處理(preprocess)<br />
<div class="outline-text-4" id="text-16-5-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">1. &#21295;&#20837;&#25152;&#38656;&#27169;&#32068;</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35201;&#23559;table&#36681;&#28858;one-hot encoding</span>
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 4: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">9527</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">2. &#21295;&#20837;Keras&#27169;&#32068;&#20197;&#19979;&#36617;MNist&#36039;&#26009;&#38598;'''</span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 8: </span>  <span style="color: #5B6268;">###  </span><span style="color: #5B6268;">&#35712;&#21462;MNist&#36039;&#26009;&#38598;</span>
<span class="linenr"> 9: </span>  (x_Train, y_Train), (x_Test, y_Test) = mnist.load_data()
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">3. &#23559;features&#36681;&#28858;4&#30697;&#38499;(6000*28*28*1)</span>
<span class="linenr">12: </span>  <span style="color: #dcaeea;">x_Train4D</span> = x_Train.reshape(x_Train.shape[<span style="color: #da8548; font-weight: bold;">0</span>],<span style="color: #da8548; font-weight: bold;">28</span>,<span style="color: #da8548; font-weight: bold;">28</span>,<span style="color: #da8548; font-weight: bold;">1</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">13: </span>  <span style="color: #dcaeea;">x_Test4D</span> = x_Test.reshape(x_Test.shape[<span style="color: #da8548; font-weight: bold;">0</span>],<span style="color: #da8548; font-weight: bold;">28</span>,<span style="color: #da8548; font-weight: bold;">28</span>,<span style="color: #da8548; font-weight: bold;">1</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">14: </span>
<span class="linenr">15: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">3. &#23559;features&#27161;&#28310;&#21270;</span>
<span class="linenr">16: </span>  <span style="color: #dcaeea;">x_Train4D_normalize</span> = x_Train4D / <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">17: </span>  <span style="color: #dcaeea;">x_Test4D_normalize</span> = x_Test4D / <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">18: </span>
<span class="linenr">19: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">4. Onehot encoding</span>
<span class="linenr">20: </span>  <span style="color: #dcaeea;">y_TrainOneHot</span> = np_utils.to_categorical(y_Train)
<span class="linenr">21: </span>  <span style="color: #dcaeea;">y_TestOneHot</span> = np_utils.to_categorical(y_Test)
</pre>
</div>
</div>
</li>

<li><a id="orgd341609"></a>建立模型<br />
<div class="outline-text-4" id="text-16-5-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">1. &#21295;&#20837;&#25152;&#38656;&#27169;&#32068;</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense, Dropout, Flatten, Conv2D, MaxPooling2D
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">2. &#24314;&#31435;keras&#30340;Sequential&#27169;&#22411;</span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">model</span> = Sequential()
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">3. &#21367;&#31309;&#23652;1&#33287;&#27744;&#21270;&#23652;1 (&#19968;&#20491;&#23436;&#25972;&#30340;&#21367;&#31309;&#36939;&#31639;&#21253;&#21547;&#19968;&#20491;&#21367;&#31309;&#23652;&#33287;&#27744;&#21270;&#23652;)&#65292;&#28670;&#37857;&#22823;&#23567;&#28858;5*5</span>
<span class="linenr"> 9: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#36664;&#20837;&#30340;&#24433;&#20687;&#28858;28*28&#65292;&#21367;&#31309;&#36939;&#31639;&#24460;&#29986;&#29983;16&#20491;&#24433;&#20687;&#65292;&#22823;&#23567;&#19981;&#35722;(padding=same)</span>
<span class="linenr">10: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#22240;&#28858;&#24433;&#20687;&#28858;&#21934;&#33394;&#65292;&#25925;&#24433;&#20687;&#22823;&#23567;&#28858; 28*28*1</span>
<span class="linenr">11: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#38928;&#35373;input_shape: image_height, image:width, image_channels</span>
<span class="linenr">12: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">16</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">5</span>), padding=<span style="color: #98be65;">'same'</span>,
<span class="linenr">13: </span>                   input_shape(<span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">1</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">14: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#23559;16&#20491;28*28&#24433;&#20687;&#36681;&#28858;16&#20491;14*14&#24433;&#20687; </span>
<span class="linenr">15: </span>  model.add(MaxPooling2D(pool_size=(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr">16: </span>
<span class="linenr">17: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">4. &#21367;&#31309;&#23652;2&#33287;&#27744;&#21270;&#23652;2 (&#19968;&#20491;&#23436;&#25972;&#30340;&#21367;&#31309;&#36939;&#31639;&#21253;&#21547;&#19968;&#20491;&#21367;&#31309;&#23652;&#33287;&#27744;&#21270;&#23652;)&#65292;&#28670;&#37857;&#22823;&#23567;&#28858;5*5</span>
<span class="linenr">18: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#23559;&#21407;&#26412;16&#20491;&#24433;&#20687;&#36681;&#25563;&#28858;36&#20491;&#24433;&#20687;</span>
<span class="linenr">19: </span>
<span class="linenr">20: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">36</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">5</span>), padding=<span style="color: #98be65;">'same'</span>,
<span class="linenr">21: </span>                   input_shape(<span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">1</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">22: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#23559;36&#20491;14*14&#24433;&#20687;&#36681;&#28858;36&#20491;7*7&#30340;&#24433;&#20687;</span>
<span class="linenr">23: </span>  model.add(MaxPooling2D(pool_size=(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr">24: </span>
<span class="linenr">25: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">5. &#21152;&#20837;Dropout&#36991;&#20813;overfitting&#65292;&#38568;&#27231;&#25918;&#26820;&#31070;&#32147;&#32178;&#36335;&#20013;25%&#30340;&#31070;&#32147;&#20803;</span>
<span class="linenr">26: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.25</span>))
<span class="linenr">27: </span>
<span class="linenr">28: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">6. &#24314;&#31435;&#31070;&#32147;&#32178;&#36335;(&#24179;&#22374;&#23652;&#12289;&#38560;&#34255;&#23652;&#12289;&#36664;&#20986;&#23652;)</span>
<span class="linenr">29: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#38560;&#34255;&#23652;&#26377;128&#20491;&#31070;&#32147;&#20803;&#65292;&#20006;&#38568;&#27231;&#25918;&#26820;50%&#30340;&#31070;&#32147;&#20803;</span>
<span class="linenr">30: </span>  model.add(Flatten())
<span class="linenr">31: </span>  model.add(Dense(<span style="color: #da8548; font-weight: bold;">128</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">32: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr">33: </span>  <span style="color: #5B6268;">###   </span><span style="color: #5B6268;">&#36664;&#20986;&#23652;&#26377;0~9&#20849;10&#20491;&#31070;&#32147;&#20803;&#65292;&#20197;softmax&#36664;&#20986;&#23565;&#26144;&#27231;&#29575;</span>
<span class="linenr">34: </span>  model.add(Dense(<span style="color: #da8548; font-weight: bold;">10</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">35: </span>
<span class="linenr">36: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">7. &#26597;&#30475;&#27169;&#22411;&#25688;&#35201;</span>
<span class="linenr">37: </span>  <span style="color: #51afef;">print</span>(model.summary())
</pre>
</div>
</div>
</li>

<li><a id="org3e12606"></a>訓練模型<br />
<div class="outline-text-4" id="text-16-5-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">1. &#23450;&#32681;&#35347;&#32244;&#26041;&#24335;   </span>
<span class="linenr"> 2: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#22312;&#28145;&#24230;&#23416;&#32722;&#20013;&#20351;&#29992;cross_entropy&#20132;&#21449;&#29109;&#35347;&#32244;&#25928;&#26524;&#36611;&#22909;</span>
<span class="linenr"> 3: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#20351;&#29992;adam&#20570;&#26368;&#20339;&#21270;&#21487;&#20197;&#35731;&#35347;&#32244;&#26356;&#24555;&#25910;&#27483;&#65292;&#25552;&#39640;&#28310;&#30906;&#29575;</span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#26368;&#24460;&#20197;accuracy&#20358;&#35413;&#20272;&#27169;&#22411;&#28310;&#30906;&#29575;</span>
<span class="linenr"> 5: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span class="linenr"> 6: </span>                optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr"> 7: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#22519;&#34892;10&#27425;&#35347;&#32244;&#36913;&#26399;&#65292;&#27599;&#19968;&#25209;&#27425;&#31639;300&#31558;&#36039;&#26009;</span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#22240;&#28858;&#20849;&#26377;48000&#31558;&#35347;&#32244;&#36039;&#26009;&#65292;&#27599;&#25209;&#27425;300&#31558;&#65292;&#32004;&#20998;160&#25209;&#27425;&#35347;&#32244;</span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">train_history</span>=model.fit(x=x_Train4D_normalize,
<span class="linenr">10: </span>                          y=y_TrainOneHot, validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr">11: </span>                          epochs=<span style="color: #da8548; font-weight: bold;">10</span>, batch_size=<span style="color: #da8548; font-weight: bold;">300</span>, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">12: </span>
<span class="linenr">13: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">2. &#30059;&#20986;&#25928;&#26524;&#22294;&#21450;&#35492;&#24046;&#22294;</span>
<span class="linenr">14: </span>  show_train_history(<span style="color: #98be65;">'acc'</span>, <span style="color: #98be65;">'val_acc'</span>)
<span class="linenr">15: </span>  show_train_history(<span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>)
</pre>
</div>
</div>
</li>

<li><a id="orgab59132"></a>評估模型準確率<br />
<div class="outline-text-4" id="text-16-5-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">scores</span> = model.evaluate(x_Test4D_normalize, y_TestOneHot)
<span class="linenr">2: </span>  <span style="color: #51afef;">print</span>(scores[<span style="color: #da8548; font-weight: bold;">1</span>])    
</pre>
</div>
</div>
</li>

<li><a id="org139b11c"></a>進行預測<br />
<div class="outline-text-4" id="text-16-5-5">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">prediction</span> = model.predict_classes(x_Test4D_normalize)
<span class="linenr">2: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">&#39023;&#31034;&#21069;10&#31558;&#38928;&#28204;&#32080;&#26524;</span>
<span class="linenr">3: </span>  <span style="color: #51afef;">print</span>(prediction[:<span style="color: #da8548; font-weight: bold;">10</span>]
<span class="linenr">4: </span>  plot_images_labels_prediction(x_Test, y_Test, prediction, idx=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">5: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">&#39023;&#31034;&#28151;&#28102;&#30697;&#38499;</span>
<span class="linenr">6: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">7: </span>  pd.crosstab(y_Test, prediction, rownames=[<span style="color: #98be65;">'label'</span>], colnames=[<span style="color: #98be65;">'predict'</span>])
</pre>
</div>
</div>
</li>

<li><a id="orgad80b69"></a>執行結果<br />
<div class="outline-text-4" id="text-16-5-6">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">define function</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> os
<span class="linenr"> 3: </span>  os.<span style="color: #dcaeea;">environ</span>[<span style="color: #98be65;">'TF_CPP_MIN_LOG_LEVEL'</span>]=<span style="color: #98be65;">'2'</span>
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 5: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_images_labels_prediction</span>(imgname, images, labels, prediction, idx, num = <span style="color: #da8548; font-weight: bold;">10</span>):
<span class="linenr"> 6: </span>      <span style="color: #dcaeea;">fig</span> = plt.gcf()
<span class="linenr"> 7: </span>      fig.set_size_inches(<span style="color: #da8548; font-weight: bold;">8</span>,<span style="color: #da8548; font-weight: bold;">4</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35373;&#23450;&#39023;&#31034;&#22294;&#24418;&#22823;&#23567;</span>
<span class="linenr"> 8: </span>      <span style="color: #51afef;">if</span> num&gt;<span style="color: #da8548; font-weight: bold;">25</span>: <span style="color: #dcaeea;">num</span>=<span style="color: #da8548; font-weight: bold;">25</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#36039;&#26009;&#31558;&#25976;&#19978;&#38480;</span>
<span class="linenr"> 9: </span>      <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">0</span>, num):
<span class="linenr">10: </span>          <span style="color: #dcaeea;">ax</span>=plt.subplot(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">1</span>+i) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27492;&#34389;&#39023;&#31034;10&#31558;&#22294;&#24418;&#65292;2*5&#20491;</span>
<span class="linenr">11: </span>          ax.imshow(images[idx], cmap=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr">12: </span>          <span style="color: #dcaeea;">title</span>= <span style="color: #98be65;">"label="</span> +<span style="color: #c678dd;">str</span>(labels[idx]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21152;&#20837;&#23376;&#22294;&#24418;title</span>
<span class="linenr">13: </span>          <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(prediction)&gt;<span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr">14: </span>              <span style="color: #dcaeea;">title</span>+=<span style="color: #98be65;">",predict="</span>+<span style="color: #c678dd;">str</span>(prediction[idx]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27161;&#38988;title&#21152;&#20837;&#38928;&#28204;&#32080;&#26524;</span>
<span class="linenr">15: </span>          ax.set_title(title,fontsize=<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">16: </span>          ax.set_xticks([]);ax.set_yticks([]) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#19981;&#39023;&#31034;&#21051;&#24230;</span>
<span class="linenr">17: </span>          <span style="color: #dcaeea;">idx</span>+=<span style="color: #da8548; font-weight: bold;">1</span> 
<span class="linenr">18: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">19: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.plot()</span>
<span class="linenr">20: </span>      plt.savefig(imgname+<span style="color: #98be65;">".png"</span>)
<span class="linenr">21: </span>
<span class="linenr">22: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(imgname, train_acc, test_acc):
<span class="linenr">23: </span>      plt.plot(train_history.history[train_acc])
<span class="linenr">24: </span>      plt.plot(train_history.history[test_acc])
<span class="linenr">25: </span>      plt.title(<span style="color: #98be65;">'Train History'</span>)
<span class="linenr">26: </span>      plt.ylabel(<span style="color: #98be65;">'Accuracy'</span>)
<span class="linenr">27: </span>      plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
<span class="linenr">28: </span>      plt.legend([<span style="color: #98be65;">'train'</span>,<span style="color: #98be65;">'test'</span>], loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">29: </span>      plt.savefig(imgname+<span style="color: #98be65;">".png"</span>)
<span class="linenr">30: </span>      plt.clf()
<span class="linenr">31: </span>
<span class="linenr">32: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">1. &#36039;&#26009;&#38928;&#34389;&#29702;(preprocess)</span>
<span class="linenr">33: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils 
<span class="linenr">34: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">35: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">9527</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr">36: </span>
<span class="linenr">37: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr">38: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">39: </span>  (x_Train, y_Train), (x_Test, y_Test) = mnist.load_data()
<span class="linenr">40: </span>  <span style="color: #dcaeea;">x_Train4D</span> = x_Train.reshape(x_Train.shape[<span style="color: #da8548; font-weight: bold;">0</span>],<span style="color: #da8548; font-weight: bold;">28</span>,<span style="color: #da8548; font-weight: bold;">28</span>,<span style="color: #da8548; font-weight: bold;">1</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">41: </span>  <span style="color: #dcaeea;">x_Test4D</span> = x_Test.reshape(x_Test.shape[<span style="color: #da8548; font-weight: bold;">0</span>],<span style="color: #da8548; font-weight: bold;">28</span>,<span style="color: #da8548; font-weight: bold;">28</span>,<span style="color: #da8548; font-weight: bold;">1</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">42: </span>  <span style="color: #dcaeea;">x_Train4D_normalize</span> = x_Train4D / <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">43: </span>  <span style="color: #dcaeea;">x_Test4D_normalize</span> = x_Test4D / <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">44: </span>  <span style="color: #dcaeea;">y_TrainOneHot</span> = np_utils.to_categorical(y_Train)
<span class="linenr">45: </span>  <span style="color: #dcaeea;">y_TestOneHot</span> = np_utils.to_categorical(y_Test)
<span class="linenr">46: </span>
<span class="linenr">47: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">2. &#24314;&#31435;&#27169;&#22411;</span>
<span class="linenr">48: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">49: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense, Dropout, Flatten, Conv2D, MaxPooling2D
<span class="linenr">50: </span>
<span class="linenr">51: </span>  <span style="color: #dcaeea;">model</span> = Sequential()
<span class="linenr">52: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.Conv2D(16, (5,5), activation='relu', input_shape=(28,28,1)))</span>
<span class="linenr">53: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">16</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">5</span>), padding=<span style="color: #98be65;">'same'</span>,
<span id="coderef-MNIST-CNN-1" class="coderef-off"><span class="linenr">54: </span>                  input_shape=(<span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">1</span>), activation=<span style="color: #98be65;">'relu'</span>))</span>
<span class="linenr">55: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.MaxPooling2D((2,2)))</span>
<span class="linenr">56: </span>  model.add(MaxPooling2D(pool_size=(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr">57: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.Conv2D(36, (5,5), activation='relu', input_shape=(28,28,1)))</span>
<span class="linenr">58: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">36</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">5</span>), padding=<span style="color: #98be65;">'same'</span>,
<span class="linenr">59: </span>                  input_shape=(<span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">1</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">60: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.MaxPooling2D((2,2)))</span>
<span class="linenr">61: </span>  model.add(MaxPooling2D(pool_size=(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr">62: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.Dropout(0.25))</span>
<span class="linenr">63: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.25</span>))
<span class="linenr">64: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.Flatten())</span>
<span class="linenr">65: </span>  model.add(Flatten())
<span class="linenr">66: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.Dense(128, activation='relu'))</span>
<span class="linenr">67: </span>  model.add(Dense(<span style="color: #da8548; font-weight: bold;">128</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">68: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.Dropout(0.5))</span>
<span class="linenr">69: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr">70: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.Dense(10, activation='softmax'))</span>
<span class="linenr">71: </span>  model.add(Dense(<span style="color: #da8548; font-weight: bold;">10</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">72: </span>  <span style="color: #51afef;">print</span>(model.summary())
<span class="linenr">73: </span>
<span class="linenr">74: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">3. &#35347;&#32244;&#27169;&#22411;</span>
<span class="linenr">75: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span class="linenr">76: </span>                optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">77: </span>  <span style="color: #dcaeea;">train_history</span>=model.fit(x=x_Train4D_normalize,
<span class="linenr">78: </span>                          y=y_TrainOneHot, validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr">79: </span>                          epochs=<span style="color: #da8548; font-weight: bold;">10</span>, batch_size=<span style="color: #da8548; font-weight: bold;">300</span>, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">80: </span>  <span style="color: #51afef;">print</span>(train_history)
<span class="linenr">81: </span>  show_train_history(<span style="color: #98be65;">"CNN-MNist-acc"</span>, <span style="color: #98be65;">'accuracy'</span>, <span style="color: #98be65;">'val_accuracy'</span>)
<span class="linenr">82: </span>  show_train_history(<span style="color: #98be65;">"CNN-MNist-loss"</span>, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>)
<span class="linenr">83: </span>
<span class="linenr">84: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">4. &#35413;&#20272;&#27169;&#22411;&#28310;&#30906;&#29575;</span>
<span class="linenr">85: </span>  <span style="color: #dcaeea;">scores</span> = model.evaluate(x_Test4D_normalize, y_TestOneHot)
<span class="linenr">86: </span>  <span style="color: #51afef;">print</span>(scores[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">87: </span>
<span class="linenr">88: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">5. &#36914;&#34892;&#38928;&#28204;</span>
<span class="linenr">89: </span>  <span style="color: #dcaeea;">prediction</span> = model.predict_classes(x_Test4D_normalize)
<span class="linenr">90: </span>  <span style="color: #51afef;">print</span>(prediction[:<span style="color: #da8548; font-weight: bold;">10</span>])
<span class="linenr">91: </span>  plot_images_labels_prediction(<span style="color: #98be65;">"CNN_MNist"</span>, x_Test, y_Test, prediction, idx=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">92: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">93: </span>  <span style="color: #dcaeea;">p</span> = pd.crosstab(y_Test, prediction, rownames=[<span style="color: #98be65;">'label'</span>], colnames=[<span style="color: #98be65;">'predict'</span>])
<span class="linenr">94: </span>  <span style="color: #51afef;">print</span>(p)
</pre>
</div>

<pre class="example">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 28, 28, 16)        416       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 14, 14, 36)        14436     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 7, 7, 36)          0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 7, 7, 36)          0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 1764)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               225920    
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                1290      
=================================================================
Total params: 242,062
Trainable params: 242,062
Non-trainable params: 0
_________________________________________________________________
None

0.9930999875068665
[7 2 1 0 4 1 4 9 5 9]
predict    0     1     2     3    4    5    6     7    8    9
label                                                        
0        977     0     0     0    0    0    1     1    1    0
1          0  1133     2     0    0    0    0     0    0    0
2          0     0  1031     0    0    0    0     1    0    0
3          0     0     1  1008    0    0    0     0    1    0
4          0     0     0     0  974    0    1     0    1    6
5          2     0     0     9    0  879    1     0    0    1
6          3     2     0     0    1    4  947     0    1    0
7          0     1     2     2    0    0    0  1022    1    0
8          1     0     1     2    0    1    0     2  965    2
9          2     1     0     3    3    1    0     2    2  995
</pre>


<div id="orgc102edb" class="figure">
<p><img src="images/CNN-MNist-acc.png" alt="CNN-MNist-acc.png" /><br />
</p>
<p><span class="figure-number">Figure 129: </span>訓練精確度</p>
</div>


<div id="orgde9c126" class="figure">
<p><img src="images/CNN-MNist-loss.png" alt="CNN-MNist-loss.png" /><br />
</p>
<p><span class="figure-number">Figure 130: </span>訓練 loss 誤差</p>
</div>


<div id="org6b046a3" class="figure">
<p><img src="images/CNN_MNist.png" alt="CNN_MNist.png" /><br />
</p>
<p><span class="figure-number">Figure 131: </span>前 10 筆預測結果</p>
</div>

<p>
在上例中，第一個卷積層輸入了大小為(28, 28, 1)(第<a href="#coderef-MNIST-CNN-1" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-MNIST-CNN-1');" onmouseout="CodeHighlightOff(this, 'coderef-MNIST-CNN-1');">54</a>行)的特徵圖，輸出了大小為(5, 5, 16)的特徵圖，此處卷積層對輸入資料做了 16 種 filter 運算，在這 16 個輸出 channel 中，每個 channel 都包含了一個 5*5 的網格值，這是每個 filter 對於輸入資料的響應圖(response map)，表示該過濾器在輸入資料(影像)中各個位置(滑動)取得的回應值(經由卷積運算)所組成的影像，這就是特徵圖(feature map)這個術語的含義：深度軸中的每個維度都是一個特徵(filter)，而 2D 輸出張量[:,:,n]是第 n 個 filter 對輸入資料回應的 2D 空間圖(response map)。<br />
</p>

<p>
卷積層由兩個關鍵參數定義：<br />
</p>

<ul class="org-ul">
<li>由輸入採樣的區塊大小(size of the patches extracted from the input): 也就是掃描窗格的大小(windows_height, window_width，通常是 3*3 或 5*5)，也就是 filter 大小。<br /></li>
<li>輸出特徵圖的深度(depth of the output feature map)：也就是卷積層過濾器數量，在本例中以 16 開頭，以 36 結束。Keras 的 Conv2D 層傳的參數為：<br /></li>
</ul>

<div class="org-src-container">
<pre class="src src-python">Conv2D(output_depth, (window_height, window_width))
</pre>
</div>

<p>
卷積層的主要工作就是透過在 3D 輸入特徵圖上滑動(sliding)3&times;3 或 5&times;5 的小窗格，在該位置上取得特徵(input__{}depth 最開始是有 RGB 顏色的，但接下來就變成 filter)；其次，將每個 3D 區塊轉換(即卷積運算)成 shape=(output_depth,)的 1D 向量，然後將所有這些向量依照空間上的位置排列重新組裝成 shape=(height, width, output_output)的 3D 輸出特徵圖。<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org38e8209" class="outline-3">
<h3 id="org38e8209"><span class="section-number-3">16.6</span> 以 Keras 實作 Cifar-10</h3>
<div class="outline-text-3" id="text-16-6">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">1.  Import Library</span>
<span class="linenr">  2: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> cifar10
<span class="linenr">  3: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  4: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">  5: </span>
<span class="linenr">  6: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">2. &#36039;&#26009;&#28310;&#20633;</span>
<span class="linenr">  7: </span>  (x_img_train,y_label_train),(x_img_test,y_label_test) = cifar10.load_data()
<span class="linenr">  8: </span>  <span style="color: #dcaeea;">x_img_train_normalize</span> = x_img_train.astype(<span style="color: #98be65;">'float32'</span>) / <span style="color: #da8548; font-weight: bold;">255.0</span>
<span class="linenr">  9: </span>  <span style="color: #dcaeea;">x_img_test_normalize</span> = x_img_test.astype(<span style="color: #98be65;">'float32'</span>) / <span style="color: #da8548; font-weight: bold;">255.0</span>
<span class="linenr"> 10: </span>  <span style="color: #83898d;">'''&#27491;&#35215;&#21270;'''</span>
<span class="linenr"> 11: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 12: </span>  <span style="color: #dcaeea;">y_label_train_OneHot</span> = np_utils.to_categorical(y_label_train)
<span class="linenr"> 13: </span>  <span style="color: #dcaeea;">y_label_test_OneHot</span> = np_utils.to_categorical(y_label_test)
<span class="linenr"> 14: </span>
<span class="linenr"> 15: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">3. &#24314;&#31435;&#27169;&#22411;</span>
<span class="linenr"> 16: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr"> 17: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense, Dropout, Activation, Flatten
<span class="linenr"> 18: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Conv2D, MaxPooling2D, ZeroPadding2D
<span class="linenr"> 19: </span>  <span style="color: #98be65;">'''&#21367;&#31309;&#23652;1&#33287;&#27744;&#21270;&#23652;1'''</span>
<span class="linenr"> 20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38568;&#27231;&#29986;&#29983;32&#20491;3*3&#30340;&#28670;&#37857;&#65292;&#36664;&#20837;&#30340;&#24433;&#20687;&#28858;32*32*3(RGB)</span>
<span class="linenr"> 21: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">32</span>,kernel_size=(<span style="color: #da8548; font-weight: bold;">3</span>,<span style="color: #da8548; font-weight: bold;">3</span>), input_shape=(<span style="color: #da8548; font-weight: bold;">32</span>, <span style="color: #da8548; font-weight: bold;">32</span>, <span style="color: #da8548; font-weight: bold;">3</span>),
<span class="linenr"> 22: </span>                   activation=<span style="color: #98be65;">'relu'</span>, padding=<span style="color: #98be65;">'same'</span>)) 
<span class="linenr"> 23: </span>  model.add(Dropout(rate=<span style="color: #da8548; font-weight: bold;">0.25</span>)) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27599;&#27425;&#35347;&#32244;&#36845;&#20195;&#26178;&#26371;&#38568;&#27231;&#25918;&#26820;25%&#30340;&#31070;&#32147;&#20803;</span>
<span class="linenr"> 24: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">32</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>),
<span class="linenr"> 25: </span>                   activation=<span style="color: #98be65;">'relu'</span>, padding=<span style="color: #98be65;">'same'</span>))
<span class="linenr"> 26: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36914;&#34892;&#31532;&#19968;&#27425;&#32302;&#28187;&#21462;&#27171;&#65292;&#23559;&#24433;&#20687;&#32302;&#28858;16*16 </span>
<span class="linenr"> 27: </span>  model.add(MaxPooling2D(pool_size=(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>))) 
<span class="linenr"> 28: </span>  <span style="color: #98be65;">'''&#21367;&#31309;&#23652;2&#33287;&#27744;&#21270;&#23652;2'''</span>
<span class="linenr"> 29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#21069;&#19968;&#23652;&#20659;&#36914;&#30340;32&#20491;16*16&#24433;&#20687;&#36681;&#28858;64&#20491;16*16&#24433;&#20687; </span>
<span class="linenr"> 30: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">64</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), <span style="color: #5B6268;">### </span><span style="color: #5B6268;">&#29986;&#29983;64&#20491;&#24433;&#20687;</span>
<span class="linenr"> 31: </span>                   activation=<span style="color: #98be65;">'relu'</span>, padding=<span style="color: #98be65;">'same'</span>))
<span class="linenr"> 32: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.25</span>))
<span class="linenr"> 33: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">64</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), 
<span class="linenr"> 34: </span>                   activation=<span style="color: #98be65;">'relu'</span>, padding=<span style="color: #98be65;">'same'</span>))
<span class="linenr"> 35: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20877;&#23559;64&#20491;16*16&#24433;&#20687;&#32302;&#28187;&#21462;&#27171;&#28858;8*8</span>
<span class="linenr"> 36: </span>  model.add(MaxPooling2D(pool_size=(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 37: </span>  <span style="color: #98be65;">'''&#21367;&#31309;&#23652;3&#33287;&#27744;&#21270;&#23652;3'''</span>
<span class="linenr"> 38: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;64&#20491;8*8&#20491;&#24433;&#20687;&#36681;&#28858;128&#20491;</span>
<span class="linenr"> 39: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">128</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), 
<span class="linenr"> 40: </span>                   activation=<span style="color: #98be65;">'relu'</span>, padding=<span style="color: #98be65;">'same'</span>))
<span class="linenr"> 41: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.3</span>))
<span class="linenr"> 42: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">128</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), 
<span class="linenr"> 43: </span>                   activation=<span style="color: #98be65;">'relu'</span>, padding=<span style="color: #98be65;">'same'</span>))
<span class="linenr"> 44: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20877;&#23559;128&#20491;8*8&#24433;&#20687;&#32302;&#28187;&#21462;&#27171;&#28858;4*4</span>
<span class="linenr"> 45: </span>  model.add(MaxPooling2D(pool_size=(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 46: </span>  <span style="color: #98be65;">'''&#24314;&#31435;&#31070;&#32147;&#32178;&#36335;(&#24179;&#22374;&#23652;&#12289;&#38577;&#34255;&#23652;&#12289;&#36664;&#20986;&#23652;)'''</span>
<span class="linenr"> 47: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;128*4*4&#30340;3&#32173;&#30697;&#38499;&#36681;&#28858;1&#32173;(2048&#20491;float&#25976;&#23383;&#65292;&#23565;&#25033;&#21040;2048&#20491;&#31070;&#32147;&#20803;)</span>
<span class="linenr"> 48: </span>  model.add(Flatten())
<span class="linenr"> 49: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.3</span>))
<span class="linenr"> 50: </span>  model.add(Dense(<span style="color: #da8548; font-weight: bold;">2500</span>, activation=<span style="color: #98be65;">'relu'</span>)) <span style="color: #5B6268;">### </span><span style="color: #5B6268;">&#38560;&#34255;&#23652;1&#26377;2500&#20491;&#31070;&#32147;&#20803;</span>
<span class="linenr"> 51: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.3</span>))
<span class="linenr"> 52: </span>  model.add(Dense(<span style="color: #da8548; font-weight: bold;">1500</span>, activation=<span style="color: #98be65;">'relu'</span>)) <span style="color: #5B6268;">### </span><span style="color: #5B6268;">&#38560;&#34255;&#23652;2&#26377;1500&#31070;&#32147;&#20803;</span>
<span class="linenr"> 53: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.3</span>))
<span class="linenr"> 54: </span>  model.add(Dense(<span style="color: #da8548; font-weight: bold;">10</span>, activation=<span style="color: #98be65;">'softmax'</span>)) <span style="color: #5B6268;">### </span><span style="color: #5B6268;">10&#20491;label</span>
<span class="linenr"> 55: </span>  <span style="color: #51afef;">print</span>(model.summary())
<span class="linenr"> 56: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">4. &#36617;&#20837;&#20043;&#21069;&#35347;&#32244;&#30340;&#27169;&#22411;</span>
<span class="linenr"> 57: </span>  <span style="color: #51afef;">try</span>:
<span class="linenr"> 58: </span>      model.load_weights(<span style="color: #98be65;">"SaveModel/cifarCnnModelnew1.h5"</span>)
<span class="linenr"> 59: </span>      <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#36617;&#20837;&#27169;&#22411;&#25104;&#21151;!&#32380;&#32396;&#35347;&#32244;&#27169;&#22411;"</span>)
<span class="linenr"> 60: </span>  <span style="color: #51afef;">except</span>:
<span class="linenr"> 61: </span>      <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#36617;&#20837;&#27169;&#22411;&#22833;&#25943;!&#38283;&#22987;&#35347;&#32244;&#19968;&#20491;&#26032;&#27169;&#22411;"</span>)
<span class="linenr"> 62: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">5. &#35347;&#32244;&#27169;&#22411;</span>
<span class="linenr"> 63: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;&#21069;&#20808;&#20197;compile&#35373;&#23450;&#27169;&#22411;, &#35373;&#23450;&#20839;&#23481;&#21253;&#21547;</span>
<span class="linenr"> 64: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1. &#25613;&#22833;&#20989;&#25976;, 2. &#26368;&#20339;&#21270;&#26041;&#27861;, 3. &#35413;&#20272;&#27169;&#22411;&#30340;&#26041;&#27861;</span>
<span class="linenr"> 65: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span class="linenr"> 66: </span>                optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr"> 67: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38283;&#22987;&#35347;&#32244;, &#22519;&#34892;50&#27425;&#35347;&#32244;&#36913;&#26399;&#12289;&#27599;&#19968;&#25209;&#27425;500&#31558;&#36039;&#26009;</span>
<span class="linenr"> 68: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">40000&#31558;&#36039;&#26009;&#65292;&#27599;&#19968;&#25209;&#27425;500&#31558;&#36039;&#26009;, &#20998;&#28858;80&#25209;&#27425;&#36914;&#34892;&#35347;&#32244;</span>
<span class="linenr"> 69: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27599;&#20491;epoch&#35347;&#32244;&#23436;&#24460;&#26371;&#23384;&#19968;&#31558;accuracy&#21644;loss&#35352;&#37636;&#21040;train_history</span>
<span class="linenr"> 70: </span>  <span style="color: #dcaeea;">train_history</span>=model.fit(x_img_train_normalize, y_label_train_OneHot,
<span class="linenr"> 71: </span>                          validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 72: </span>                          epochs=<span style="color: #da8548; font-weight: bold;">50</span>, batch_size=<span style="color: #da8548; font-weight: bold;">500</span>, verbose=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 73: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 74: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(train_acc,test_acc):
<span class="linenr"> 75: </span>      plt.plot(train_history.history[train_acc])
<span class="linenr"> 76: </span>      plt.plot(train_history.history[test_acc])
<span class="linenr"> 77: </span>      plt.title(<span style="color: #98be65;">'Train History'</span>)
<span class="linenr"> 78: </span>      plt.ylabel(<span style="color: #98be65;">'Accuracy'</span>)
<span class="linenr"> 79: </span>      plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
<span class="linenr"> 80: </span>      plt.legend([<span style="color: #98be65;">'train'</span>, <span style="color: #98be65;">'test'</span>], loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr"> 81: </span>      plt.show()
<span class="linenr"> 82: </span>  show_train_history(<span style="color: #98be65;">'acc'</span>,<span style="color: #98be65;">'val_acc'</span>)
<span class="linenr"> 83: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">6. &#35413;&#20272;&#27169;&#22411;&#28310;&#30906;&#29575;</span>
<span class="linenr"> 84: </span>  <span style="color: #dcaeea;">scores</span> = model.evaluate(x_img_test_normalize, 
<span class="linenr"> 85: </span>                          y_label_test_OneHot, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 86: </span>  scores[<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr"> 87: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">7. &#36914;&#34892;&#38928;&#28204;</span>
<span class="linenr"> 88: </span>  <span style="color: #dcaeea;">prediction</span>=model.predict_classes(x_img_test_normalize)
<span class="linenr"> 89: </span>  <span style="color: #51afef;">print</span>(prediction[:<span style="color: #da8548; font-weight: bold;">10</span>])
<span class="linenr"> 90: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">8. &#26597;&#30475;&#38928;&#28204;&#32080;&#26524;</span>
<span class="linenr"> 91: </span>  <span style="color: #dcaeea;">label_dict</span>={<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #98be65;">"airplane"</span>,<span style="color: #da8548; font-weight: bold;">1</span>:<span style="color: #98be65;">"automobile"</span>,<span style="color: #da8548; font-weight: bold;">2</span>:<span style="color: #98be65;">"bird"</span>,<span style="color: #da8548; font-weight: bold;">3</span>:<span style="color: #98be65;">"cat"</span>,<span style="color: #da8548; font-weight: bold;">4</span>:<span style="color: #98be65;">"deer"</span>,
<span class="linenr"> 92: </span>              <span style="color: #da8548; font-weight: bold;">5</span>:<span style="color: #98be65;">"dog"</span>,<span style="color: #da8548; font-weight: bold;">6</span>:<span style="color: #98be65;">"frog"</span>,<span style="color: #da8548; font-weight: bold;">7</span>:<span style="color: #98be65;">"horse"</span>,<span style="color: #da8548; font-weight: bold;">8</span>:<span style="color: #98be65;">"ship"</span>,<span style="color: #da8548; font-weight: bold;">9</span>:<span style="color: #98be65;">"truck"</span>}
<span class="linenr"> 93: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 94: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_images_labels_prediction</span>(images,labels,prediction,
<span class="linenr"> 95: </span>                                    idx,num=<span style="color: #da8548; font-weight: bold;">10</span>):
<span class="linenr"> 96: </span>      <span style="color: #dcaeea;">fig</span> = plt.gcf()
<span class="linenr"> 97: </span>      fig.set_size_inches(<span style="color: #da8548; font-weight: bold;">12</span>, <span style="color: #da8548; font-weight: bold;">14</span>)
<span class="linenr"> 98: </span>      <span style="color: #51afef;">if</span> num&gt;<span style="color: #da8548; font-weight: bold;">25</span>: <span style="color: #dcaeea;">num</span>=<span style="color: #da8548; font-weight: bold;">25</span> 
<span class="linenr"> 99: </span>      <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">0</span>, num):
<span class="linenr">100: </span>          <span style="color: #dcaeea;">ax</span>=plt.subplot(<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">1</span>+i)
<span class="linenr">101: </span>          ax.imshow(images[idx],cmap=<span style="color: #98be65;">'binary'</span>)
<span class="linenr">102: </span>
<span class="linenr">103: </span>          <span style="color: #dcaeea;">title</span>=<span style="color: #c678dd;">str</span>(i)+<span style="color: #98be65;">','</span>+label_dict[labels[i][<span style="color: #da8548; font-weight: bold;">0</span>]]
<span class="linenr">104: </span>          <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(prediction)&gt;<span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr">105: </span>              <span style="color: #dcaeea;">title</span>+=<span style="color: #98be65;">'=&gt;'</span>+label_dict[prediction[i]]
<span class="linenr">106: </span>
<span class="linenr">107: </span>          ax.set_title(title,fontsize=<span style="color: #da8548; font-weight: bold;">10</span>) 
<span class="linenr">108: </span>          ax.set_xticks([]);ax.set_yticks([])        
<span class="linenr">109: </span>          <span style="color: #dcaeea;">idx</span>+=<span style="color: #da8548; font-weight: bold;">1</span> 
<span class="linenr">110: </span>      plt.show()
<span class="linenr">111: </span>
<span class="linenr">112: </span>  plot_images_labels_prediction(x_img_test,y_label_test,
<span class="linenr">113: </span>                                prediction,<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">114: </span>
<span class="linenr">115: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">9. &#26597;&#30475;&#38928;&#28204;&#27231;&#29575;</span>
<span class="linenr">116: </span>  <span style="color: #dcaeea;">Predicted_Probability</span>=model.predict(x_img_test_normalize)
<span class="linenr">117: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_Predicted_Probability</span>(y,prediction,
<span class="linenr">118: </span>                                 x_img,Predicted_Probability,i):
<span class="linenr">119: </span>      <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'label:'</span>,label_dict[y[i][<span style="color: #da8548; font-weight: bold;">0</span>]],
<span class="linenr">120: </span>            <span style="color: #98be65;">'predict:'</span>,label_dict[prediction[i]])
<span class="linenr">121: </span>      plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr">122: </span>      plt.imshow(np.reshape(x_img_test[i],(<span style="color: #da8548; font-weight: bold;">32</span>, <span style="color: #da8548; font-weight: bold;">32</span>,<span style="color: #da8548; font-weight: bold;">3</span>)))
<span class="linenr">123: </span>      plt.show()
<span class="linenr">124: </span>      <span style="color: #51afef;">for</span> j <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">10</span>):
<span class="linenr">125: </span>          <span style="color: #51afef;">print</span>(label_dict[j]+
<span class="linenr">126: </span>                <span style="color: #98be65;">' Probability:%1.9f'</span>%(Predicted_Probability[i][j]))
<span class="linenr">127: </span>
<span class="linenr">128: </span>  show_Predicted_Probability(y_label_test,prediction,
<span class="linenr">129: </span>                             x_img_test,Predicted_Probability,<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">130: </span>  show_Predicted_Probability(y_label_test,prediction,
<span class="linenr">131: </span>                             x_img_test,Predicted_Probability,<span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">132: </span>
<span class="linenr">133: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">10. confusion matrix</span>
<span class="linenr">134: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">135: </span>  <span style="color: #51afef;">print</span>(label_dict)
<span class="linenr">136: </span>  pd.crosstab(y_label_test.reshape(-<span style="color: #da8548; font-weight: bold;">1</span>),prediction,
<span class="linenr">137: </span>              rownames=[<span style="color: #98be65;">'label'</span>],colnames=[<span style="color: #98be65;">'predict'</span>])
<span class="linenr">138: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">11. Save model to JSON</span>
<span class="linenr">139: </span>  <span style="color: #dcaeea;">model_json</span> = model.to_json()
<span class="linenr">140: </span>  <span style="color: #51afef;">with</span> <span style="color: #c678dd;">open</span>(<span style="color: #98be65;">"SaveModel/cifarCnnModelnew.json"</span>, <span style="color: #98be65;">"w"</span>) <span style="color: #51afef;">as</span> <span style="color: #dcaeea;">json_file</span>:
<span class="linenr">141: </span>      json_file.write(model_json)
<span class="linenr">142: </span>
<span class="linenr">143: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">12. Save Model to YAML</span>
<span class="linenr">144: </span>  model_yaml = model.to_yaml()
<span class="linenr">145: </span>  <span style="color: #51afef;">with</span> <span style="color: #c678dd;">open</span>(<span style="color: #98be65;">"SaveModel/cifarCnnModelnew.yaml"</span>, <span style="color: #98be65;">"w"</span>) <span style="color: #51afef;">as</span> yaml_file:
<span class="linenr">146: </span>      yaml_file.write(model_yaml)
<span class="linenr">147: </span>
<span class="linenr">148: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">13. Save Weight to h5</span>
<span class="linenr">149: </span>  model.save_weights(<span style="color: #98be65;">"SaveModel/cifarCnnModelnew.h5"</span>)
<span class="linenr">150: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"Saved model to disk"</span>)
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgbb0aef1" class="outline-2">
<h2 id="orgbb0aef1"><span class="section-number-2">17</span> Keras GAN</h2>
<div class="outline-text-2" id="text-17">
<p>
<a href="https://github.com/eriklindernoren/Keras-GAN">https://github.com/eriklindernoren/Keras-GAN</a><br />
</p>
</div>
</div>

<div id="outline-container-org23cb4e7" class="outline-2">
<h2 id="org23cb4e7"><span class="section-number-2">18</span> 深度學習</h2>
<div class="outline-text-2" id="text-18">
<p>
深度學習是加深層數後的多層神經網路。MNIST 歷年的競賽前幾名都是以 CNN 為基礎，進一步提高辨識準確率的方法還包括整體學習、學習率遞減（learning rate decay）、資料擴增（Data Augmentation, 如利用旋轉、垂直或水平移動輸入影像來小幅改變輸入資料以增加輸入影像張數）。<br />
關於增力層數的重要性，目前還缺乏理論佐證，但從過往的研究或實驗中，有幾點可以說明。<br />
</p>
<ol class="org-ol">
<li>在 ILSVRC 這種大型視覺辨識競賽結果中，加深層數的比例多與辨識效能成正比。<br /></li>
<li>加深層數可以在減少網路參數的狀況下得到相同成效，透過重叠層級，可以讓 ReLU 等活化函數夾在卷積層之間，進一步提高網路的表現力，因為透過活化函數，可以在網路增加「非線性」的能力，重叠非線性性函數，也能達到更複雜的表現力。<br /></li>
<li>學習的效率也是加深層數的優點之一，卷積層的神經元會反應出邊界等單純形狀，隨著層數增加，可以反應出紋理、物體部位等特質，依照階層逐漸變複雜。<br /></li>
<li>以辨識「狗」為例子，如果要以層數較少的網路來解決這個問題，卷積層就要一次「理解」眾多特徵，還要因應不同拍攝環境帶來的變化，一次處理這些龐大的資料會花費許多學習時間； 如果加深層數，就能用階層分解必須學習的問題，每一層可以處理單純的問題，例如，最初的層級可以只學習邊界，利用少量的學習資料來進行效率化的學習。<br /></li>
<li>加深層數可以階層性的傳遞資料，例如，擷取出邊界的下一層會使用邊界資料來學習更高階的問題（如判斷形狀）。<br /></li>
</ol>


<p>
典型的深度學習如圖<a href="#org028554b">132</a>，在此例中，輸入為一張手寫數字的影像，經由 4 層的深度學習模型後得知此數字為 4。<br />
</p>


<div id="org028554b" class="figure">
<p><img src="images/img-191107113927.jpg" alt="img-191107113927.jpg" /><br />
</p>
<p><span class="figure-number">Figure 132: </span>典型的深度神經網路-1</p>
</div>

<p>
圖<a href="#orgb7d9f00">133</a>進一步說明網路模型中每一層的作用，可以將每一層網路視為對影像的特殊運算，如此一層一層逐一精煉(purified)，最後得到結果。<br />
</p>


<div id="orgb7d9f00" class="figure">
<p><img src="images/img-1911071139277.jpg" alt="img-1911071139277.jpg" /><br />
</p>
<p><span class="figure-number">Figure 133: </span>典型的深度神經網路-2</p>
</div>
</div>

<div id="outline-container-org40ab2dc" class="outline-3">
<h3 id="org40ab2dc"><span class="section-number-3">18.1</span> 深度學習運作原理</h3>
<div class="outline-text-3" id="text-18-1">
<p>
前歷深度學習中的每一「層」(layer)如何運作，取決於儲存於該層的權重(weight)，而權重是由多個數字組成。從技術層面來看，layer 是由各個權重參數(parameters)來和輸入的資料進行運算以執行資料轉換的工作(如圖<a href="#org9de6cea">134</a>)。而所謂的學習，指的就是幫助神經網路的每一層找出適當的權重值，讓神經網路可以將輸入的訓練資料經由與權重的運作推導出接近標準答案的運算結果(即圖<a href="#org9de6cea">134</a>中的預測 Y)。然而，這在實際運作上是十分困難的，因為一個深度神經網路可以包含數千萬個權重，此外，其中一個權重被改變後，往往會影響其他權重的運作。<br />
</p>


<div id="org9de6cea" class="figure">
<p><img src="images/img-191107115233.jpg" alt="img-191107115233.jpg" /><br />
</p>
<p><span class="figure-number">Figure 134: </span>nn 中 layer 的 parameter</p>
</div>

<p>
為了提高神經網路的效能(預測的準確率)，我們要即時的掌握目前的輸出(Y)與真正的標準答案還差多少，這個評估由神經網路的損失函數(loss function, 或稱目標函數, objective function)來負責，如圖<a href="#org7c2c37e">135</a>。損失函數會取得神經網路的預測結果與標準答案二者的損失分數(又稱差距分數)，做為每一次學習的表現效能之評估標準。<br />
</p>



<div id="org7c2c37e" class="figure">
<p><img src="images/img-191107115304.jpg" alt="img-191107115304.jpg" /><br />
</p>
<p><span class="figure-number">Figure 135: </span>損失函數</p>
</div>

<p>
而深度學習的基本工作就是使用損失函數做為回饋訊息來一步步微調權重，逐步降低每次學習的損失分數，最終目標在於讓損失函數結果達到最小，而這個微調工作則由優化器(optimizer，也稱最佳化函數)來執行。優化器實作了反向傳播演算法(Backpropagation)，這也是深度學習中的核心演算法，藉此來週整權重。<br />
</p>


<div id="orgb30af12" class="figure">
<p><img src="images/img-1911071153041.jpg" alt="img-1911071153041.jpg" /><br />
</p>
<p><span class="figure-number">Figure 136: </span>優化器</p>
</div>

<p>
那麼，在最初一次的學習，權重的值是如何設定的呢？可以先全數設為零，但更常用的做法是隨機指定，隨著多次學習後，權重會逐步往正確的方向調整，損失分數也會慢慢降低。<br />
</p>
</div>
</div>

<div id="outline-container-org3fe0ce7" class="outline-3">
<h3 id="org3fe0ce7"><span class="section-number-3">18.2</span> 深度學習應用領域</h3>
<div class="outline-text-3" id="text-18-2">
</div>
<ol class="org-ol">
<li><a id="org08f713e"></a>影像辨識<br />
<ol class="org-ol">
<li><a id="orgabeed8a"></a>卷積神經網路 CNN<br />
<div class="outline-text-5" id="text-18-2-1-1">
<p>
傳統機器學習進行圖片識別，主要是希望能透過原始像數值找出一種適合的分類器(classifier)，但事實證明這麼做不管用，因為信噪比太低。後來的改善方式是由人類挑選出重要特徵，然後由機器學習演算法使用這些「特徵向量(feature vectors)」進行分類判斷。這種特徵提取(feature extraction)的做法確實改善了信噪比，但是如果圖片的重要特點因光線或其他因素難以識別，則精確率會降低很多，而且，事前的人工挑選特徵花去太多人力，以深度學習進行圖片視覺就是設法消除那些既繁瑣又會造成侷限性的特徵選取程序。David Hubel 和 Torsten Wiesel 發現動物視覺皮層有一部份專門負責檢測邊緣，1959 年他們把電極插入貓的大腦中，在螢幕上投射出黑白圖案，發現有些神經元只有在出現垂直線時被激發，有些則只有在出現水平線時被激發，有些則是只有看到某特定角度的線時被激發。進一步的研究確認，視覺皮層是以分層的結構組織起來的，每一層都會根據前一層所偵測到的特徵得出進一步的訊息，從線條、輪廓、形狀，一直到整個物體。由上述研究得來的第一個概念就是「過濾器(filter)」。<br />
典型的過濾器如下：<br />
</p>
<ul class="org-ul">
<li>blur = [[1./9, 1./9, 1./9], [1./9, 1./9, 1./9], [1./9, 1./9, 1./9]]<br /></li>
</ul>

<div id="org4af33cb" class="figure">
<p><img src="images/blur-filter.png" alt="blur-filter.png" /><br />
</p>
<p><span class="figure-number">Figure 137: </span>模糊過濾器</p>
</div>
<ul class="org-ul">
<li>edges = [[1, 1, 1], [1, -8, 1], [1, 1, 1]]<br /></li>
</ul>
<p>
<img src="images/edges-filter.png" alt="edges-filter.png" /><br />
圖<a href="#org4af33cb">137</a>為一 3*3 的模楜強過濾器產生的效果，圖<a href="#org829ad3c">150</a>則為邊緣強週器的效果。過濾器可以改變圖形，並顯示可用於「圖形偵測」和「圖形分類」的特徵。例如，為了對數字進行分類，內部的顏色並不重要，此時，邊緣強調過濾器就有助於辨識數字的一般形狀，進而提升數字識別效能。<br />
</p>

<p>
我們可以用「類神經網路」的方式來理解「過濾器」，將我們定義的「過濾器」視為一組加權，最終的值又做為下一層的啟動值（輸入）。如圖<a href="#org2aa58a2">138</a>，過濾器會逐次掃過整張圖，然後建立一組新的圖片，<br />
</p>

<div id="org2aa58a2" class="figure">
<p><img src="images/filter-scanner.png" alt="filter-scanner.png" /><br />
</p>
<p><span class="figure-number">Figure 138: </span>過濾器的掃瞄計算</p>
</div>
</div>
</li>
</ol>
</li>

<li><a id="org98a528e"></a>語言模型<br />
<ol class="org-ol">
<li><a id="orge075654"></a>遞迴類神經網路(Recurrent Neural Networks, RNNs)<br />
<div class="outline-text-5" id="text-18-2-2-1">
<p>
RNN 能夠處理「任意個數的輸入序列」，所以十分適合用在「語言塑模」或「語音辨識」。理論上，RNN 可以用來處理任何問題，因戈大火弓它已被證明具有「圖靈完備性」(Turing-Complete)。以遞迴關係的函數表示 RNN 可將其視為 \(S_t=f(S_{t-1},X_t)\)，這裡的\(S_t\)表示第\(t\)步的狀態，它是由函數\(f\)對上一步(\(t-1\))的狀態(即\(S_{t-1}\))與這一步的輸入\(X_t\)所計算出來的結果，這裡的函數\(f\)可以是任何可微分的函數，如\(S_t=tang(S_{t-1}*W+X_t*U)\)。<br />
正因為每個狀態都會與之前所有的計算有關，其所代表的重要含義為：隨著時間的推移，RNNs 可以說是有記憶力的，因為狀態 S 包含了之前所有步驟的資訊。<br />
</p>

<p>
語言塑模的目標是計算「字的序列」的機率，這在「語音辨識」、OCR、「機器翻譯」、「拼字校正」上都非常重要。以「字」為基準的「語言模型」是由「字的序列」來定義機率分佈，給定一個長度為\(m\)的字序列，它會為整個字序列給定一個機率\(P(w_1,...,w_m)\)，其「聯合機率」(joint probability)可以由公式\eqref{org495a379}中的連鎖規則(chain rule)計算出來：<br />
</p>
\begin{equation}
\label{org495a379}
P(w_1,...,w_m)=P(w_1)P(w_2|w_1)P(w3|w_2,w_1)...P(w_m|w_1,...,w_{m-1})
\end{equation}

<p>
這個聯合機率一般是基於一個「獨立性假設」(independence assumption)，即，第 i 個字只會相依於它之前的 n-1 個字，如果我們的模型是連續 n 個字的聯合機率，就稱為「n元」(n-gram)。例：<br />
</p>
<ul class="org-ul">
<li>1-gram / unigram: &ldquo;The&rdquo;, &ldquo;quick&rdquo;, &ldquo;brown&rdquo; and &ldquo;fox&rdquo;<br /></li>
<li>2-grams / bigram: &ldquo;The quick&rdquo;, &ldquo;quick brown&rdquo; and &ldquo;brown fox&rdquo;<br /></li>
<li>3-grams / trigram: &ldquo;The quick brown&rdquo; and &ldquo;quick brown fox&rdquo;<br /></li>
<li>4-grams: &ldquo;The quick brown fox&rdquo;<br /></li>
</ul>

<p>
現在，如果我們有一個巨大的語料庫(corpus of text)，我們就可以用一個特定的 n(通常為 2-4)搜尋所有「n元」在「語料庫」中出現的次數，進而在「給定前 n-1 個字的前提下」，估計出每個 n 元中最後一個字出現的機率。<br />
</p>
</div>
</li>
</ol>
</li>

<li><a id="orgaaa659e"></a>棋盤遊戲<br />
<div class="outline-text-4" id="text-18-2-3">
<p>
大約在 50 年代，研究人員開始建立具有 AI 的遊戲，這些遊戲以「西洋跳棋」(checkers)和「西洋棋」(chess)為主，這兩種遊戲有一些共同之處：<br />
</p>
<ul class="org-ul">
<li>它們是所謂的「零和遊戲」(zero-sum games)，即一個玩家所得到的奬勵就來自另一個玩家相對應的損失。另一類相對的遊戲則是指兩位玩家可以選擇合作，如 「囚徒困境」(prisoner&rsquo;s dilemma)。<br /></li>
<li>它們都具有「完全資訊」(perfect information)，兩方不同玩家都知道遊戲的整個狀態；另一種相對的遊戲則是撲克。因為得知目前狀態就可以導出最好的行動，所以這種遊戲可以減少 AI 所需處理問題的複雜度。<br /></li>
<li>兩種遊戲都有「明確性」(deterministic): 如果一個玩家下了一步，這步就會導致一個明確的下一個狀態；另一種相對的遊戲中，玩家下的一步可能是丟一次骰子或是抽一張牌，這就無法導致一個明確的下一步。<br /></li>
</ul>
</div>
</li>

<li><a id="org75736b1"></a>電腦遊戲<br /></li>
<li><a id="orgfe265a2"></a>異常偵測<br /></li>
<li><a id="org5b93b0a"></a>ex: 入侵偵測系統<br /></li>
</ol>
</div>

<div id="outline-container-orgdc187f5" class="outline-3">
<h3 id="orgdc187f5"><span class="section-number-3">18.3</span> 深度學習的幾種類型</h3>
<div class="outline-text-3" id="text-18-3">
</div>
<ol class="org-ol">
<li><a id="org86dfb59"></a>VGG<br />
<div class="outline-text-4" id="text-18-3-1">
<p>
VGG 為由卷積層與池化層構成的基本 CNN。特色是含權重層（卷積層及全連接層）共 16-19 層，有時會稱為 VGG16 或 VGG19。VGG 由於結構非常簡單，應用性高，所以多數技術人員喜歡使用以 VGG 為最基礎的網路。<br />
</p>
</div>
</li>

<li><a id="orgbc8ff65"></a>GoodLeNet<br />
<div class="outline-text-4" id="text-18-3-2">
<p>
GoogLeLeNet 基本上與 CNN 相同，其特色是不僅會往垂直方向加深網路，也會往水平方向加深。GoogLeNet 往水平方向的做法稱為「Inception 結構」。<br />
</p>
</div>
</li>

<li><a id="org8cdd96f"></a>ResNet<br />
<div class="outline-text-4" id="text-18-3-3">
<p>
ResNet 是由 Microsoft 團隊開發的網路，特色是具有能加深比過去更多層的「結構」，為了解決因加深過多層數無法順利學習的問題，ResNet 導入了「跳躍結構」（也稱為捷徑或分流）。跳躍結構是「直接」傳遞輸入資料，所以在反向傳播時，也會將上層的梯度「直接」傳遞給下層。透過這種跳躍結構，不用擔心梯度變小（或變得太大），可以把「具有意義的梯度」傳遞給上層。因此，跳躍結構能減少之前因為加深層數，使得梯度變小，出現梯度消失的問題。<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org0490260" class="outline-3">
<h3 id="org0490260"><span class="section-number-3">18.4</span> 深度學習的高速化</h3>
<div class="outline-text-3" id="text-18-4">
<p>
由於大數據與大型網路的關係，使得深度學習必須進行大量運算，過去我們使用 CPU 來進行運算，如今多數深度學習的框架多支援 GPU，甚至支援以多個 GPU 與多台裝置進行分散式學習。GPU 原本是圖形專用處理器，可以快速處理平行運算，GPU 運算的目標是把其強大的效能運用在各種用途。比較 CPU 與 GPU 在 AlexNet 的學習，CPU 需花費 40 天以上，GPU 則可以在 6 天內完成。<br />
</p>

<p>
利用 GPU 除了可以大幅提升深度學習的運算速度，但是一旦變成多層網路時，就需要花費數天或數週的時間來學習，Google 的 TensorFlow、Microsoft 的 CNTK 便是針對分散式學習來開發的，100 個分散式的 GPU 可以提升比單一 GPU 高到 56 倍的速度，意味著原本要有天才能完成的學習，只要 3 小時就可以結束。<br />
</p>

<p>
在深度學習的高速化過程中，包含運算量在內，記憶體容量、匯流排頻寬等，都會造成瓶頸，就記憶體容量來說，必須考慮到大量權重參數及中間資料會儲存在記憶體的情況。至於匯流排頻寛，一旦通過 GPU(或 CPU)的匯流排資料超過一定的限制，該處就會形成瓶頸，所以，最好能儘量減少通過網路的資料位元數。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="orgba23e3d"></a>GPU v.s. CPU<br />
<div class="outline-text-4" id="text-18-4-1">
<ul class="org-ul">
<li>CPU 是由幾個每次可處理數個獨立「執行緒」(threads)的核心(core)所組成；GPU 則有數百個這樣的核心，同時可以處理上千個執行緒<br /></li>
<li>CPU 主要是線性執行； GPU 則是個高度平行化的單元<br /></li>
<li>CPU 的發展主要致力於最佳化系統的遲滯時間，讓系統能有迅速流暢的反應；GPU 的發展則是朝頻寬最佳化努力。在深度神經網路中，頻寬為主要的系統瓶頸<br /></li>
<li>GPU 的 Level 1 cache 比 CPU 快且大，在深度神經網路中，大部份的資料都會再次被使用到<br /></li>
</ul>
</div>
</li>
</ol>
</div>

<div id="outline-container-org66535fb" class="outline-3">
<h3 id="org66535fb"><span class="section-number-3">18.5</span> 深度學習的應用範例</h3>
<div class="outline-text-3" id="text-18-5">
<ol class="org-ol">
<li>物體偵測：從影像中分析出物體位置，進行分類。物體偵測比物體辨識的問題更困難，最著名的方式為 R-CNN，R-CNN 的實際處理流程有點複雜，包括把影像變形成正方形，使用 SVM 分類。<br /></li>
<li>影像分割：指針對影像以像素標籤進行類別分類，利用神經網路進行影像分割，最簡單的方法就是以全部的像素為對象，再依照各個像素進行推論處理。典型做法為 FCN(Fully Convolutional Network)，相對於一般 CNN 含有全連接層的情況，FCN 把全連接層更換成「執行相同動作的卷積層」，在物體辨識的網路全連接層中，中間資料的空間大小當作排列成 1 行節點來處理。<br /></li>
<li>產生圖說：針對影像自動產生說明該影像的內容，代表性方法為 NIC (Neural Image Caption)模型，NIC 是由處理多層 CNN 與自然語言的 RNN(Recurrent Neural Network)所構成，RNN 指擁有遞迴功能的網路，常用在自然語言、時間序列資料等有連續性的資料上。<br /></li>
<li>影像風格轉換：代表論文為 A Neural Algorithm of Artistic Style。<br /></li>
<li>產生影像：例如，從零開始產生「臥室」影像，代表性方法為 DCGAN(Deep Convolutional Generative Adversarial Network)。DCGAN 利用大量影像（如大量拍攝臥室影像）來學習，結束學習後，只要利用該模組就能產生新的影像。DCGAN 運用了 Generator(生成器)與 Discriminator(判別器)等兩個神經網路，Generator 產生與本尊相似的影像，Discriminator 判斷是否為本尊，即，確定是由 Generator 產生的影像或是實際拍攝的影像。兩者彼此制䚘學習，Generator 可以學習到更精巧的偽裝影像技術，Discriminator 則學習更高的鑑定技能，二者相互切磋成長，最終，Generator 能學會畫出與本尊一模一樣的影像。<br /></li>
<li>自動駕駛：最近在辨識周圍環境的技術中，深度學習的能力頗受期待，例如以 CNN 為基礎的網路 SegNet 即可精確辨識走路的環境。<br /></li>
<li>Deep Q-Network (強化學習): 人類是透過嚐試錯誤來學習，例如騎腳踏車，在電腦領域中，也有從嚐試錯誤的過程中進行自主學習的例子，稱為強化學習(reinforcement learning)。在強化學習中，代理人(Agent)根據環境狀況來決定要採取的行動，利用該行動讓㼈境變化。隨環境變化，代理人獲得某些報酬。強化學習的目的是決定代理人的行動方針，以獲得更好的報酬。典型的 DQN 可以讓遊戲自動學習，達到超越人類等級的能力，使用 DQN 的 CNN 可以輸入遊戲影像(如連續 4 個畫面)，最後針對遊戲的控制器動作(搖桿的動作與按鈕)分別輸出該動作的「價值」。由於 DQN 的輸入只是影像，所以不用隨著遊戲的不同來改變設定，同一套 DQN 可以學習「小精靈」與「Atari」。DQN 與 AlphaGo 都是 Google Deep Mind 公司的研究。<br /></li>
</ol>
</div>
</div>

<div id="outline-container-org9c28e00" class="outline-3">
<h3 id="org9c28e00"><span class="section-number-3">18.6</span> 梯度遞減問題</h3>
<div class="outline-text-3" id="text-18-6">
</div>
<ol class="org-ol">
<li><a id="org655d65d"></a>深度網路誤差曲面的局部極小值<br />
<div class="outline-text-4" id="text-18-6-1">
<p>
最佳化深度學習模型的挑戰在於我們只能運用局部的訊息去推斷誤差曲面的整體結構，雖然梯度遞減法可以確保我們找到極小值，但若曲面結構非碗型（即，存在不只一個谷地，或稱局部極小值），則即便我們探用隨機誤差曲面演算法，也是無法解決問題。<br />
局部極小值與「模型可區分性(model indentifiability)」的概念有關，在全連接(fully-conntectd)的正向饋送神經網路中，同一層的神經元就算重新排列組合，網路末端還是會出現相同的最終輸出，結果，一層有 n 個神經元的網路就存在\(n!\)種排列方式，對於有 l 層的深度網路而言，則其等效配置方式就有\(n!^l\)種。結果，不論送進什麼輸入值，表現出來的行為也全都相同而無法區分；換言之，無論用的是訓練組、驗證組、測試組的樣本，所有的這些等效配置都會表現出相同的誤差。<br />
局部極小值不是太嚴重的問題，但若找到的是「假的（spurious）」局部極小值則就是個大問題，所謂假的局部極小值指的是它在神經網路中所對應的權重值，會比真正的整體最小值所對應的權重值帶來更大的誤差），從事深度學習的人總是把訓練深度網路時所遇到的問題歸咎於假的局部極小值。想解決這個問題，有個天真的想法：在訓練深度神經網路的過程中，同時畫出誤差函數隨時間而變的值，但是這個策略並不能針對誤差曲面提供足夠的訊息，因為我們很難判斷誤差的變化是來自曲面本身的「顛簸」或是因為遲遲無法找到最佳的前進方向。<br />
Goodfellow 等人<sup><a id="fnr.19" class="footref" href="#fn.19">19</a></sup>（Google 和 Standford 合作的研究小組）在 2014 年發表一篇論文試圖解決上述問題，他們沒有去分析誤差的函數隨時間而變的情況，而是在隨機選取的初始化參數向量和最後真正的最佳點之間，運用線性插值取點，再觀察這些插值點在誤差曲面上呈現什麼樣的變化，也就是說，只要給定一個隨機初始化參數向量\(\theta_i\)，加上隨機梯度遞減法(SBD)最後找到的最佳點\(\theta_f\)，我們就可以沿著線性插值的每個點，計算出相應的誤差函數值\(\theta_\alpha = \alpha \cdot \theta_f + (1-\alpha) \cdot \theta_i \)。<br />
</p>

<p>
Goodfellow 等人的研究顯示，對於各種具有不同型態神經元的實際網路而言，參數空間中隨機選取的初始點數與隨機梯度遞減最佳解之間直接相連的路經，並不會受到局部極小值的影響；換言之，我們應該把重點放在「尋找合適的前進方向」上。<br />
</p>
</div>
</li>

<li><a id="orgdc1733f"></a>找出正確的移動軌跡<br />
<div class="outline-text-4" id="text-18-6-2">
<p>
梯度通常不是尋找最小值時最好的移動軌跡參考指標，最佳應用時機是等高線為完美㘣形，然而多數等高線均為楕圓，此時梯度所指的方向就會與正確方向有所偏差。對參數空間中的每個權向\(w_i\)來說，梯度計算的是\(\frac{\partial{E}}{\partial{w_i}}\)，代表當\(w_i\)被改變時，誤差如何隨之變化的程度。因此，只要綜合考慮參數空間的所有權重，梯度就可以給出遞減最快的方向；然而，當我們朝著這個方向移動一步後，此時的梯度又會隨之改變。<br />
</p>

<p>
進一步量化我們往某方向移等時腳下梯度變化的程度，我們必須計算二階導函數，即求出\(\farc{\partial{\frac{ \parital{E}}{\partial{w_j} }}}{\partial{w_i}}\)，代表當我們改變\(w_i\)的值時，梯度中的分量\(w_j\)如何隨之而改變。將這些訊息編寫之的矩陣稱之為「海森矩陣 (Hessian matrix)」，在描述誤差曲面時，如果我們往遞減最快方向移動，腳下的梯度也跟著改變，我們就會說這是個病態(ill-conditioned)矩陣。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org648db89"></a>動量<br />
<div class="outline-text-5" id="text-18-6-2-1">
<p>
病態海森矩陣的問題往往會以梯度大幅波動的形式表現出來，因此我們可以考慮如何在訓練期間消除這些波動。想像一顆球滾落至誤差曲面中，最終一定會抵達曲面的最低點，而且不會有大幅波動。球的平滑滾落動作不只受到加速度的影響，也受到「速度」的影響，而球的速度以一種記憶的形式讓球往最低方向更有效的累積移動量，同時抵消正交(orthogonal)方向上的振盪加速度；為了模擬出球體的自然動作，我們可以在最佳化演算法中以某種方式引入速度的概念，也就是追蹤之前梯度的「指數加權衺減量」。換言之，我們用一個「動量超參數」\(m\)，以決定在新的更新值中，前一次速度要保留多少比例，藉此把我們對前一個梯度值的「記憶」添加至目前最新的梯度值中。這種做法所運用到的概念通常就動為「動量(momentum)」。<br />
</p>
</div>
</li>
<li><a id="org37b6f0c"></a>Nesterow 動量<br />
<div class="outline-text-5" id="text-18-6-2-2">
<p>
為 Sutskever 等人在 2013 年，基於改進古典動量技術所提出的動量替代方案<br />
</p>
</div>
</li>
<li><a id="orgcf512cf"></a>共軛梯度遞減(conjugate gradient descent)<br />
<div class="outline-text-5" id="text-18-6-2-3">
<p>
這是試圖改進單純最陡遞減法的另一做法，最陡遞減法是計算梯度方向，然後沿此方向搜索最小值，跳到最小值處再重新計算，實際情況則會大幅波動，這是因為每次往最陡方向移動，往往會稍微抵消另一方向的進展，補救方式是不往最陡方向移動，而是相對先前所選擇的方向，往其「共軛方向(conjugate direction)」移動。<br />
</p>
</div>
</li>
<li><a id="org81482e3"></a>BFGS(Broyden-Fletcher-Goldfarb-Shanno)<br />
<div class="outline-text-5" id="text-18-6-2-4">
<p>
以迭代方式計算海森矩陣的逆矩陣，以有效最佳化參數向量<br />
</p>
</div>
</li>
<li><a id="orga02cb1d"></a>L-BFGS<br />
<div class="outline-text-5" id="text-18-6-2-5">
<p>
解決 BFGS 佔用記憶體的問題<br />
</p>
</div>
</li>
</ol>
</li>

<li><a id="orgedcc79c"></a>學習率自動調整<br />
<ol class="org-ol">
<li><a id="org50e90c2"></a>AdaGrad<br />
<div class="outline-text-5" id="text-18-6-3-1">
<p>
根據累積歷史梯度，對整體學習率進行自動調整 由 Duchi 等人在 2011 年提出<br />
</p>
</div>
</li>
<li><a id="org1b27238"></a>RMSProp<br />
<div class="outline-text-5" id="text-18-6-3-2">
<p>
累似以動量抑制梯度波動的做法，改以指數加權移動平均，將很久以前的值也納入考慮。<br />
</p>
</div>
</li>
<li><a id="org2cc85e3"></a>Adam<br />
<div class="outline-text-5" id="text-18-6-3-3">
<p>
可視為 RMSProp 與動量的變種組合<br />
</p>
</div>
</li>
<li><a id="org1401774"></a>AdaDelta<br /></li>
</ol>
</li>
</ol>
</div>

<div id="outline-container-orgde553af" class="outline-3">
<h3 id="orgde553af"><span class="section-number-3">18.7</span> 最佳選擇</h3>
<div class="outline-text-3" id="text-18-7">
<p>
對於大多數深度學習實作者，推動深度學習的最佳途徑並不是創造出更高級的最佳化演算法，相反的，過去幾十年來絕大多數深度學習的突破，都是因為發現了更容易訓練的架構，而不是因為與那些討厭的誤差曲面搏鬥所得到的成果。<br />
</p>
</div>
</div>
</div>

<div id="outline-container-org910315f" class="outline-2">
<h2 id="org910315f"><span class="section-number-2">19</span> TensorFlow</h2>
<div class="outline-text-2" id="text-19">
<p>
<a href="https://www.tensorflow.org/tutorials/generative/adversarial_fgsm">官網學習資源</a><br />
</p>

<p>
是 Google 繼 2011 年開發了 DistBelief 之後的產品，最初由 Google Brain Team 團隊開發，Google 使用 TensorFlow 進行研究及自身產品開發，並於 2015 年 11 月開放原始碼。Google 的 TensorFlow 產品應用包括 GMail 垃圾郵件過濾、Google 語音辨識、Google 圖像辨識、Google 翻譯等等。<br />
</p>

<p>
TensorFlow 可以在 CPU、GPU、TPU(Tensor Processing Unit，由 google 為 AI 研發的專屬晶片)上執行，此外，TensorFlow 也有極佳的跨平台能力，可以在不修改程式碼的前題下，於下列平台上執行訓練，包括：Windows、Linux、iOS、Android、Raspberry Pi。<br />
</p>

<p>
TensorFlow 為較低階的深度學習 API，所以模型必須自行設計，包括張量乘積、卷積等底層操作也要自行撰寫，優點是可以自行規劃模型內容，缺點是開發需要更多時間。所有，有許多開發商以 TensorFlow 為底層開發了許多高階 API，如：Keras、TF-Learn、TF-Slim、TF-Layer。<br />
</p>

<p>
透過使用資料流(flow)圖像，來進行數值演算的新一代開源機器學習工具。這個機器學習工具的基礎設計，主要透過圖學裡的「節點」來表達數學運算，「邊」來表示「節點」間的多維度資料陣列 (tensors，張量)，因此命名做 TensorFlow。<br />
</p>

<p>
TensorFlow 的設計模式核心為「計算圖」(computational graph)，可分為建立計算圖與執行計算圖兩個部份，其程式架構大致如下。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>  <span style="color: #98be65;">'''1. &#24314;&#31435;&#35336;&#31639;&#22294;'''</span>
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">W</span> = tf.Variable(tf.random_normal([<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">2</span>]), name=<span style="color: #98be65;">'W'</span>)
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">b</span> = tf.Variable(tf.random_normal([<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>]), name=<span style="color: #98be65;">'b'</span>)
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">X</span> = tf.placeholder(<span style="color: #98be65;">"float"</span>, [<span style="color: #a9a1e1;">None</span>,<span style="color: #da8548; font-weight: bold;">3</span>], name=<span style="color: #98be65;">'X'</span>)
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">y</span> = tf.nn.sigmoid(tf.matmul(X,W)+b, <span style="color: #98be65;">'y'</span>)
<span class="linenr"> 8: </span>  <span style="color: #83898d;">'''2. &#22519;&#34892;&#35336;&#31639;&#22294;'''</span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">with</span> tf.Session() <span style="color: #51afef;">as</span> <span style="color: #dcaeea;">sess</span>:
<span class="linenr">10: </span>      init= tf.global_variables_initializer()
<span class="linenr">11: </span>      sess.run(init)
<span class="linenr">12: </span>      <span style="color: #dcaeea;">X_array</span> = np.array([[<span style="color: #da8548; font-weight: bold;">0.4</span>, <span style="color: #da8548; font-weight: bold;">0.2</span>, <span style="color: #da8548; font-weight: bold;">0.4</span>],
<span class="linenr">13: </span>                          [<span style="color: #da8548; font-weight: bold;">0.3</span>, <span style="color: #da8548; font-weight: bold;">0.4</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>],
<span class="linenr">14: </span>                          [<span style="color: #da8548; font-weight: bold;">0.3</span>, -<span style="color: #da8548; font-weight: bold;">0.4</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>]])
<span class="linenr">15: </span>      (_b,_W,_X,_y) = sess.run((b,W,X,y), feed_dict={X:X_array})
<span class="linenr">16: </span>
<span class="linenr">17: </span>  <span style="color: #51afef;">print</span>(_b)
<span class="linenr">18: </span>  <span style="color: #51afef;">print</span>(_W)
<span class="linenr">19: </span>  <span style="color: #51afef;">print</span>(_X)
<span class="linenr">20: </span>  <span style="color: #51afef;">print</span>(_y)
</pre>
</div>

<pre class="example">
[[-0.28260672  2.0333097 ]]
[[-1.5098639   0.07314453]
 [ 0.5849383   1.2003893 ]
 [ 1.8941628   0.44426546]]
[[ 0.4  0.2  0.4]
 [ 0.3  0.4  0.5]
 [ 0.3 -0.4  0.5]]
[[0.49702516 0.92275286]
 [0.60956657 0.9403379 ]
 [0.4943853  0.8578114 ]]
</pre>
</div>

<div id="outline-container-org4742bff" class="outline-3">
<h3 id="org4742bff"><span class="section-number-3">19.1</span> 安裝</h3>
<div class="outline-text-3" id="text-19-1">
</div>
<ol class="org-ol">
<li><a id="org5bc6c28"></a>windows 10<br />
<div class="outline-text-4" id="text-19-1-1">
<ol class="org-ol">
<li>download python3<br /></li>
<li>download CUDA 9/10<br /></li>
<li>download and setup cuDNN<br /></li>
</ol>
</div>
</li>
</ol>
</div>

<div id="outline-container-orge9f393c" class="outline-3">
<h3 id="orge9f393c"><span class="section-number-3">19.2</span> Tenser</h3>
<div class="outline-text-3" id="text-19-2">
<p>
一個 TensorFlow 的基礎數值運算需要三個步驟：<br />
</p>
<ol class="org-ol">
<li>宣告張量為常數<br /></li>
</ol>
<p>
TensorFlow 的基本運算單位是張量（Tensor），張量的維度可以是零（零維張量更常見的名稱是純量，Scalar） 、可以是 1（1 維張量更常見的名稱是向量，Vector）、可以是 2（2 維張量更常見的名稱是矩陣，Matrix）亦可以為 n（n 維張量），這個設計與 NumPy 中 ndarray 不謀而合，對於熟悉 NumPy 的 Python 使用者是一大福音。<br />
</p>
<ol class="org-ol">
<li>宣告數值運算的公式<br /></li>
</ol>
<p>
基礎的數值運算（四則運算、次方、求餘數或求商數等）都有 TensorFlow 的函數可以呼叫。<br />
</p>
<ul class="org-ul">
<li>相加 +：tf.add()<br /></li>
<li>相減 -： tf.sub()<br /></li>
<li>相乘 *： tf.multiply()<br /></li>
<li>相除 /： tf.divide()<br /></li>
<li>次方 **： tf.pow()<br /></li>
<li>求餘數 %： tf.mod()<br /></li>
<li>求商數 //：tf.div()<br /></li>
</ul>
<p>
與 Python 數值運算不同的是，這些函數都必須在 TensorFlow 的 Session 中執行才會有運算結果的輸出，否則只是顯示張量物件的資訊而已。<br />
</p>
<ol class="org-ol">
<li>以 Session 執行數值運算<br /></li>
</ol>
<p>
利用 tf.Session() 建立 Session 後再執行數值運算是正規的 TensorFlow 寫法，但對於慣用 Jupyter Notebook 的資料科學家可能會略嫌麻煩，這時可以選擇以 tf.InteractiveSession() 互動模式啟動 Session，一但設定為互動模式後，執行運算變為呼叫張量的 .eval() 方法，並記得在使用完畢之後呼叫 Session 的 .close() 方法關閉互動模式。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org379684a"></a>常用的常數向量建構函數<br />
<div class="outline-text-4" id="text-19-2-1">
<p>
除了使用 tf.constant() 創造常數張量以外，常用的建構函數有：<br />
</p>
<ol class="org-ol">
<li>tf.zeros() ：建構內容數值皆為 0 的常數張量<br /></li>
<li>tf.ones() ：建構內容數值皆為 1 的常數張量<br /></li>
<li>tf.fill() ：建構內容數值皆為特定值的常數張量<br /></li>
<li>tf.range() ：建構內容數值為 (start, limit, delta) 數列的常數張量<br /></li>
<li>tf.random_normal() ：建構內容數值為符合常態分佈數列的常數張量<br /></li>
<li>tf.random_uniform() ：建構內容數值為符合均勻分佈數列的常數張量<br /></li>
</ol>
</div>
</li>
<li><a id="org26f2392"></a>常用的矩陣建構與計算函數<br />
<div class="outline-text-4" id="text-19-2-2">
<p>
TensorFlow 的二維張量與 NumPy 的二維陣列相同為矩陣提供了各種方便使用者呼叫的建構、運算函數，而矩陣亦是 NumPy 與 TensorFlow 應用實務中最常被使用的類型，而常用的矩陣建構與計算函數有：<br />
</p>
<ol class="org-ol">
<li>tf.reshape() ：調整矩陣外觀<br /></li>
<li>tf.eye() ：建構單位矩陣<br /></li>
<li>tf.diag() ：建構對角矩陣<br /></li>
<li>tf.matrix_transpose() ：轉置矩陣<br /></li>
<li>tf.matmul() ：矩陣相乘<br /></li>
</ol>
</div>
</li>
<li><a id="org6dd53a6"></a>變數<br />
<div class="outline-text-4" id="text-19-2-3">
<p>
雖然以常數進行數值運算很方便，但就如同在程式設計中不可能永遠只倚賴值（Values）一般，常見的情況是為了保持彈性，必須將值宣告賦值給變數（Variables）讓使用者能夠動態地進行相同的計算來得到不同的結果，這在 TensorFlow 中是以 tf.Variable() 來完成，就像是在 Python 中簡單宣告變數一般。<br />
</p>

<p>
不過在 TensorFlow 的觀念之中，宣告變數張量並不如 Python 或者先前宣告常數張量那麼單純，它需要兩個步驟：<br />
</p>
<ol class="org-ol">
<li>宣告變數張量的初始值、類型與外觀<br /></li>
<li>初始化變數張量<br /></li>
</ol>
<p>
如果宣告的變數張量沒有經過初始化，我們將會得到<br />
</p>

<p>
該如何初始化變數張量呢？只需將變數張量的 initializer 屬性放入 Session 中執行即可。初始化成功後的變數張量，可以透過 .assign() 方法賦予不同值。<br />
值得注意的地方是對變數張量重新賦値這件事對 TensorFlow 來說也是一個運算，必須在宣告之後放入 Session 中執行，否則重新賦值並不會有作用。變數張量一但被宣告之後，重新賦值時必須要注意類型，賦予不同類型的值會得到 TypeError。不僅是值的類型，外觀也必須跟當初所宣告的相同，賦予不同外觀的值會得到 ValueError。<br />
</p>
</div>
</li>
<li><a id="org4b258a6"></a>Placeholder<br />
<div class="outline-text-4" id="text-19-2-4">
<p>
第三種在 TensorFlow 中張量將被宣告的類型稱為 Placeholder，這是一種常見將資料輸入 TensorFlow 計算圖形（Graph）的方法，我們可以將它對照為像是在佔有一個長度卻沒有初始值的 Python None、或者是 NumPy np.NaN ，差異在於 None 或 np.NaN 不需要將之後想要擺放的資料類型預先定義，但是 Placeholder 張量和變數張量一樣，必須預先定義好之後欲輸入的資料類型與外觀。使用 tf.placeholder() 可以建出 Placeholder 張量，未來利用 TensorFlow 訓練機器學習與深度學習的模型時，將會使用 Placeholder 將 X 與 y 的資料輸入計算圖形。<br />
那麼宣告完 Placeholder 張量以後，又該如何將資料輸入？TensorFlow 的術語稱作是 Feed dictionaries，意即將資料以 Python dict 餵進（Feed）Placeholder 張量之中，而 TensorFlow 術語則將完成計算後的輸出資料稱作是 Fetch，是 ndarray 的類型。<br />
Placeholder 張量具備隱性型別轉換的功能，假如我們用浮點數餵入已經被宣告為 tf.int32 的 Placeholder 張量中，將會被自動轉換為整數。假如餵入資料外觀與 Placeholder 張量所定義的不同，則會產生錯誤。<br />
</p>
</div>
</li>
<li><a id="org724a1a3"></a>placeholder 與 variable 的差異<br />
<div class="outline-text-4" id="text-19-2-5">
<p>
The difference is that with tf.Variable you have to provide an initial value when you declare it. With tf.placeholder you don&rsquo;t have to provide an initial value and you can specify it at run time with the feed_dict argument inside Session.run<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org6f13505" class="outline-3">
<h3 id="org6f13505"><span class="section-number-3">19.3</span> TenserBoard</h3>
<div class="outline-text-3" id="text-19-3">
<p>
TensorFlow 的開發團隊提供了 TensorBoard 應用程式可以在本機端啟動網頁伺服器，並在其中顯示 TensorFlow 程式的相關視覺化，若希望能夠在 TensorBoard 中檢視，必須手動在 TensorFlow 程式中加入 tf.summary.FileWriter() 來指定 TensorBoard 儲存日誌的檔案路徑，例如寫作一個簡單的張量相加運算，並將這個運算記錄在 graphs/tf_add 資料夾中。<br />
</p>

<p>
接著回到終端機，在啟動 Jupyter Notebook 的路徑下執行指令啟動 TensorBoard 服務，並指定日誌儲存的檔案路徑 &#x2013;logdir=graphs/tf_add 以及 TensorBoard 網頁伺服器的啟動位址（本機位址 localhost） &#x2013;host=127.0.0.1 。<br />
</p>

<p>
最後打開瀏覽器在網址列輸入 127.0.0.1:6006 就可以瀏覽 TensorBoard 的服務內容，點選 GRAPHS 標籤就可以觀察一個張量相加運算在 TensorBoard 的視覺化。<br />
</p>

<p>
檢視完畢之後可以回到終端機畫面以 CTRL-C 終止 TensorBoard 服務。<br />
</p>
</div>
</div>

<div id="outline-container-org4f8769c" class="outline-3">
<h3 id="org4f8769c"><span class="section-number-3">19.4</span> 情緒分析 sentiment analysis</h3>
<div class="outline-text-3" id="text-19-4">
<p>
<a href="https://github.com/MyDearGreatTeacher/TensorSecurity/blob/master/code/TF/RNN/%E7%AF%84%E4%BE%8B%E7%A8%8B%E5%BC%8F_%E6%83%85%E7%B7%92%E5%88%86%E6%9E%90sentiment%20analysis.md">https://github.com/MyDearGreatTeacher/TensorSecurity/blob/master/code/TF/RNN/%E7%AF%84%E4%BE%8B%E7%A8%8B%E5%BC%8F_%E6%83%85%E7%B7%92%E5%88%86%E6%9E%90sentiment%20analysis.md</a><br />
</p>
</div>
</div>

<div id="outline-container-orgdc2cf21" class="outline-3">
<h3 id="orgdc2cf21"><span class="section-number-3">19.5</span> IMDb-Movie-Review</h3>
</div>


<div id="outline-container-orgd509e85" class="outline-3">
<h3 id="orgd509e85"><span class="section-number-3">19.6</span> 使用 XLNet(2019)</h3>
<div class="outline-text-3" id="text-19-6">
</div>
</div>
</div>

<div id="outline-container-org23241ef" class="outline-2">
<h2 id="org23241ef"><span class="section-number-2">20</span> PyTorch</h2>
<div class="outline-text-2" id="text-20">
</div>
<div id="outline-container-orga095b40" class="outline-3">
<h3 id="orga095b40"><span class="section-number-3">20.1</span> 簡介</h3>
<div class="outline-text-3" id="text-20-1">
<p>
PyTorch 為 Facebook 在 2017 年初開源的深度學習框架，其建立在 Torch 之上，且標榜 Python First ，為量身替 Python 語言所打造，使用起來就跟寫一般 Python 專案沒兩樣，也能和其他 Python 套件無痛整合。PyTorch 的優勢在於其概念相當直觀且語法簡潔優雅，因此視為新手入門的一個好選項；再來其輕量架構讓模型得以快速訓練且有效運用資源<sup><a id="fnr.20" class="footref" href="#fn.20">20</a></sup>。<br />
</p>
</div>
</div>

<div id="outline-container-org30b79b3" class="outline-3">
<h3 id="org30b79b3"><span class="section-number-3">20.2</span> PyTorch 基本架構</h3>
<div class="outline-text-3" id="text-20-2">
</div>
<ol class="org-ol">
<li><a id="org10bffe1"></a>先確定 pytorch 的版本<br />
<div class="outline-text-4" id="text-20-2-1">
<p>
1.2 版於 2019-08release<br />
</p>
<div class="org-src-container">
<pre class="src src-sh"><span class="linenr">1: </span>pip3 list | <span style="color: #ECBE7B;">grep</span> torch
</pre>
</div>

<pre class="example">
torch                1.2.0   
</pre>
</div>
</li>

<li><a id="orgd0e04cc"></a>可用 model<br />
<div class="outline-text-4" id="text-20-2-2">
<ul class="org-ul">
<li>永遠使用最新發展的套件，效能永遠最好，比賽成績永遠最好 :D，<br /></li>
<li>練習：一個主題用三個 model 跑一次，調參數<br /></li>
<li>TorchVision<br /></li>
<li>Torchtext<br /></li>
<li>TorchAudio<br /></li>
<li>2018 BIRT<br /></li>
<li>2918 XLNET<br /></li>
</ul>
</div>
</li>

<li><a id="org90756df"></a>torchsummary<br />
<div class="outline-text-4" id="text-20-2-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span>  <span style="color: #51afef;">from</span> torchsummary <span style="color: #51afef;">import</span> summary
<span class="linenr">3: </span>  <span style="color: #51afef;">from</span> torchvision.models <span style="color: #51afef;">import</span> vgg11 <span style="color: #5B6268;"># </span><span style="color: #5B6268;">vgg16,19 &#29992;GPU&#36305;&#21487;&#33021;&#35201;&#36305;2,3&#36913;</span>
<span class="linenr">4: </span>
<span class="linenr">5: </span>  <span style="color: #dcaeea;">model</span> = vgg11(pretrained=<span style="color: #a9a1e1;">False</span>)
<span class="linenr">6: </span>  <span style="color: #51afef;">if</span> torch.cuda.is_available():  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22914;&#26524;&#21487;&#20197;&#23601;&#20351;&#29992;GPU&#35336;&#31639;</span>
<span class="linenr">7: </span>      model.cuda()
<span class="linenr">8: </span>  summary(model, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">224</span>, <span style="color: #da8548; font-weight: bold;">224</span>))  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">RGB&#19977;&#33394;&#65292;224*224</span>
</pre>
</div>

<pre class="example">
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 224, 224]           1,792
              ReLU-2         [-1, 64, 224, 224]               0
         MaxPool2d-3         [-1, 64, 112, 112]               0
            Conv2d-4        [-1, 128, 112, 112]          73,856
              ReLU-5        [-1, 128, 112, 112]               0
         MaxPool2d-6          [-1, 128, 56, 56]               0
            Conv2d-7          [-1, 256, 56, 56]         295,168
              ReLU-8          [-1, 256, 56, 56]               0
            Conv2d-9          [-1, 256, 56, 56]         590,080
             ReLU-10          [-1, 256, 56, 56]               0
        MaxPool2d-11          [-1, 256, 28, 28]               0
           Conv2d-12          [-1, 512, 28, 28]       1,180,160
             ReLU-13          [-1, 512, 28, 28]               0
           Conv2d-14          [-1, 512, 28, 28]       2,359,808
             ReLU-15          [-1, 512, 28, 28]               0
        MaxPool2d-16          [-1, 512, 14, 14]               0
           Conv2d-17          [-1, 512, 14, 14]       2,359,808
             ReLU-18          [-1, 512, 14, 14]               0
           Conv2d-19          [-1, 512, 14, 14]       2,359,808
             ReLU-20          [-1, 512, 14, 14]               0
        MaxPool2d-21            [-1, 512, 7, 7]               0
AdaptiveAvgPool2d-22            [-1, 512, 7, 7]               0
           Linear-23                 [-1, 4096]     102,764,544
             ReLU-24                 [-1, 4096]               0
          Dropout-25                 [-1, 4096]               0
           Linear-26                 [-1, 4096]      16,781,312
             ReLU-27                 [-1, 4096]               0
          Dropout-28                 [-1, 4096]               0
           Linear-29                 [-1, 1000]       4,097,000
================================================================
Total params: 132,863,336
Trainable params: 132,863,336
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 125.37
Params size (MB): 506.83
Estimated Total Size (MB): 632.78
----------------------------------------------------------------
</pre>
</div>
</li>

<li><a id="org8dd69f1"></a>核心套件<br />
<div class="outline-text-4" id="text-20-2-4">
<ul class="org-ul">
<li>torch<br /></li>
<li>torch.Tensor<br /></li>
<li>Tensor Attributes<br /></li>
<li>Type Info<br /></li>
<li>torch.sparse<br /></li>
<li>torch.cuda<br /></li>
<li>torch.Storage<br /></li>
<li>torch.nn (*)<br /></li>
<li>torch.nn.functional<br /></li>
<li>torch.nn.init<br /></li>
<li>torch.optim<br /></li>
<li>torch.autograd<br /></li>
<li>torch.distributed<br /></li>
<li>torch.distributions<br /></li>
<li>torch.hub<br /></li>
<li>torch.jit<br /></li>
<li>torch.multiprocessing<br /></li>
<li>torch.random<br /></li>
<li>torch.utils.bottleneck<br /></li>
<li>torch.utils.checkpoint<br /></li>
<li>torch.utils.cpp_extension<br /></li>
<li>torch.utils.data<br /></li>
<li>torch.utils.dlpack<br /></li>
<li>torch.utils.model_zoo<br /></li>
<li>torch.utils.tensorboard<br /></li>
</ul>
</div>
</li>

<li><a id="orgb9d3178"></a>torch.nn.functional 功能模組<br />
<div class="outline-text-4" id="text-20-2-5">
<ol class="org-ol">
<li>Convolution functions (*) 卷積函數<br /></li>
<li>Pooling functions  池化函數<br /></li>
<li>Non-linear activation functions 激活函數<br /></li>
<li>Normalization functions<br /></li>
<li>Linear functions<br /></li>
<li>Dropout functions<br /></li>
<li>Sparse functions<br /></li>
<li>Distance functions<br /></li>
<li>Loss functions<br /></li>
<li>Vision functions<br /></li>
<li>DataParallel functions (multi-GPU, distributed)<br /></li>
</ol>
</div>
</li>

<li><a id="orgd474a07"></a>torch.nn.functional.conv1d<br />
<ol class="org-ol">
<li><a id="org1d2a283"></a>torch.nn.functional.conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor<br />
<ol class="org-ol">
<li><a id="org1e9fac9"></a>Parameters<br />
<div class="outline-text-6" id="text-20-2-6-1-1">
<ul class="org-ul">
<li>input – input tensor of shape (minibatch,in_channels,iW)(\text{minibatch} , \text{in\_channels} , iW)(minibatch,in_channels,iW)<br /></li>
<li>weight – filters of shape (out_channels,in_channelsgroups,kW)(\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kW)(out_channels,groupsin_channels​,kW)<br /></li>
<li>bias – optional bias of shape (out_channels)(\text{out\_channels})(out_channels) . Default: None<br /></li>
<li>stride – the stride of the convolving kernel. Can be a single number or a one-element tuple (sW,). Default: 1<br /></li>
<li>padding – implicit paddings on both sides of the input. Can be a single number or a one-element tuple (padW,). Default: 0<br /></li>
<li>dilation – the spacing between kernel elements. Can be a single number or a one-element tuple (dW,). Default: 1<br /></li>
<li>groups – split input into groups, in_channels\text{in\_channels}in_channels should be divisible by the number of groups. Default: 1<br /></li>
</ul>
</div>
</li>

<li><a id="org48ef6a2"></a>Examples:<br />
<div class="outline-text-6" id="text-20-2-6-1-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #51afef;">from</span> torch.autograd <span style="color: #51afef;">import</span> Variable
<span class="linenr">3: </span><span style="color: #51afef;">from</span> torch <span style="color: #51afef;">import</span> nn
<span class="linenr">4: </span><span style="color: #51afef;">import</span> torch.nn.functional <span style="color: #51afef;">as</span> F
<span class="linenr">5: </span><span style="color: #dcaeea;">filters</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">33</span>, <span style="color: #da8548; font-weight: bold;">16</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">6: </span><span style="color: #dcaeea;">inputs</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">20</span>, <span style="color: #da8548; font-weight: bold;">16</span>, <span style="color: #da8548; font-weight: bold;">50</span>)
<span class="linenr">7: </span>F.conv1d(inputs, filters)
<span class="linenr">8: </span><span style="color: #51afef;">print</span>(F)
</pre>
</div>

<pre class="example">
&lt;module 'torch.nn.functional' from '/usr/local/lib/python3.7/site-packages/torch/nn/functional.py'&gt;
</pre>
</div>
</li>
</ol>
</li>
</ol>
</li>

<li><a id="orgf46981e"></a>torch.optim 優化模組<br />
<ol class="org-ol">
<li><a id="orgbe87a4a"></a>To use torch.optim you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients.<br /></li>
<li><a id="org09f01cc"></a>To construct an Optimizer you have to give it an iterable containing the parameters (all should be Variable s) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.<br /></li>
<li><a id="orgdebb931"></a>語法<br />
<div class="outline-text-5" id="text-20-2-7-3">
<ul class="org-ul">
<li>#1<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #51afef;">for</span> <span style="color: #c678dd;">input</span>, target <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">dataset</span>:
<span class="linenr">3: </span>    optimizer.zero_grad()
<span class="linenr">4: </span>    output = model(<span style="color: #c678dd;">input</span>)
<span class="linenr">5: </span>    <span style="color: #dcaeea;">loss</span> = loss_fn(output, target)
<span class="linenr">6: </span>    loss.backward()
<span class="linenr">7: </span>    optimizer.step()
</pre>
</div>

<ul class="org-ul">
<li>#2<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #51afef;">for</span> <span style="color: #c678dd;">input</span>, target <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">dataset</span>:
<span class="linenr">3: </span>    <span style="color: #51afef;">def</span> <span style="color: #c678dd;">closure</span>():
<span class="linenr">4: </span>        optimizer.zero_grad()
<span class="linenr">5: </span>        output = model(<span style="color: #c678dd;">input</span>)
<span class="linenr">6: </span>        <span style="color: #dcaeea;">loss</span> = loss_fn(output, target)
<span class="linenr">7: </span>        loss.backward()
<span class="linenr">8: </span>        <span style="color: #51afef;">return</span> loss
<span class="linenr">9: </span>    optimizer.step(closure)
</pre>
</div>
</div>
</li>
<li><a id="orgb796f1e"></a>source code<br />
<div class="outline-text-5" id="text-20-2-7-4">
<p>
github: <a href="https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py">https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py</a><br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">  2: </span><span style="color: #51afef;">from</span> .optimizer <span style="color: #51afef;">import</span> Optimizer, required
<span class="linenr">  3: </span>
<span class="linenr">  4: </span><span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">SGD</span>(Optimizer):
<span class="linenr">  5: </span>    <span style="color: #83898d;">"""Implements stochastic gradient descent (optionally with momentum).</span>
<span class="linenr">  6: </span>
<span class="linenr">  7: </span><span style="color: #83898d;">    Nesterov momentum is based on the formula from</span>
<span class="linenr">  8: </span><span style="color: #83898d;">    `On the importance of initialization and momentum in deep learning`__.</span>
<span class="linenr">  9: </span>
<span class="linenr"> 10: </span><span style="color: #83898d;">    Args:</span>
<span class="linenr"> 11: </span><span style="color: #83898d;">        params (iterable): iterable of parameters to optimize or dicts defining</span>
<span class="linenr"> 12: </span><span style="color: #83898d;">            parameter groups</span>
<span class="linenr"> 13: </span><span style="color: #83898d;">        lr (float): learning rate</span>
<span class="linenr"> 14: </span><span style="color: #83898d;">        momentum (float, optional): momentum factor (default: 0)</span>
<span class="linenr"> 15: </span><span style="color: #83898d;">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span>
<span class="linenr"> 16: </span><span style="color: #83898d;">        dampening (float, optional): dampening for momentum (default: 0)</span>
<span class="linenr"> 17: </span><span style="color: #83898d;">        nesterov (bool, optional): enables Nesterov momentum (default: False)</span>
<span class="linenr"> 18: </span>
<span class="linenr"> 19: </span><span style="color: #83898d;">    Example:</span>
<span class="linenr"> 20: </span><span style="color: #83898d;">        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span>
<span class="linenr"> 21: </span><span style="color: #83898d;">        &gt;&gt;&gt; optimizer.zero_grad()</span>
<span class="linenr"> 22: </span><span style="color: #83898d;">        &gt;&gt;&gt; loss_fn(model(input), target).backward()</span>
<span class="linenr"> 23: </span><span style="color: #83898d;">        &gt;&gt;&gt; optimizer.step()</span>
<span class="linenr"> 24: </span>
<span class="linenr"> 25: </span><span style="color: #83898d;">    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf</span>
<span class="linenr"> 26: </span>
<span class="linenr"> 27: </span><span style="color: #83898d;">    .. note::</span>
<span class="linenr"> 28: </span><span style="color: #83898d;">        The implementation of SGD with Momentum/Nesterov subtly differs from</span>
<span class="linenr"> 29: </span><span style="color: #83898d;">        Sutskever et. al. and implementations in some other frameworks.</span>
<span class="linenr"> 30: </span>
<span class="linenr"> 31: </span><span style="color: #83898d;">        Considering the specific case of Momentum, the update can be written as</span>
<span class="linenr"> 32: </span>
<span class="linenr"> 33: </span><span style="color: #83898d;">        .. math::</span>
<span class="linenr"> 34: </span><span style="color: #83898d;">                  v_{t+1} = \mu * v_{t} + g_{t+1} \\</span>
<span class="linenr"> 35: </span><span style="color: #83898d;">                  p_{t+1} = p_{t} - lr * v_{t+1}</span>
<span class="linenr"> 36: </span>
<span class="linenr"> 37: </span><span style="color: #83898d;">        where p, g, v and :math:`\mu` denote the parameters, gradient,</span>
<span class="linenr"> 38: </span><span style="color: #83898d;">        velocity, and momentum respectively.</span>
<span class="linenr"> 39: </span>
<span class="linenr"> 40: </span><span style="color: #83898d;">        This is in contrast to Sutskever et. al. and</span>
<span class="linenr"> 41: </span><span style="color: #83898d;">        other frameworks which employ an update of the form</span>
<span class="linenr"> 42: </span>
<span class="linenr"> 43: </span><span style="color: #83898d;">        .. math::</span>
<span class="linenr"> 44: </span><span style="color: #83898d;">             v_{t+1} = \mu * v_{t} + lr * g_{t+1} \\</span>
<span class="linenr"> 45: </span><span style="color: #83898d;">             p_{t+1} = p_{t} - v_{t+1}</span>
<span class="linenr"> 46: </span>
<span class="linenr"> 47: </span><span style="color: #83898d;">        The Nesterov version is analogously modified.</span>
<span class="linenr"> 48: </span><span style="color: #83898d;">    """</span>
<span class="linenr"> 49: </span>
<span class="linenr"> 50: </span>    <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>, params, lr=required, momentum=<span style="color: #da8548; font-weight: bold;">0</span>, dampening=<span style="color: #da8548; font-weight: bold;">0</span>,
<span class="linenr"> 51: </span>                 weight_decay=<span style="color: #da8548; font-weight: bold;">0</span>, nesterov=<span style="color: #a9a1e1;">False</span>):
<span class="linenr"> 52: </span>        <span style="color: #51afef;">if</span> lr <span style="color: #51afef;">is</span> <span style="color: #51afef;">not</span> required <span style="color: #51afef;">and</span> lr &lt; <span style="color: #da8548; font-weight: bold;">0.0</span>:
<span class="linenr"> 53: </span>            <span style="color: #51afef;">raise</span> <span style="color: #ECBE7B;">ValueError</span>(<span style="color: #98be65;">"Invalid learning rate: {}"</span>.<span style="color: #c678dd;">format</span>(lr))
<span class="linenr"> 54: </span>        <span style="color: #51afef;">if</span> momentum &lt; <span style="color: #da8548; font-weight: bold;">0.0</span>:
<span class="linenr"> 55: </span>            <span style="color: #51afef;">raise</span> <span style="color: #ECBE7B;">ValueError</span>(<span style="color: #98be65;">"Invalid momentum value: {}"</span>.<span style="color: #c678dd;">format</span>(momentum))
<span class="linenr"> 56: </span>        <span style="color: #51afef;">if</span> weight_decay &lt; <span style="color: #da8548; font-weight: bold;">0.0</span>:
<span class="linenr"> 57: </span>            <span style="color: #51afef;">raise</span> <span style="color: #ECBE7B;">ValueError</span>(<span style="color: #98be65;">"Invalid weight_decay value: {}"</span>.<span style="color: #c678dd;">format</span>(weight_decay))
<span class="linenr"> 58: </span>
<span class="linenr"> 59: </span>        defaults = <span style="color: #c678dd;">dict</span>(lr=lr, momentum=momentum, dampening=dampening,
<span class="linenr"> 60: </span>                        weight_decay=weight_decay, nesterov=nesterov)
<span class="linenr"> 61: </span>        <span style="color: #51afef;">if</span> nesterov <span style="color: #51afef;">and</span> (momentum &lt;= <span style="color: #da8548; font-weight: bold;">0</span> <span style="color: #51afef;">or</span> dampening != <span style="color: #da8548; font-weight: bold;">0</span>):
<span class="linenr"> 62: </span>            <span style="color: #51afef;">raise</span> <span style="color: #ECBE7B;">ValueError</span>(<span style="color: #98be65;">"Nesterov momentum requires a momentum and zero dampening"</span>)
<span class="linenr"> 63: </span>        <span style="color: #c678dd;">super</span>(SGD, <span style="color: #51afef;">self</span>).__init__(params, defaults)
<span class="linenr"> 64: </span>
<span class="linenr"> 65: </span>    <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__setstate__</span>(<span style="color: #51afef;">self</span>, state):
<span class="linenr"> 66: </span>        <span style="color: #c678dd;">super</span>(SGD, <span style="color: #51afef;">self</span>).__setstate__(state)
<span class="linenr"> 67: </span>        <span style="color: #51afef;">for</span> group <span style="color: #51afef;">in</span> <span style="color: #51afef;">self</span>.param_groups:
<span class="linenr"> 68: </span>            group.setdefault(<span style="color: #98be65;">'nesterov'</span>, <span style="color: #a9a1e1;">False</span>)
<span class="linenr"> 69: </span>
<span class="linenr"> 70: </span>    <span style="color: #51afef;">def</span> <span style="color: #c678dd;">step</span>(<span style="color: #51afef;">self</span>, closure=<span style="color: #a9a1e1;">None</span>):
<span class="linenr"> 71: </span>        <span style="color: #83898d;">"""Performs a single optimization step.</span>
<span class="linenr"> 72: </span>
<span class="linenr"> 73: </span><span style="color: #83898d;">        Arguments:</span>
<span class="linenr"> 74: </span><span style="color: #83898d;">            closure (callable, optional): A closure that reevaluates the model</span>
<span class="linenr"> 75: </span><span style="color: #83898d;">                and returns the loss.</span>
<span class="linenr"> 76: </span><span style="color: #83898d;">        """</span>
<span class="linenr"> 77: </span>        loss = <span style="color: #a9a1e1;">None</span>
<span class="linenr"> 78: </span>        <span style="color: #51afef;">if</span> closure <span style="color: #51afef;">is</span> <span style="color: #51afef;">not</span> <span style="color: #a9a1e1;">None</span>:
<span class="linenr"> 79: </span>            loss = closure()
<span class="linenr"> 80: </span>
<span class="linenr"> 81: </span>        <span style="color: #51afef;">for</span> group <span style="color: #51afef;">in</span> <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">param_groups</span>:
<span class="linenr"> 82: </span>            weight_decay = group[<span style="color: #98be65;">'weight_decay'</span>]
<span class="linenr"> 83: </span>            <span style="color: #dcaeea;">momentum</span> = group[<span style="color: #98be65;">'momentum'</span>]
<span class="linenr"> 84: </span>            <span style="color: #dcaeea;">dampening</span> = group[<span style="color: #98be65;">'dampening'</span>]
<span class="linenr"> 85: </span>            <span style="color: #dcaeea;">nesterov</span> = group[<span style="color: #98be65;">'nesterov'</span>]
<span class="linenr"> 86: </span>
<span class="linenr"> 87: </span>            <span style="color: #51afef;">for</span> p <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">group</span>[<span style="color: #98be65;">'params'</span>]:
<span class="linenr"> 88: </span>                <span style="color: #51afef;">if</span> p.grad <span style="color: #51afef;">is</span> <span style="color: #a9a1e1;">None</span>:
<span class="linenr"> 89: </span>                    <span style="color: #51afef;">continue</span>
<span class="linenr"> 90: </span>                d_p = p.grad.data
<span class="linenr"> 91: </span>                <span style="color: #51afef;">if</span> weight_decay != <span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr"> 92: </span>                    d_p.add_(weight_decay, p.data)
<span class="linenr"> 93: </span>                <span style="color: #51afef;">if</span> momentum != <span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr"> 94: </span>                    <span style="color: #dcaeea;">param_state</span> = <span style="color: #51afef;">self</span>.state[p]
<span class="linenr"> 95: </span>                    <span style="color: #51afef;">if</span> <span style="color: #98be65;">'momentum_buffer'</span> <span style="color: #51afef;">not</span> <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">param_state</span>:
<span class="linenr"> 96: </span>                        buf = <span style="color: #dcaeea;">param_state</span>[<span style="color: #98be65;">'momentum_buffer'</span>] = torch.clone(d_p).detach()
<span class="linenr"> 97: </span>                    <span style="color: #51afef;">else</span>:
<span class="linenr"> 98: </span>                        buf = param_state[<span style="color: #98be65;">'momentum_buffer'</span>]
<span class="linenr"> 99: </span>                        buf.mul_(momentum).add_(<span style="color: #da8548; font-weight: bold;">1</span> - dampening, d_p)
<span class="linenr">100: </span>                    <span style="color: #51afef;">if</span> <span style="color: #dcaeea;">nesterov</span>:
<span class="linenr">101: </span>                        d_p = d_p.add(momentum, buf)
<span class="linenr">102: </span>                    <span style="color: #51afef;">else</span>:
<span class="linenr">103: </span>                        d_p = buf
<span class="linenr">104: </span>
<span class="linenr">105: </span>                p.data.add_(-group[<span style="color: #98be65;">'lr'</span>], d_p)
<span class="linenr">106: </span>
<span class="linenr">107: </span>        <span style="color: #51afef;">return</span> loss
</pre>
</div>
</div>
</li>
</ol>
</li>
</ol>
</div>

<div id="outline-container-org51fac67" class="outline-3">
<h3 id="org51fac67"><span class="section-number-3">20.3</span> PyTorch 基礎運算</h3>
<div class="outline-text-3" id="text-20-3">
</div>
<ol class="org-ol">
<li><a id="org2097f17"></a>創造矩陣<br />
<div class="outline-text-4" id="text-20-3-1">
<p>
註:<sup><a id="fnr.21" class="footref" href="#fn.21">21</a></sup><br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #51afef;">print</span>(torch.ones(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>))    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21109;&#36896;&#19968;&#20491;&#22635;&#28415;1&#30340;&#30697;&#38499;</span>
<span class="linenr">3: </span>
<span class="linenr">4: </span>torch.zeros(<span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">3</span>)          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21109;&#36896;&#19968;&#20491;&#22635;&#28415;0&#30340;&#30697;&#38499;</span>
<span class="linenr">5: </span>
<span class="linenr">6: </span><span style="color: #51afef;">print</span>(torch.eye(<span style="color: #da8548; font-weight: bold;">3</span>))        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21109;&#36896;&#19968;&#20491;4x4&#30340;&#21934;&#20301;&#30697;&#38499;</span>
<span class="linenr">7: </span>
<span class="linenr">8: </span><span style="color: #51afef;">print</span>(torch.rand(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>) )   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21109;&#36896;&#19968;&#20491;&#20803;&#32032;&#22312;[0,1)&#20013;&#38568;&#27231;&#20998;&#20296;&#30340;&#30697;&#38499;</span>
<span class="linenr">9: </span><span style="color: #51afef;">print</span>(torch.randn(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>))   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21109;&#36896;&#19968;&#20491;&#20803;&#32032;&#24478;&#24120;&#24907;&#20998;&#20296;(0, 1)&#38568;&#27231;&#21462;&#20540;&#30340;&#30697;&#38499;</span>
</pre>
</div>

<pre class="example">
tensor([[1., 1., 1.],
        [1., 1., 1.]])
tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]])
tensor([[0.0469, 0.1002, 0.3170],
        [0.6663, 0.0566, 0.3846]])
tensor([[-1.0977, -0.0200, -1.2239],
        [-0.4332, -0.6190,  0.7148]])
</pre>
</div>
</li>

<li><a id="org6ee5860"></a>矩陣操作<br />
<div class="outline-text-4" id="text-20-3-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> torch
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> torch.autograd <span style="color: #51afef;">import</span> Variable
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> torch.optim <span style="color: #51afef;">import</span> SGD
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">m1</span> = torch.ones(<span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">m2</span> = torch.zeros(<span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #51afef;">print</span>(torch.cat((m1, m2), <span style="color: #da8548; font-weight: bold;">1</span>)  )  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;m1&#21644;m2&#20841;&#20491;&#30697;&#38499;&#22312;&#31532;&#19968;&#20491;&#32173;&#24230;&#21512;&#20341;&#36215;&#20358;</span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">print</span>(torch.stack((m1, m2), <span style="color: #da8548; font-weight: bold;">1</span>))  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;m1&#21644;m2&#20841;&#20491;&#30697;&#38499;&#22312;&#26032;&#30340;&#32173;&#24230;&#65288;&#31532;&#19968;&#32173;&#65289;&#30090;&#36215;&#20358;</span>
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #51afef;">print</span>(m1 + m2)                   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30697;&#38499;element-wise&#30456;&#21152;&#65292;&#20854;&#20182;&#22522;&#26412;&#36939;&#31639;&#26159;&#19968;&#27171;&#30340;</span>
<span class="linenr">12: </span>
</pre>
</div>

<pre class="example">
tensor([[1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0.]])
tensor([[[1., 1., 1.],
         [0., 0., 0.]],

        [[1., 1., 1.],
         [0., 0., 0.]],

        [[1., 1., 1.],
         [0., 0., 0.]],

        [[1., 1., 1.],
         [0., 0., 0.]],

        [[1., 1., 1.],
         [0., 0., 0.]]])
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
</pre>
</div>
</li>

<li><a id="org9ee633b"></a>常用方法<sup><a id="fnr.22" class="footref" href="#fn.22">22</a></sup><br />
<ol class="org-ol">
<li><a id="orgf819bd6"></a>torch.rand(*sizes, out=None) → Tensor<br />
<div class="outline-text-5" id="text-20-3-3-1">
<p>
返回一個張量，包含了從區間[0, 1)的均勻分佈中抽取的一組隨機數。張量的形狀由參數 sizes 定義。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="orgcc375ec"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-1-1">
<ul class="org-ul">
<li>sizes (int&#x2026;) - 整數序列，定義了輸出張量的形狀<br /></li>
<li>out (Tensor, optional) - 結果張量<br /></li>
</ul>
</div>
</li>
<li><a id="org18f697b"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-1-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span>  <span style="color: #51afef;">print</span>(torch.rand(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
</pre>
</div>

<pre class="example">
tensor([[0.8113, 0.0879, 0.8150],
        [0.6705, 0.5357, 0.4015]])
</pre>
</div>
</li>
</ol>
</li>

<li><a id="orgd4ccb44"></a>torch.randn(*sizes, out=None) → Tensor<br />
<div class="outline-text-5" id="text-20-3-3-2">
<p>
返回一個張量，包含了從標準正態分佈（均值爲 0，方差爲 1，即高斯白噪聲）中抽取的一組隨機數。張量的形狀由參數 sizes 定義。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org7a26974"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-2-1">
<ul class="org-ul">
<li>sizes (int&#x2026;) - 整數序列，定義了輸出張量的形狀<br /></li>
<li>out (Tensor, optional) - 結果張量<br /></li>
</ul>
</div>
</li>
<li><a id="orgb88e6b1"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-2-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #51afef;">print</span>(torch.randn(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">3</span>))
</pre>
</div>

<pre class="example">
tensor([[ 0.3587, -0.8452,  0.1752],
        [ 0.3541, -2.2761, -1.1221]])
</pre>
</div>
</li>
</ol>
</li>

<li><a id="org210044c"></a>torch.normal(means, std, out=None) → → Tensor<br />
<div class="outline-text-5" id="text-20-3-3-3">
<p>
返回一個張量，包含了從指定均值 means 和標準差 std 的離散正態分佈中抽取的一組隨機數。<br />
標準差 std 是一個張量，包含每個輸出元素相關的正態分佈標準差。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="orgcdfdf60"></a>參數:<br />
<div class="outline-text-6" id="text-20-3-3-3-1">
<ul class="org-ul">
<li>means (float, optional) - 均值<br /></li>
<li>std (Tensor) - 標準差<br /></li>
<li>out (Tensor) - 輸出張量<br /></li>
</ul>
</div>
</li>
<li><a id="orgcad43e9"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-3-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #51afef;">print</span>(torch.normal(mean=<span style="color: #da8548; font-weight: bold;">0.5</span>, std=torch.arange(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">6</span>)))
</pre>
</div>
</div>
</li>
</ol>
</li>

<li><a id="org93f721f"></a>torch.linspace(start, end, steps=100, out=None) → Tensor<br />
<div class="outline-text-5" id="text-20-3-3-4">
<p>
返回一個 1 維張量，包含在區間 start 和 end 上均勻間隔的 step 個點。<br />
輸出張量的長度由 steps 決定。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org3b44472"></a>參數:<br />
<div class="outline-text-6" id="text-20-3-3-4-1">
<ul class="org-ul">
<li>start (float) - 區間的起始點<br /></li>
<li>end (float) - 區間的終點<br /></li>
<li>steps (int) - 在 start 和 end 間生成的樣本數<br /></li>
<li>out (Tensor, optional) - 結果張量<br /></li>
</ul>
</div>
</li>
<li><a id="org86b465d"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-4-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #51afef;">print</span>(torch.linspace(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">10</span>, steps=<span style="color: #da8548; font-weight: bold;">7</span>))
</pre>
</div>

<pre class="example">
tensor([ 3.0000,  4.1667,  5.3333,  6.5000,  7.6667,  8.8333, 10.0000])
</pre>
</div>
</li>
</ol>
</li>

<li><a id="org0da13fe"></a>torch.sum(input) → float<br />
<div class="outline-text-5" id="text-20-3-3-5">
<p>
返回输入向量 input 中所有元素的和。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org510a0c6"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-5-1">
<ul class="org-ul">
<li>input (Tensor) - 输入张量<br /></li>
</ul>
</div>
</li>
<li><a id="orgccfb29e"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-5-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #dcaeea;">a</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">3: </span><span style="color: #51afef;">print</span>(a)
<span class="linenr">4: </span><span style="color: #51afef;">print</span>(torch.<span style="color: #c678dd;">sum</span>(a))
<span class="linenr">5: </span><span style="color: #dcaeea;">b</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">6</span>)
<span class="linenr">6: </span><span style="color: #51afef;">print</span>(b)
<span class="linenr">7: </span><span style="color: #51afef;">print</span>(torch.<span style="color: #c678dd;">sum</span>(b))
</pre>
</div>

<pre class="example">
tensor([[-0.7402,  0.5678, -0.4867]])
tensor(-0.6591)
tensor([[-0.4197, -0.9463, -0.0399,  0.3360,  0.7250,  1.7494],
        [-0.6536,  0.1619, -1.8099,  0.8848, -0.4127, -0.3032]])
tensor(-0.7282)
</pre>
</div>
</li>
</ol>
</li>

<li><a id="org233be5f"></a>torch.sum(input, dim, keepdim=False, out=None) → Tensortorch<br />
<div class="outline-text-5" id="text-20-3-3-6">
<p>
返回新的张量，其中包括输入张量 input 中指定维度 dim 中每行的和。<br />
</p>

<p>
若 keepdim 值为 True，则在输出张量中，除了被操作的 dim 维度值降为 1，其它维度与输入张量 input 相同。否则，dim 维度相当于被执行 torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org7e19f18"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-6-1">
<ul class="org-ul">
<li>input (Tensor) - 输入 Tensor<br /></li>
<li>dim (int) - 指定维度<br /></li>
<li>keepdim (bool) - 输出张量是否保持与输入张量有相同数量的维度<br /></li>
<li>out (Tensor,optional) - 结果张量<br /></li>
</ul>
</div>
</li>
<li><a id="orge7522c1"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-6-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #dcaeea;">a</span> = torch.rand(<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">4</span>)
<span class="linenr">3: </span><span style="color: #51afef;">print</span>(a)
<span class="linenr">4: </span><span style="color: #51afef;">print</span>(torch.<span style="color: #c678dd;">sum</span>(a, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #a9a1e1;">True</span>))
<span class="linenr">5: </span><span style="color: #51afef;">print</span>(torch.<span style="color: #c678dd;">sum</span>(a, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #a9a1e1;">False</span>))
<span class="linenr">6: </span><span style="color: #51afef;">print</span>(torch.<span style="color: #c678dd;">sum</span>(a, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #a9a1e1;">True</span>))
<span class="linenr">7: </span><span style="color: #51afef;">print</span>(torch.<span style="color: #c678dd;">sum</span>(a, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #a9a1e1;">False</span>))
</pre>
</div>

<pre class="example">
tensor([[0.4779, 0.2066, 0.1321, 0.0134],
        [0.4947, 0.9133, 0.4378, 0.3185],
        [0.0167, 0.6487, 0.8574, 0.4689],
        [0.6329, 0.9853, 0.6804, 0.4612]])
tensor([[0.8300],
        [2.1643],
        [1.9917],
        [2.7598]])
tensor([0.8300, 2.1643, 1.9917, 2.7598])
tensor([[1.6222, 2.7539, 2.1078, 1.2620]])
tensor([0.8300, 2.1643, 1.9917, 2.7598])
</pre>
</div>
</li>
</ol>
</li>

<li><a id="orgc71b6b9"></a>torch.prod(input) → float<br />
<div class="outline-text-5" id="text-20-3-3-7">
<p>
返回输入张量 input 所有元素的乘积。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org3f1b334"></a>參數:<br />
<div class="outline-text-6" id="text-20-3-3-7-1">
<ul class="org-ul">
<li>input (Tensor) - 输入 Tensor<br /></li>
</ul>
</div>
</li>
<li><a id="org1489ed5"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-7-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span>  <span style="color: #dcaeea;">a</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">3: </span>  <span style="color: #51afef;">print</span>(a)
<span class="linenr">4: </span>  <span style="color: #51afef;">print</span>(torch.prod(a))
<span class="linenr">5: </span>  <span style="color: #dcaeea;">b</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">6: </span>  <span style="color: #51afef;">print</span>(b)
<span class="linenr">7: </span>  <span style="color: #51afef;">print</span>(torch.prod(b))
</pre>
</div>

<pre class="example">
tensor([[ 0.5791, -0.8357, -0.7155]])
tensor(0.3463)
tensor([[ 1.2112, -0.6698,  1.2576],
        [ 0.3263, -1.0300, -0.8296]])
tensor(-0.2845)
</pre>
</div>
</li>
</ol>
</li>

<li><a id="orgab26338"></a>torch.prod(input, dim, keepdim=False, out=None) → Tensor<br />
<div class="outline-text-5" id="text-20-3-3-8">
<p>
返回新的张量，其中包括输入张量 input 中指定维度 dim 中每行的乘积。<br />
</p>

<p>
若 keepdim 值为 True，则在输出张量中，除了被操作的 dim 维度值降为 1，其它维度与输入张量 input 相同。否则，dim 维度相当于被执行 torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="orgc9fbe04"></a>參數:<br />
<div class="outline-text-6" id="text-20-3-3-8-1">
<ul class="org-ul">
<li>input (Tensor) - 输入<br /></li>
<li>Tensordim (int) - 指定维度<br /></li>
<li>keepdim (bool) - 输出张量是否保持与输入张量有相同数量的维度<br /></li>
<li>out (Tensor,optional) - 结果张量<br /></li>
</ul>
</div>
</li>
<li><a id="orgdb5efef"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-8-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #dcaeea;">a</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">4</span>)
<span class="linenr">3: </span><span style="color: #51afef;">print</span>(a)
<span class="linenr">4: </span><span style="color: #51afef;">print</span>(torch.prod(a,<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #a9a1e1;">True</span>))
<span class="linenr">5: </span><span style="color: #51afef;">print</span>(torch.prod(a,<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #a9a1e1;">False</span>))
<span class="linenr">6: </span><span style="color: #51afef;">print</span>(torch.prod(a,<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #a9a1e1;">True</span>))
</pre>
</div>

<pre class="example">
tensor([[-0.0529,  1.9590, -0.8325, -1.5983],
        [-2.0691, -1.6575, -1.0308,  0.4937]])
tensor([[-0.1378],
        [-1.7454]])
tensor([-0.1378, -1.7454])
tensor([[ 0.1094, -3.2470,  0.8581, -0.7891]])
</pre>
</div>
</li>
</ol>
</li>

<li><a id="org0ca82b9"></a>torch.Tensor.indexadd(dim, index, tensor) → Tensor<br />
<div class="outline-text-5" id="text-20-3-3-9">
<p>
按索引参数 index 中所确定的顺序，将参数张量 tensor 中的元素与执行本方法的张量的元素逐个相加。参数 tensor 的尺寸必须严格地与执行方法的张量匹配，否则会发生错误。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="orged34eee"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-9-1">
<ul class="org-ul">
<li>dim (int) - 索引 index 所指向的维度<br /></li>
<li>index (LongTensor) - 包含索引数的张量<br /></li>
<li>tensor (Tensor) - 含有相加元素的张量<br /></li>
</ul>
</div>
</li>
<li><a id="orgaf3df8f"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-9-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #dcaeea;">x</span> = torch.Tensor([[<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>]])
<span class="linenr">3: </span><span style="color: #51afef;">print</span>(x)
<span class="linenr">4: </span><span style="color: #dcaeea;">t</span> = torch.Tensor([[<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">6</span>], [<span style="color: #da8548; font-weight: bold;">7</span>, <span style="color: #da8548; font-weight: bold;">8</span>, <span style="color: #da8548; font-weight: bold;">9</span>]])
<span class="linenr">5: </span><span style="color: #dcaeea;">index</span> = torch.LongTensor([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">6: </span><span style="color: #51afef;">print</span>(x.index_add_(<span style="color: #da8548; font-weight: bold;">0</span>, index, t))
</pre>
</div>

<pre class="example">
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
tensor([[ 2.,  3.,  4.],
        [ 8.,  9., 10.],
        [ 5.,  6.,  7.]])
</pre>
</div>
</li>
</ol>
</li>

<li><a id="org487e246"></a>torch.mean(input)<br />
<div class="outline-text-5" id="text-20-3-3-10">
<p>
返回输入张量 input 中每个元素的平均值。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org579d598"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-10-1">
<p>
input (Tensor) – 输入张量<br />
</p>
</div>
</li>
<li><a id="org1d962eb"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-10-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #dcaeea;">a</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">3: </span><span style="color: #51afef;">print</span>(a)
<span class="linenr">4: </span><span style="color: #51afef;">print</span>(torch.mean(a))
<span class="linenr">5: </span><span style="color: #dcaeea;">b</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">3</span>,<span style="color: #da8548; font-weight: bold;">5</span>)
<span class="linenr">6: </span><span style="color: #51afef;">print</span>(b)
<span class="linenr">7: </span><span style="color: #51afef;">print</span>(torch.mean(b))
</pre>
</div>

<pre class="example">
tensor([[ 0.3628, -0.4053, -1.5295]])
tensor(-0.5240)
tensor([[ 0.5471, -1.1326, -1.1903,  0.2861, -0.3787],
        [ 1.9305,  0.6218,  0.0304,  0.6502,  0.3808],
        [-0.1583, -0.5865,  0.9115,  1.3594, -1.1852]])
tensor(0.1391)
</pre>
</div>
</li>
</ol>
</li>

<li><a id="org60ca4c8"></a>torch.mean(input, dim, keepdim=False, out=None)<br />
<div class="outline-text-5" id="text-20-3-3-11">
<p>
返回新的张量，其中包含输入张量 input 指定维度 dim 中每行的平均值。<br />
</p>

<p>
若 keepdim 值为 True，则在输出张量中，除了被操作的 dim 维度值降为 1，其它维度与输入张量 input 相同。否则，dim 维度相当于被执行 torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org730461d"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-11-1">
<ul class="org-ul">
<li>input (Tensor) - 输入张量<br /></li>
<li>dim (int) - 指定进行均值计算的维度<br /></li>
<li>keepdim (bool, optional) - 输出张量是否保持与输入张量有相同数量的维度<br /></li>
<li>out (Tensor) - 结果张量<br /></li>
</ul>
</div>
</li>
<li><a id="org86b557c"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-11-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #dcaeea;">a</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">3</span>,<span style="color: #da8548; font-weight: bold;">4</span>)
<span class="linenr">3: </span><span style="color: #51afef;">print</span>(a)
<span class="linenr">4: </span><span style="color: #51afef;">print</span>(torch.mean(a, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #a9a1e1;">True</span>))
<span class="linenr">5: </span><span style="color: #51afef;">print</span>(torch.mean(a, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #a9a1e1;">False</span>))
<span class="linenr">6: </span><span style="color: #51afef;">print</span>(torch.mean(a, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #a9a1e1;">True</span>))
</pre>
</div>

<pre class="example">
tensor([[-0.6692, -0.8781, -1.1615, -0.4666],
        [ 1.6185, -1.4797,  0.3792,  0.6478],
        [ 2.5029,  0.5638, -0.3332,  0.6206]])
tensor([[-0.7939],
        [ 0.2914],
        [ 0.8385]])
tensor([-0.7939,  0.2914,  0.8385])
tensor([[ 1.1507, -0.5980, -0.3718,  0.2673]])
</pre>
</div>
</li>
</ol>
</li>

<li><a id="org53fa244"></a>orch.var(input, unbiased=True) → float<br />
<div class="outline-text-5" id="text-20-3-3-12">
<p>
返回输入向量 input 中所有元素的方差。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="orgb10f14f"></a>參數:<br />
<div class="outline-text-6" id="text-20-3-3-12-1">
<ul class="org-ul">
<li>input (Tensor) - 输入张量<br /></li>
<li>unbiased (bool) - 是否使用基于修正贝塞尔函数的无偏估计<br /></li>
</ul>
</div>
</li>
<li><a id="org6db08b2"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-12-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #dcaeea;">a</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">3: </span><span style="color: #51afef;">print</span>(a)
<span class="linenr">4: </span><span style="color: #51afef;">print</span>(torch.var(a))
<span class="linenr">5: </span><span style="color: #51afef;">print</span>(torch.var(a, unbiased = <span style="color: #a9a1e1;">False</span>))
</pre>
</div>

<pre class="example">
tensor([[-1.6751,  0.0469, -1.4719]])
tensor(0.8856)
tensor(0.5904)
</pre>
</div>
</li>
</ol>
</li>

<li><a id="orgdec37ee"></a>torch.var(input, dim, keepdim=False, unbiased=True, out=None) → Tensor<br />
<div class="outline-text-5" id="text-20-3-3-13">
<p>
返回新的张量，其中包括输入张量 input 中指定维度 dim 中每行的方差。<br />
</p>

<p>
若 keepdim 值为 True，则在输出张量中，除了被操作的 dim 维度值降为 1，其它维度与输入张量 input 相同。否则，dim 维度相当于被执行 torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org3d917f4"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-13-1">
<ul class="org-ul">
<li>input (Tensor) - 输入 Tensor<br /></li>
<li>dim (int) - 指定维度<br /></li>
<li>keepdim (bool) - 输出张量是否保持与输入张量有相同数量的维度<br /></li>
<li>unbiased (bool) - 是否使用基于修正贝塞尔函数的无偏估计<br /></li>
</ul>
<p>
= out (Tensor,optional) - 结果张量<br />
</p>
</div>
</li>
<li><a id="orga7b887e"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-13-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #dcaeea;">a</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">4</span>)
<span class="linenr">3: </span><span style="color: #51afef;">print</span>(a)
<span class="linenr">4: </span><span style="color: #51afef;">print</span>(torch.var(a, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #a9a1e1;">True</span>))
<span class="linenr">5: </span><span style="color: #51afef;">print</span>(torch.var(a, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #a9a1e1;">False</span>))
<span class="linenr">6: </span><span style="color: #51afef;">print</span>(torch.var(a, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #a9a1e1;">True</span>))
<span class="linenr">7: </span><span style="color: #51afef;">print</span>(torch.var(a, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #a9a1e1;">False</span>))
</pre>
</div>

<pre class="example">
tensor([[ 0.1367, -0.7436, -1.1812, -1.2855],
        [-0.1468, -1.8947, -0.3956,  2.1252],
        [-1.4925, -0.2174, -0.5797, -1.0848],
        [-0.0302, -0.5172, -1.6083,  1.1392]])
tensor([0.4192, 2.7533, 0.3137, 1.2982])
tensor([0.3144, 2.0650, 0.2352, 0.9736])
tensor([0.5604, 0.5378, 0.3104, 2.8145])
tensor([0.4203, 0.4033, 0.2328, 2.1109])
</pre>
</div>
</li>
</ol>
</li>

<li><a id="orgb14d425"></a>torch.max(input) → float<br />
<div class="outline-text-5" id="text-20-3-3-14">
<p>
返回输入张量所有元素的最大值。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="orgc17cda7"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-14-1">
<ul class="org-ul">
<li>input (Tensor) - 输入张量<br /></li>
</ul>
</div>
</li>
<li><a id="orgd5149d6"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-14-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #dcaeea;">a</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">3: </span><span style="color: #51afef;">print</span>(a)
<span class="linenr">4: </span><span style="color: #51afef;">print</span>(torch.<span style="color: #c678dd;">max</span>(a))
</pre>
</div>

<pre class="example">
tensor([[ 0.0471, -1.2301, -0.5085],
        [ 0.5317, -1.2448,  0.2068],
        [ 0.5227,  0.4452, -0.3307]])
tensor(0.5317)
</pre>
</div>
</li>
</ol>
</li>

<li><a id="orgd6a02ab"></a>torch.max(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)<br />
<div class="outline-text-5" id="text-20-3-3-15">
<p>
返回新的张量，其中包括输入张量 input 中指定维度 dim 中每行的最大值，同时返回每个最大值的位置索引。<br />
</p>

<p>
若 keepdim 值为 True，则在输出张量中，除了被操作的 dim 维度值降为 1，其它维度与输入张量 input 相同。否则，dim 维度相当于被执行 torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org10a59a6"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-15-1">
<ul class="org-ul">
<li>input (Tensor) - 输入 Tensor<br /></li>
<li>dim (int) - 指定维度<br /></li>
<li>keepdim (bool) - 输出张量是否保持与输入张量有相同数量的维度<br /></li>
</ul>
<p>
= out (Tensor,optional) - 结果张量<br />
</p>
</div>
</li>
<li><a id="org2450e3c"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-15-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #dcaeea;">a</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">3: </span><span style="color: #51afef;">print</span>(a)
<span class="linenr">4: </span><span style="color: #51afef;">print</span>(torch.<span style="color: #c678dd;">max</span>(a, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #a9a1e1;">True</span>))
<span class="linenr">5: </span><span style="color: #51afef;">print</span>(torch.<span style="color: #c678dd;">max</span>(a, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #a9a1e1;">True</span>))
</pre>
</div>

<pre class="example">
tensor([[-0.4094, -0.6951, -0.1855],
        [-1.2454,  0.8155, -0.3386],
        [ 0.0014,  0.0167, -0.5409]])
torch.return_types.max(
values=tensor([[-0.1855],
        [ 0.8155],
        [ 0.0167]]),
indices=tensor([[2],
        [1],
        [1]]))
torch.return_types.max(
values=tensor([[ 0.0014,  0.8155, -0.1855]]),
indices=tensor([[2, 1, 0]]))
</pre>
</div>
</li>
</ol>
</li>

<li><a id="orgbb3f62f"></a>torch.max(input, other, out=None) → Tensor<br />
<div class="outline-text-5" id="text-20-3-3-16">
<p>
逐个元素比较张量 input 与张量 other，将比较出的最大值保存到输出张量中。<br />
两个张量尺寸不需要完全相同，但需要支持自动扩展法则。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="orgc96b7a5"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-16-1">
<ul class="org-ul">
<li>input (Tensor) - 输入 Tensor<br /></li>
<li>other (Tensor) - 另一个输入的 Tensor<br /></li>
<li>out (Tensor,optional) - 结果张量<br /></li>
</ul>
</div>
</li>
<li><a id="org02b6fe7"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-16-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #dcaeea;">a</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">4</span>)
<span class="linenr">3: </span><span style="color: #51afef;">print</span>(a)
<span class="linenr">4: </span><span style="color: #dcaeea;">b</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">5: </span><span style="color: #51afef;">print</span>(b)
<span class="linenr">6: </span><span style="color: #51afef;">print</span>(torch.<span style="color: #c678dd;">max</span>(a, b))
</pre>
</div>

<pre class="example">
tensor([-0.8179,  0.5469,  2.1019, -0.3898])
tensor([1.5829])
tensor([1.5829, 1.5829, 2.1019, 1.5829])
</pre>
</div>
</li>
</ol>
</li>

<li><a id="org1b7b88b"></a>torch.min(input) → float<br />
<div class="outline-text-5" id="text-20-3-3-17">
<p>
返回输入张量所有元素的最小值。torch<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org041ec76"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-17-1">
<ul class="org-ul">
<li>input (Tensor) - 输入 Tensor<br /></li>
</ul>
</div>
</li>
<li><a id="org919c161"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-17-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #dcaeea;">a</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">3: </span><span style="color: #51afef;">print</span>(a)
<span class="linenr">4: </span><span style="color: #51afef;">print</span>(torch.<span style="color: #c678dd;">min</span>(a))
<span class="linenr">5: </span>
</pre>
</div>

<pre class="example">
tensor([[ 1.0204, -0.4053,  0.7325],
        [ 2.9686,  0.0682,  1.0189],
        [ 0.3362, -1.4859,  1.3518]])
tensor(-1.4859)
</pre>
</div>
</li>
</ol>
</li>

<li><a id="org72f1b38"></a>torch.min(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)<br />
<div class="outline-text-5" id="text-20-3-3-18">
<p>
返回新的张量，其中包括输入张量 input 中指定维度 dim 中每行的最小值，同时返回每个最小值的位置索引。<br />
</p>

<p>
若 keepdim 值为 True，则在输出张量中，除了被操作的 dim 维度值降为 1，其它维度与输入张量 input 相同。否则，dim 维度相当于被执行 torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org1dd65d6"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-18-1">
<ul class="org-ul">
<li>input (Tensor) - 输入 Tensor<br /></li>
<li>dim (int) - 指定维度<br /></li>
<li>keepdim (bool) - 输出张量是否保持与输入张量有相同数量的维度<br /></li>
</ul>
<p>
= out (Tensor,optional) - 结果张量<br />
</p>
</div>
</li>
<li><a id="org0107bce"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-18-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">3: </span><span style="color: #dcaeea;">a</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">4: </span><span style="color: #51afef;">print</span>(a)
<span class="linenr">5: </span><span style="color: #51afef;">print</span>(torch.<span style="color: #c678dd;">min</span>(a, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #a9a1e1;">True</span>))
<span class="linenr">6: </span><span style="color: #51afef;">print</span>(torch.<span style="color: #c678dd;">min</span>(a, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #a9a1e1;">True</span>))
</pre>
</div>

<pre class="example">
tensor([[ 0.4263, -0.1528, -0.6943],
        [-0.5110, -0.4034, -1.8242],
        [ 0.0458,  1.1824,  1.5221]])
torch.return_types.min(
values=tensor([[-0.6943],
        [-1.8242],
        [ 0.0458]]),
indices=tensor([[2],
        [2],
        [0]]))
torch.return_types.min(
values=tensor([[-0.5110, -0.4034, -1.8242]]),
indices=tensor([[1, 1, 1]]))
</pre>
</div>
</li>
</ol>
</li>

<li><a id="orgba5f41b"></a>torch.min(input, other, out=None) → Tensor<br />
<div class="outline-text-5" id="text-20-3-3-19">
<p>
逐个元素比较张量 input 与张量 other，将比较出的最小值保存到输出张量中。<br />
两个张量尺寸不需要完全相同，但需要支持自动扩展法则。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="orgf75132e"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-19-1">
<ul class="org-ul">
<li>input (Tensor) - 输入 Tensor<br /></li>
<li>other (Tensor) - 另一个输入的 Tensor<br /></li>
<li>out (Tensor,optional) - 结果张量<br /></li>
</ul>
</div>
</li>
<li><a id="org3bee717"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-19-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #dcaeea;">a</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">4</span>)
<span class="linenr">3: </span><span style="color: #51afef;">print</span>(a)
<span class="linenr">4: </span><span style="color: #dcaeea;">b</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">5: </span><span style="color: #51afef;">print</span>(b)
<span class="linenr">6: </span><span style="color: #51afef;">print</span>(torch.<span style="color: #c678dd;">min</span>(a, b))
</pre>
</div>

<pre class="example">
tensor([0.8462, 0.6718, 0.2461, 0.2186])
tensor([1.2698])
tensor([0.8462, 0.6718, 0.2461, 0.2186])
</pre>
</div>
</li>
</ol>
</li>

<li><a id="org74c346b"></a>torch.rsqrt(input) → Tensor<br />
<div class="outline-text-5" id="text-20-3-3-20">
<p>
返回新的张量，其中包含 input 张量每个元素平方根的倒数。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org4307b45"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-20-1">
<ul class="org-ul">
<li>input (Tensor) – 输入张量<br /></li>
<li>out (Tensor, optional) – 输出张量<br /></li>
</ul>
</div>
</li>
<li><a id="orgb085673"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-20-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span><span style="color: #dcaeea;">a</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">4</span>)
<span class="linenr">3: </span><span style="color: #51afef;">print</span>(a)
<span class="linenr">4: </span><span style="color: #51afef;">print</span>(torch.rsqrt(a))
</pre>
</div>

<pre class="example">
tensor([-0.1436,  0.4214,  0.5820, -0.7108])
tensor([   nan, 1.5404, 1.3108,    nan])
</pre>
</div>
</li>
</ol>
</li>

<li><a id="org9c8d520"></a>torch.lerp(star,end,weight) → Tensor<br />
<div class="outline-text-5" id="text-20-3-3-21">
<p>
基于 weight 对输入的两个张量 start 与 end 逐个元素计算线性插值，结果返回至输出张量。<br />
</p>

<p>
返回结果是： \( outs_i = start_i + weight * (end_i - start_i) \)<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org6a68400"></a>參數:<br />
<div class="outline-text-6" id="text-20-3-3-21-1">
<ul class="org-ul">
<li>start (Tensor) – 起始点张量<br /></li>
<li>end (Tensor) – 终止点张量<br /></li>
<li>weight (float) – 插入公式的 weight<br /></li>
<li>out (Tensor, optional) – 结果张量<br /></li>
</ul>
</div>
</li>
<li><a id="orgf41cc59"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-21-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">start</span> = torch.arange(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">5</span>)
<span class="linenr">4: </span><span style="color: #dcaeea;">end</span> = torch.Tensor(<span style="color: #da8548; font-weight: bold;">4</span>).fill_(<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">5: </span><span style="color: #51afef;">print</span>(start)
<span class="linenr">6: </span><span style="color: #51afef;">print</span>(torch.lerp(start, end, <span style="color: #da8548; font-weight: bold;">0.5</span>))
</pre>
</div>
</div>
</li>
</ol>
</li>

<li><a id="org3d6a2ef"></a>torch.tanh(input, out=None) → Tensor<br />
<div class="outline-text-5" id="text-20-3-3-22">
<p>
返回新的张量，其中包括输入张量 input 中每个元素的双曲正切。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org051ff0c"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-22-1">
<ul class="org-ul">
<li>input (Tensor) - 输入张量<br /></li>
<li>out (Tensor,optional) - 结果张量<br /></li>
</ul>
</div>
</li>
<li><a id="org7fcfe10"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-22-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">a</span> = torch.randn(<span style="color: #da8548; font-weight: bold;">4</span>)
<span class="linenr">4: </span><span style="color: #51afef;">print</span>(a)
<span class="linenr">5: </span><span style="color: #51afef;">print</span>(torch.tanh(a))
</pre>
</div>

<pre class="example">
tensor([-0.9255, -1.4683,  0.0821, -0.3610])
tensor([-0.7285, -0.8992,  0.0819, -0.3461])
</pre>
</div>
</li>
</ol>
</li>

<li><a id="orgfc1af1c"></a>torch.equal(tensor1, tensor2) → bool<br />
<div class="outline-text-5" id="text-20-3-3-23">
<p>
如果两个张量的尺寸和元素都相同，则返回 True，否则返回 False。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org82a0855"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-23-1">
<ul class="org-ul">
<li>tensor1 (Tensor) - 要比较的张量<br /></li>
<li>tensor2 (Tensor) - 要比较的张量<br /></li>
</ul>
</div>
</li>
<li><a id="org937c161"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-23-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #51afef;">print</span>(torch.equal(torch.Tensor([<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>]), torch.Tensor([<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>])))
</pre>
</div>

<pre class="example">
True
</pre>
</div>
</li>
</ol>
</li>

<li><a id="org2d74f20"></a>torch.gt(input, other, out=None) → Tensor<br />
<div class="outline-text-5" id="text-20-3-3-24">
<p>
计算 input tensor &gt; other<br />
</p>

<p>
逐个元素比较输入张量 input 是否大于另外的张量或浮点数 other。若大于则返回为 True，否则返回 False。<br />
若张量 other 无法自动扩展成与输入张量 input 相同尺寸，则返回为 False。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="orgf00b470"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-24-1">
<ul class="org-ul">
<li>input (Tensor) - 要比较的张量<br /></li>
<li>other (Tensor or float) - 要比较的张量或浮点数<br /></li>
<li>out (Tensor, optional) - 输出张量，必须是 ByteTensor 或与输入张量相同。<br /></li>
</ul>
</div>
</li>
<li><a id="orgfc791b6"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-24-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">a</span> = torch.Tensor([[<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>], [<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">4</span>]])
<span class="linenr">4: </span><span style="color: #dcaeea;">b</span> = torch.Tensor([[<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">4</span>]])
<span class="linenr">5: </span>
<span class="linenr">6: </span><span style="color: #51afef;">print</span>(torch.gt(a, b))
<span class="linenr">7: </span><span style="color: #51afef;">print</span>(torch.gt(torch.Tensor([[<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>], [<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">4</span>]]), torch.Tensor([[<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">4</span>]])))
</pre>
</div>

<pre class="example">
tensor([[False,  True],
        [False, False]])
tensor([[False,  True],
        [False, False]])
</pre>
</div>
</li>
</ol>
</li>

<li><a id="orgd4c4484"></a>torch.ge(input, other, out=None) → Tensor<br />
<div class="outline-text-5" id="text-20-3-3-25">
<p>
计算 input tensor &gt;= other<br />
</p>

<p>
逐个元素比较输入张量 input 是否大于或等于另外的张量或浮点数 other。若大于或等于则返回为 True，否则返回 False。<br />
若张量 other 无法自动扩展成与输入张量 input 相同尺寸，则返回为 False。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org3610742"></a>參數<br />
<div class="outline-text-6" id="text-20-3-3-25-1">
<ul class="org-ul">
<li>input (Tensor) - 要比较的张量<br /></li>
<li>other (Tensor or float) - 要比较的张量或浮点数<br /></li>
<li>out (Tensor, optional) - 输出张量，必须是 ByteTensor 或与输入张量相同。<br /></li>
</ul>
</div>
</li>
<li><a id="orgc6e177a"></a>Example<br />
<div class="outline-text-6" id="text-20-3-3-25-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">a</span> = torch.Tensor([[<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>], [<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">4</span>]])
<span class="linenr">4: </span><span style="color: #dcaeea;">b</span> = torch.Tensor([[<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">4</span>]])
<span class="linenr">5: </span>
<span class="linenr">6: </span><span style="color: #51afef;">print</span>(torch.ge(a, b))
</pre>
</div>

<pre class="example">
tensor([[ True,  True],
        [False,  True]])
</pre>
</div>
</li>
</ol>
</li>
</ol>
</li>

<li><a id="org0bc61e8"></a>Variable（變數）以及模型自動更新器<br />
<div class="outline-text-4" id="text-20-3-4">
<p>
我們常常聽到人說「train 一個 model」，一個模型（model）代表著一系列的運算，將一個代表輸入（可能是文字、音訊、影像等任何你想的到的資料）的矩陣變成結果（可能是文字翻譯、影像辨識結果）的過程。而訓練（training）即是更新模型參數的過程。<br />
所以究竟該怎麼訓練一個模型？首先，我們必須先了解誤差（loss）是什麼。誤差代表我們的模型預測出來的結果和真實情況的差距，通常是一個純量（scalar）。得到誤差後，我們通常使用梯度下降法（gradient descent），藉由反向傳播（<a href="https://www.youtube.com/watch?v=ibJpTrp5mcE">backpropagation</a>）來更新我們的模型。如果你不知道什麼是<a href="https://www.youtube.com/watch?v=NjZygLDXxjg">梯度下降</a>和反向傳播，你可以想像是一個更新參數的方式，在更新的過程中，誤差將會從結果往資料流向相反的方向傳遞。我們並不是一次就能夠更新到最後的結果；相反的，我們每次只走了一小步（還不一定每次都是正確的方向）。不過我們希望，這些更新的累積能減少誤差，使我們的預測越來越接近真實結果。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="orge9b2bd0"></a>Variable<br />
<div class="outline-text-5" id="text-20-3-4-1">
<p>
Variable 為 PyTorch 裡建立模型的最小元件。深度學習的模型常常用一層一層的 layer 來作為變數操作的單位。Layer 又是五花八門，常用的有 Full-connected layer，Convolutional layer、Recurrent layer 等等。每一種 layer 通常包含不只一個 Variable 的操作。Pytorch 的模組可以把這些操作群組在一起。模組甚至可以包含其他模組，組成一個樹狀結構。如此一來，變數的建立與管理變得十分方便。事實上，我們通常把整個模型包裝成一個模組，這麼做尤其在儲存和載入模型的時候非常有用。<br />
</p>

<p>
一個 Variable 最重要的屬性（attribute）是 data，它是一個 Tensor 物件，儲存這個變數現在的值。一個 Variable 創建與使用方式長這個樣子：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> torch.autograd <span style="color: #51afef;">import</span> Variable
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">m1</span> = torch.ones(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr"> 5: </span><span style="color: #dcaeea;">m2</span> = torch.ones(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr"> 6: </span><span style="color: #dcaeea;">a</span> = Variable(m1)
<span class="linenr"> 7: </span><span style="color: #dcaeea;">b</span> = Variable(m2)
<span class="linenr"> 8: </span><span style="color: #dcaeea;">c</span> = a + b
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #51afef;">print</span>(c)
</pre>
</div>

<pre class="example">
tensor([[2., 2., 2.],
        [2., 2., 2.]])
</pre>


<p>
<img src="images/autograd-2.jpg" alt="autograd-2.jpg" /><br />
<img src="images/autograd-3.jpg" alt="autograd-3.jpg" /><br />
</p>

<p>
PyTorch 的優勢之一為：幾乎所有對 Tensor 的操作都可以用在 Variable 上！所以我們使用者只需要熟悉一種語法。而，搭配前面對於有向圖的說明，大家必須了解到其背後所做的事情是不一樣的！Tensor 的操作是單純的資料修改，沒有紀錄；而 Variable 的操作除了 data 的資料會有改動，所有的操作也會記錄下來變成一個有向圖，藉由 creator 這個屬性儲存起來。<br />
Variable 還有兩個重要的屬性。<br />
</p>
<ul class="org-ul">
<li>requires_grad<br /></li>
</ul>
<p>
指定要不要更新這個變數，對於不需要更新的變數可以把他設定成 False，可以加快運算。<br />
</p>
<ul class="org-ul">
<li>volatile<br /></li>
</ul>
<p>
指定需不需要保留紀錄用的變數。指定變數為 True 代表運算不需要記錄，可以加快運算。如果一個變數的 volatil 是 True，則它的 requires_grad 一定是 False。<br />
</p>
</div>
</li>
<li><a id="org315261e"></a>Parameter<br />
<div class="outline-text-5" id="text-20-3-4-2">
<p>
參數（Parameter）是變數（Variable）的子物件。意思是說，它們能做到的事情幾乎一模一樣。唯一的不同點是，因為 Module 會維護自己用到參數的集合，當我們將 Parameter 物件指定給模組的屬性時，它就會被記錄在這個集合裡，而且還會有一個唯一對應的名稱；Variable 不會有這種效果。<br />
</p>

<p>
模組的套疊在這個邏輯也能正確的運作。例如說，子模組的參數也會自動變成父模組的參數。在下面的範例中，nn.Conv2d 是個內建的模組，包含兩個參數 weight 和 bias，正確的變成我們自訂模組的參數。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> torch.nn <span style="color: #51afef;">as</span> nn
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> torch.nn.functional <span style="color: #51afef;">as</span> F
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">Model</span>(nn.Module):
<span class="linenr"> 5: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>):
<span class="linenr"> 6: </span>          <span style="color: #c678dd;">super</span>().__init__()
<span class="linenr"> 7: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">conv1</span> = nn.Conv2d(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">20</span>, <span style="color: #da8548; font-weight: bold;">5</span>)    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35387;&#20874;&#20102;conv1&#36889;&#20491;&#21517;&#23383;</span>
<span class="linenr"> 8: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">conv2</span> = nn.Conv2d(<span style="color: #da8548; font-weight: bold;">20</span>, <span style="color: #da8548; font-weight: bold;">20</span>, <span style="color: #da8548; font-weight: bold;">5</span>)   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35387;&#20874;&#20102;conv2&#36889;&#20491;&#21517;&#23383;</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">forward</span>(<span style="color: #51afef;">self</span>, x):
<span class="linenr">11: </span>         <span style="color: #dcaeea;">x</span> = F.relu(<span style="color: #51afef;">self</span>.conv1(x))
<span class="linenr">12: </span>         <span style="color: #51afef;">return</span> F.relu(<span style="color: #51afef;">self</span>.conv2(x))
<span class="linenr">13: </span>
<span class="linenr">14: </span>  <span style="color: #51afef;">print</span>(Model().parameters())    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26371;&#21360;&#20986;4&#20491;&#21443;&#25976;'conv1.weight', 'conv1.bias',</span>
<span class="linenr">15: </span>                                 <span style="color: #5B6268;"># </span><span style="color: #5B6268;">'conv2.weight', 'conv2.bias'&#30340;&#20540;</span>
</pre>
</div>


<pre class="example">
&lt;generator object Module.parameters at 0x1239cabd0&gt;
</pre>


<p>
簡單來說，參數才是我們使用 Module 時候會面對到的物件，但一般來說這些差異都已經被包裝起來了，就如同上面的範例一樣。<br />
</p>
</div>
</li>

<li><a id="org2ea1394"></a>Module<br />
<div class="outline-text-5" id="text-20-3-4-3">
<p>
那怎麼使用模組呢？一般來說，我們只需要定義模組創建的時候用到的參數，以及模組從輸入到輸出做了怎樣的操作。前者被定義在__init__函數裡，後者被定義在 forward 函數裡。讓我們再看一次上面的範例：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> torch.nn <span style="color: #51afef;">as</span> nn
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> torch.nn.functional <span style="color: #51afef;">as</span> F
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">Model</span>(nn.Module):
<span class="linenr"> 5: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>):
<span class="linenr"> 6: </span>          <span style="color: #83898d;">"""</span>
<span class="linenr"> 7: </span><span style="color: #83898d;">          &#22312;__init__&#20989;&#25976;&#35041;&#23450;&#32681;&#36889;&#20491;&#27169;&#32068;&#26371;&#29992;&#21040;&#30340;&#21443;&#25976;</span>
<span class="linenr"> 8: </span><span style="color: #83898d;">          """</span>
<span class="linenr"> 9: </span>          <span style="color: #c678dd;">super</span>().__init__()
<span class="linenr">10: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">conv1</span> = nn.Conv2d(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">20</span>, <span style="color: #da8548; font-weight: bold;">5</span>)
<span class="linenr">11: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">conv2</span> = nn.Conv2d(<span style="color: #da8548; font-weight: bold;">20</span>, <span style="color: #da8548; font-weight: bold;">20</span>, <span style="color: #da8548; font-weight: bold;">5</span>)
<span class="linenr">12: </span>
<span class="linenr">13: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">forward</span>(<span style="color: #51afef;">self</span>, x):
<span class="linenr">14: </span>         <span style="color: #83898d;">"""</span>
<span class="linenr">15: </span><span style="color: #83898d;">         &#22312;forward&#20989;&#25976;&#35041;&#23450;&#32681;&#36664;&#20837;&#21644;&#36664;&#20986;&#20540;&#30340;&#38364;&#20418;</span>
<span class="linenr">16: </span><span style="color: #83898d;">         """</span>
<span class="linenr">17: </span>         <span style="color: #dcaeea;">x</span> = F.relu(<span style="color: #51afef;">self</span>.conv1(x))
<span class="linenr">18: </span>         <span style="color: #51afef;">return</span> F.relu(<span style="color: #51afef;">self</span>.conv2(x))
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20551;&#35373; _input&#26159;&#19968;&#20491;&#35722;&#25976;</span>
<span class="linenr">21: </span>  <span style="color: #dcaeea;">model</span> = Model()
<span class="linenr">22: </span>  <span style="color: #dcaeea;">y</span> = model(_input)    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">y&#23601;&#26159;&#25105;&#20497;&#27169;&#32068;&#30340;&#36664;&#20986;</span>
<span class="linenr">23: </span>
</pre>
</div>

<p>
PyTorch 使用 Python class 來代表管理一群參數的單位，我們能夠用物件的屬性直接存取內部用到的參數，這樣的架構是非常直覺並符合語義（semantics）的。<br />
</p>

<p>
幾個 Module 非常重要的功能：<br />
</p>

<ul class="org-ul">
<li>將資料搬到 CPU/GPU<br /></li>
</ul>

<p>
之前提過 PyTorch 支援 GPU 運算。Module 可以讓我們一次把所有包含的變數一次搬到 CPU/GPU。注意到兩個 Tensor 的運算只能在同一個 CPU/GPU 上執行，所以將所有變數一次搬移是個很重要的功能。呼叫 cpu()和 cuda()可以執行這個功能。另外，我們可以用 torch.cuda.is_available()來檢查我們可不可以使用 CUDA 來運算。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">model</span> = Model()
<span class="linenr">2: </span>  <span style="color: #51afef;">if</span> torch.cuda.is_available():
<span class="linenr">3: </span>      model.cuda()
</pre>
</div>

<ul class="org-ul">
<li>訓練/運算模式<br /></li>
</ul>

<p>
有很多模組在訓練的時候和預測的時候用到同樣的參數，但是執行的運算不一樣，例如 Dropout、Batch Normalization 等。因此在訓練和運算的時候，記得分別呼叫 train()和 eval()來切換模式。<br />
</p>

<p>
一般來說，我們會分別用不同的函式來包裝訓練和預測的功能。所以一個典型的程式會長的像下面這個樣子。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">update</span>(model, loader):
<span class="linenr">2: </span>      model.train()
<span class="linenr">3: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">...</span>
<span class="linenr">4: </span>
<span class="linenr">5: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">evaluate</span>(model, loader):
<span class="linenr">6: </span>      model.<span style="color: #c678dd;">eval</span>()
<span class="linenr">7: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">...</span>
</pre>
</div>

<ul class="org-ul">
<li>儲存/載入模型<br /></li>
</ul>
<p>
當我們訓練完一個模型，最重要的當然是把它儲存起來在日後使用。當我們呼叫 state_dict()，會拿到一個參數名稱對應到值的字典，然後我們可以呼叫 PyTorch 的內建函式把它儲存起來。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>torch.save(model.state_dict(), PATH)
</pre>
</div>

<p>
而日後要拿回來的時候，可以呼叫 load_state_dict 把值載入到對應的參數名稱。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>model.load_state_dict(torch.load(PATH))
</pre>
</div>
</div>
</li>

<li><a id="org0624356"></a>Autograd<br />
<div class="outline-text-5" id="text-20-3-4-4">
<p>
如同上面所說的，反向傳播是我們現在廣泛使用的更新模型方式。當我們定義了誤差如何計算的同時，其實也隱含定義了反向傳播的傳遞方向。這正是 Autograd 的運作原理：藉由前面所說的有向圖，PyTorch 可以自動幫我們計算梯度。我們只要對於誤差的 Variable 物件呼叫 backward 函數，就可以把沿途所用到參數的 gradient 都計算出來，儲存在各個參數的 grad 屬性裡。最後，更新每個參數的 data 值。通常，我們使用優化器（optimizer）來更新它們。<br />
</p>

<p>
優化器的使用方法也非常簡單。首先在初始化優化器時提供被更新參數的清單。在每一次更新前，先呼叫優化器的 zero_grad 把上一次更新時用到的梯度歸零（這一步很容易忘記。如果沒有做，backward 得到的梯度會被累加）。接著，呼叫 backward 將參數的 grad 算出來後，再呼叫 step 利用儲存的 grad 和 data 來計算新的 data 的值。<br />
</p>

<p>
就讓我們延續上面的範例來解釋使用原理。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> torch
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> torch.autograd <span style="color: #51afef;">import</span> Variable
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> torch.optim <span style="color: #51afef;">import</span> SGD
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">m1</span> = torch.ones(<span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">m2</span> = torch.ones(<span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35352;&#24471;&#35201;&#23559;requires_grad&#35373;&#25104;True</span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">a</span> = Variable(m1, requires_grad=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">10: </span>  <span style="color: #dcaeea;">b</span> = Variable(m2, requires_grad=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">11: </span>
<span class="linenr">12: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21021;&#22987;&#21270;&#20778;&#21270;&#22120;&#65292;&#20351;&#29992;SGD&#36889;&#20491;&#26356;&#26032;&#26041;&#24335;&#20358;&#26356;&#26032;a&#21644;b</span>
<span class="linenr">13: </span>  <span style="color: #dcaeea;">optimizer</span> = SGD([a, b], lr=<span style="color: #da8548; font-weight: bold;">0.1</span>)
<span class="linenr">14: </span>
<span class="linenr">15: </span>  <span style="color: #51afef;">for</span> _ <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">10</span>):        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25105;&#20497;&#31034;&#31684;&#26356;&#26032;10&#27425;</span>
<span class="linenr">16: </span>      <span style="color: #dcaeea;">loss</span> = (a + b).<span style="color: #c678dd;">sum</span>()   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20551;&#35373;a + b&#23601;&#26159;&#25105;&#20497;&#30340;loss</span>
<span class="linenr">17: </span>      <span style="color: #51afef;">print</span>(loss)
<span class="linenr">18: </span>      optimizer.zero_grad()
<span class="linenr">19: </span>      loss.backward()
<span class="linenr">20: </span>      optimizer.step()       <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26356;&#26032;</span>
<span class="linenr">21: </span>
</pre>
</div>

<pre class="example">
tensor(30., grad_fn=&lt;SumBackward0&gt;)
tensor(27.0000, grad_fn=&lt;SumBackward0&gt;)
tensor(24.0000, grad_fn=&lt;SumBackward0&gt;)
tensor(21.0000, grad_fn=&lt;SumBackward0&gt;)
tensor(18.0000, grad_fn=&lt;SumBackward0&gt;)
tensor(15.0000, grad_fn=&lt;SumBackward0&gt;)
tensor(12.0000, grad_fn=&lt;SumBackward0&gt;)
tensor(9.0000, grad_fn=&lt;SumBackward0&gt;)
tensor(6.0000, grad_fn=&lt;SumBackward0&gt;)
tensor(3.0000, grad_fn=&lt;SumBackward0&gt;)
</pre>

<p>
Backword 基本更新步驟：<br />
</p>
<ol class="org-ol">
<li>操作現有的參數與輸入的變數，得到預測。利用預測和正確答案定義我們的誤差。<br /></li>
<li>呼叫優化器的 zero_grad 將上次更新的梯度歸零。<br /></li>
<li>呼叫誤差的 backward 算出所有參數的梯度。<br /></li>
<li>呼叫優化器的 step 更新參數。<br /></li>
</ol>
</div>
</li>
</ol>
</li>

<li><a id="org146885e"></a>PyTorch 的優勢<br />
<ol class="org-ol">
<li><a id="org2e69d00"></a>與 Python、NumPy 的操作十分類似<br />
<div class="outline-text-5" id="text-20-3-5-1">
<p>
PyTorch 中 Tensor 的使用方式就和 NumPy 差不多，與 NumPy 之間的轉換也是非常容易。不過就算沒有用過 NumPy，也許你也能夠看出來 Tensor 的操作就和使用一個普通的 Python 變數沒有什麼差異。由於這些操作的方法有統一的規則，就算臨時忘記要用的功能，查閱說明文件很快就能夠找到。<br />
</p>
</div>
</li>

<li><a id="org8e59e76"></a>動態建立模型<br />
<div class="outline-text-5" id="text-20-3-5-2">
<p>
上述利用 autograd 更新的過程揭露了 PyTorch 和 TensorFlow、Theano 等其他深度學習框架最不一樣的差異：PyTorch 會動態的在每一次更新/計算結果的過程建立有向圖，每一行對 Variable 的操作都是建立模型的過程；其他框架會先編譯整個模型再開始更新/計算。也許有人會懷疑，每一次都要重新建立模型是否會讓運算速度變慢，但就我們的使用經驗是感覺不出來的。此一動態建立有向圖的過程有兩個好處：<br />
</p>
<ul class="org-ul">
<li>當我們的模型有錯誤的時候，PyTorch 會被迫中止在發生錯誤的地方，並立即回報錯誤原因。其他框架如 Keras，因為需要靜態建立模型並呼叫 compile，會在執行編譯時才回報錯誤的原因。要從錯誤的原因回推造成錯誤的程式碼不一定非常容易，這方面的差異大大的影響我們除錯的速度。<br /></li>
<li>動態的建立模型代表我們能夠根據每一次的輸入來建立對應的模型，這點對於某些特殊的 RNN 模型特別有用，在 TensorFlow 這樣靜態建立模型的框架中是很難實踐的。<br /></li>
</ul>
</div>
</li>

<li><a id="org72e7dd9"></a>PyTorch 與靜態建立模型的框架（TensorFlow）比較<br />
<div class="outline-text-5" id="text-20-3-5-3">
<p>
如果你有使用過 TensorFlow，這個段落的描述應該十分熟悉。對於 Tensorflow，如果要根據輸入來判斷執行不同的運算，唯一的做法是針對每一種可能的操作都預先建立模型，再利用一個判斷的物件 tf.cond 來執行不同的操作，如下面的範例（改寫自一篇 StackOverflow 的回答）<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">x</span> = tf.placeholder(tf.float32, shape=[<span style="color: #a9a1e1;">None</span>, <span style="color: #da8548; font-weight: bold;">20</span>], name=<span style="color: #98be65;">"x_input"</span>)
<span class="linenr">2: </span>      <span style="color: #dcaeea;">condition</span> = tf.placeholder(tf.int32, shape=[], name=<span style="color: #98be65;">"condition"</span>)
<span class="linenr">3: </span>      <span style="color: #dcaeea;">W</span> = tf.Variable(tf.zeros([<span style="color: #da8548; font-weight: bold;">20</span>, <span style="color: #da8548; font-weight: bold;">10</span>]), name=<span style="color: #98be65;">"weights"</span>)
<span class="linenr">4: </span>      <span style="color: #dcaeea;">b</span> = tf.Variable(tf.zeros([<span style="color: #da8548; font-weight: bold;">10</span>]), name=<span style="color: #98be65;">"bias"</span>)
<span class="linenr">5: </span>
<span class="linenr">6: </span>      <span style="color: #dcaeea;">y</span> = tf.cond(condition &gt; <span style="color: #da8548; font-weight: bold;">0</span>, 
<span class="linenr">7: </span>                  <span style="color: #51afef;">lambda</span>: tf.matmul(x, W) + b,
<span class="linenr">8: </span>                  <span style="color: #51afef;">lambda</span>: tf.matmul(x, W) - b)
<span class="linenr">9: </span>
</pre>
</div>
<p>
注意最後三行，對於 condition &gt; 0 的兩種操作都會在靜態建立模型時被執行到，而且必須被包裝成函式的型態（這裡使用 lambda 來建立匿名函式）。同樣的情況也出現在需要用到迴圈的模型。筆者認為這樣子的架構下寫出來的程式是十分迂迴，不符合直覺的。以下是 PyTorch 版本，因為是動態建立模型，直接使用一般的 Python 運算式。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #dcaeea;">x</span> = Variable(torch.randn(<span style="color: #da8548; font-weight: bold;">32</span>, <span style="color: #da8548; font-weight: bold;">20</span>), requires_grad=<span style="color: #a9a1e1;">True</span>)
<span class="linenr"> 2: </span>      <span style="color: #dcaeea;">W</span> = Variable(torch.zeros(<span style="color: #da8548; font-weight: bold;">20</span>, <span style="color: #da8548; font-weight: bold;">10</span>), requires_grad=<span style="color: #a9a1e1;">True</span>)
<span class="linenr"> 3: </span>      <span style="color: #dcaeea;">b</span> = Variable(torch.zeros(<span style="color: #da8548; font-weight: bold;">10</span>), requires_grad=<span style="color: #a9a1e1;">True</span>)
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>      <span style="color: #dcaeea;">y</span> = x.mm(W)
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>      <span style="color: #51afef;">if</span> condition &gt; <span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr"> 8: </span>          <span style="color: #dcaeea;">y</span> = y + b.expand_as(y)
<span class="linenr"> 9: </span>      <span style="color: #51afef;">else</span>:
<span class="linenr">10: </span>          y = y - b.expand_as(y)
<span class="linenr">11: </span>
</pre>
</div>
</div>
</li>
</ol>
</li>
</ol>
</div>


<div id="outline-container-org505d180" class="outline-3">
<h3 id="org505d180"><span class="section-number-3">20.4</span> PyTorch 簡單案例：線性迴歸</h3>
<div class="outline-text-3" id="text-20-4">
</div>
<ol class="org-ol">
<li><a id="orgdb8c985"></a>Example #1: 線性迴歸<br />
<div class="outline-text-4" id="text-20-4-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> sys
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> torch
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> torch.nn <span style="color: #51afef;">as</span> nn
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 5: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 6: </span>  <span style="color: #51afef;">from</span> torch.autograd <span style="color: #51afef;">import</span> Variable
<span class="linenr"> 7: </span>  <span style="color: #51afef;">from</span> torchsummary <span style="color: #51afef;">import</span> summary
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Hyper Parameters</span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">input_size</span> = <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">11: </span>  <span style="color: #dcaeea;">output_size</span> = <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">12: </span>  <span style="color: #dcaeea;">num_epochs</span> = <span style="color: #da8548; font-weight: bold;">1000</span>
<span class="linenr">13: </span>  <span style="color: #dcaeea;">learning_rate</span> = <span style="color: #da8548; font-weight: bold;">0.001</span>
<span class="linenr">14: </span>
<span class="linenr">15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">xtrain&#29986;&#29983;&#30697;&#38499;&#36039;&#26009;</span>
<span class="linenr">16: </span>  <span style="color: #dcaeea;">x_train</span> = np.array([[<span style="color: #da8548; font-weight: bold;">2.3</span>], [<span style="color: #da8548; font-weight: bold;">4.4</span>], [<span style="color: #da8548; font-weight: bold;">3.7</span>], [<span style="color: #da8548; font-weight: bold;">6.1</span>], [<span style="color: #da8548; font-weight: bold;">7.3</span>], [<span style="color: #da8548; font-weight: bold;">2.1</span>],[<span style="color: #da8548; font-weight: bold;">5.6</span>], [<span style="color: #da8548; font-weight: bold;">7.7</span>], [<span style="color: #da8548; font-weight: bold;">8.7</span>], [<span style="color: #da8548; font-weight: bold;">4.1</span>],
<span class="linenr">17: </span>                      [<span style="color: #da8548; font-weight: bold;">6.7</span>], [<span style="color: #da8548; font-weight: bold;">6.1</span>], [<span style="color: #da8548; font-weight: bold;">7.5</span>], [<span style="color: #da8548; font-weight: bold;">2.1</span>], [<span style="color: #da8548; font-weight: bold;">7.2</span>],
<span class="linenr">18: </span>                      [<span style="color: #da8548; font-weight: bold;">5.6</span>], [<span style="color: #da8548; font-weight: bold;">5.7</span>], [<span style="color: #da8548; font-weight: bold;">7.7</span>], [<span style="color: #da8548; font-weight: bold;">3.1</span>]], dtype=np.float32)
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #dcaeea;">y_train</span> = np.array([[<span style="color: #da8548; font-weight: bold;">3.7</span>], [<span style="color: #da8548; font-weight: bold;">4.76</span>], [<span style="color: #da8548; font-weight: bold;">4</span>.], [<span style="color: #da8548; font-weight: bold;">7.1</span>], [<span style="color: #da8548; font-weight: bold;">8.6</span>], [<span style="color: #da8548; font-weight: bold;">3.5</span>],[<span style="color: #da8548; font-weight: bold;">5.4</span>], [<span style="color: #da8548; font-weight: bold;">7.6</span>], [<span style="color: #da8548; font-weight: bold;">7.9</span>], [<span style="color: #da8548; font-weight: bold;">5.3</span>],
<span class="linenr">21: </span>                      [<span style="color: #da8548; font-weight: bold;">7.3</span>], [<span style="color: #da8548; font-weight: bold;">7.5</span>], [<span style="color: #da8548; font-weight: bold;">8.5</span>], [<span style="color: #da8548; font-weight: bold;">3.2</span>], [<span style="color: #da8548; font-weight: bold;">8.7</span>],
<span class="linenr">22: </span>                      [<span style="color: #da8548; font-weight: bold;">6.4</span>], [<span style="color: #da8548; font-weight: bold;">6.6</span>], [<span style="color: #da8548; font-weight: bold;">7.9</span>], [<span style="color: #da8548; font-weight: bold;">5.3</span>]], dtype=np.float32)
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Linear Regression Model</span>
<span class="linenr">25: </span>  <span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">LinearRegression</span>(nn.Module):
<span class="linenr">26: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">__init__&#23450;&#32681; model &#20013;&#38656;&#35201;&#30340;&#21443;&#25976;&#65292;weight&#12289;bias &#31561;&#31561;</span>
<span class="linenr">27: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>, input_size, output_size):
<span class="linenr">28: </span>          <span style="color: #c678dd;">super</span>(LinearRegression, <span style="color: #51afef;">self</span>).__init__()
<span class="linenr">29: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">linear</span> = nn.Linear(input_size, output_size)  
<span class="linenr">30: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23450;&#32681; model &#25509;&#25910; input &#26178;&#65292;data &#35201;&#24590;&#40636;&#20659;&#36958;&#12289;&#32147;&#36942;&#21738;&#20123; activation function &#31561;&#31561;</span>
<span class="linenr">31: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">forward</span>(<span style="color: #51afef;">self</span>, x):
<span class="linenr">32: </span>          <span style="color: #dcaeea;">out</span> = <span style="color: #51afef;">self</span>.linear(x)
<span class="linenr">33: </span>          <span style="color: #51afef;">return</span> out
<span class="linenr">34: </span>
<span class="linenr">35: </span>  <span style="color: #dcaeea;">model</span> = LinearRegression(input_size, output_size)
<span class="linenr">36: </span>
<span class="linenr">37: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Loss and Optimizer</span>
<span class="linenr">38: </span>  <span style="color: #dcaeea;">criterion</span> = nn.MSELoss()
<span class="linenr">39: </span>  <span style="color: #dcaeea;">optimizer</span> = torch.optim.SGD(model.parameters(), lr=learning_rate)
<span class="linenr">40: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Train the Model </span>
<span class="linenr">41: </span>  <span style="color: #51afef;">for</span> epoch <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(num_epochs):
<span class="linenr">42: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;x&#24478;numpy&#30340;&#26684;&#24335;&#36681;&#28858;torch&#30340;&#35722;&#25976;&#26684;&#24335;</span>
<span class="linenr">43: </span>      <span style="color: #dcaeea;">inputs</span> = Variable(torch.from_numpy(x_train))
<span class="linenr">44: </span>      <span style="color: #dcaeea;">targets</span> = Variable(torch.from_numpy(y_train))
<span class="linenr">45: </span>
<span class="linenr">46: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Forward + Backward + Optimize</span>
<span class="linenr">47: </span>      optimizer.zero_grad()  
<span class="linenr">48: </span>      <span style="color: #dcaeea;">outputs</span> = model(inputs)
<span class="linenr">49: </span>      <span style="color: #dcaeea;">loss</span> = criterion(outputs, targets)
<span class="linenr">50: </span>      loss.backward()
<span class="linenr">51: </span>      optimizer.step()
<span class="linenr">52: </span>
<span class="linenr">53: </span>      <span style="color: #51afef;">if</span> (epoch+<span style="color: #da8548; font-weight: bold;">1</span>) % <span style="color: #da8548; font-weight: bold;">100</span>   == <span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr">54: </span>  <span style="color: #5B6268;">#        </span><span style="color: #5B6268;">print ('Epoch [%d/%d], Loss: %.4f' %(epoch+1, num_epochs, loss.data[0]))</span>
<span class="linenr">55: </span>            <span style="color: #51afef;">print</span> (<span style="color: #98be65;">'Epoch [%d/%d], Loss: %.4f'</span> %(epoch+<span style="color: #da8548; font-weight: bold;">1</span>, num_epochs, loss.data))
<span class="linenr">56: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Plot the graph</span>
<span class="linenr">57: </span>  model.<span style="color: #c678dd;">eval</span>()
<span class="linenr">58: </span>  <span style="color: #dcaeea;">predicted</span> = model(Variable(torch.from_numpy(x_train))).data.numpy()
<span class="linenr">59: </span>
<span class="linenr">60: </span>  plt.figure() 
<span class="linenr">61: </span>  plt.scatter(x_train,y_train)
<span class="linenr">62: </span>  plt.xlabel(<span style="color: #98be65;">'x_train'</span>)
<span class="linenr">63: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">x&#36600;&#21517;&#31281;</span>
<span class="linenr">64: </span>  plt.ylabel(<span style="color: #98be65;">'y_train'</span>)
<span class="linenr">65: </span>
<span class="linenr">66: </span>  plt.plot(x_train, y_train, <span style="color: #98be65;">'ro'</span>)
<span class="linenr">67: </span>  plt.plot(x_train, predicted, label=<span style="color: #98be65;">'predict'</span>)
<span class="linenr">68: </span>  plt.legend()
<span class="linenr">69: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show() #for Google coLab environment</span>
<span class="linenr">70: </span>  plt.plot()
<span class="linenr">71: </span>  plt.savefig(<span style="color: #98be65;">"linearReg.png"</span>)
<span class="linenr">72: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21576;&#29694;model&#30340;layers&#29376;&#27841;</span>
<span class="linenr">73: </span>  summary(model, (<span style="color: #da8548; font-weight: bold;">19</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>))
</pre>
</div>

<pre class="example">
  Epoch [100/1000], Loss: 0.5649
  Epoch [200/1000], Loss: 0.5559
  Epoch [300/1000], Loss: 0.5474
  Epoch [400/1000], Loss: 0.5393
  Epoch [500/1000], Loss: 0.5316
  Epoch [600/1000], Loss: 0.5243
  Epoch [700/1000], Loss: 0.5172
  Epoch [800/1000], Loss: 0.5105
  Epoch [900/1000], Loss: 0.5042
  Epoch [1000/1000], Loss: 0.4981
  ----------------------------------------------------------------
          Layer (type)               Output Shape         Param #
  ================================================================
              Linear-1             [-1, 19, 1, 1]               2
  ================================================================
  Total params: 2
  Trainable params: 2
  Non-trainable params: 0
  ----------------------------------------------------------------
  Input size (MB): 0.00
  Forward/backward pass size (MB): 0.00
  Params size (MB): 0.00
  Estimated Total Size (MB): 0.00
  ----------------------------------------------------------------
</pre>


<div id="org2159d01" class="figure">
<p><img src="images/linearReg.png" alt="linearReg.png" /><br />
</p>
<p><span class="figure-number">Figure 139: </span>線性迴歸分佈圖</p>
</div>
</div>
</li>

<li><a id="org29a680a"></a>Example #2: 世界人口數預測<br />
<div class="outline-text-4" id="text-20-4-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> torch
<span class="linenr"> 5: </span>  <span style="color: #51afef;">import</span> torch.nn
<span class="linenr"> 6: </span>  <span style="color: #51afef;">import</span> torch.optim
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#19979;&#36617;&#20154;&#25976;</span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">try</span>:
<span class="linenr">10: </span>      url = r<span style="color: #98be65;">'http://en.wikipedia.org/wiki/World_population_estimates'</span>
<span class="linenr">11: </span>      <span style="color: #dcaeea;">df</span> = pd.read_html(url, header=<span style="color: #da8548; font-weight: bold;">0</span>, attrs={<span style="color: #98be65;">"class"</span> : <span style="color: #98be65;">"wikitable"</span>})[<span style="color: #da8548; font-weight: bold;">2</span>]
<span class="linenr">12: </span>  <span style="color: #51afef;">except</span>:
<span class="linenr">13: </span>      url = <span style="color: #98be65;">'https://raw.githubusercontent.com/MyDearGreatTeacher/AI201909/master/data/population.csv'</span>
<span class="linenr">14: </span>      <span style="color: #dcaeea;">df</span> = pd.read_csv(url, index_col=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">15: </span>  df
<span class="linenr">16: </span>
<span class="linenr">17: </span>
<span class="linenr">18: </span>  <span style="color: #dcaeea;">years</span> = torch.tensor(df.iloc[:, <span style="color: #da8548; font-weight: bold;">0</span>], dtype=torch.float32)
<span class="linenr">19: </span>  <span style="color: #dcaeea;">populations</span> = torch.tensor(df.iloc[:, <span style="color: #da8548; font-weight: bold;">1</span>], dtype=torch.float32)
<span class="linenr">20: </span>
<span class="linenr">21: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32218;&#24615;&#22238;&#27512;</span>
<span class="linenr">22: </span>  <span style="color: #dcaeea;">x</span> = torch.stack([years, torch.ones_like(years)], <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">23: </span>  <span style="color: #dcaeea;">y</span> = populations
<span class="linenr">24: </span>  <span style="color: #dcaeea;">wr</span>, <span style="color: #dcaeea;">_</span> = torch.lstsq(y, x)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26368;&#23567;&#24179;&#26041;&#21644;</span>
<span class="linenr">25: </span>  <span style="color: #dcaeea;">slope</span>, <span style="color: #dcaeea;">intercept</span> = wr[:<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">0</span>] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27714;&#26012;&#29575;&#12289;&#25130;&#36317;</span>
<span class="linenr">26: </span>  <span style="color: #dcaeea;">result</span> = <span style="color: #98be65;">'population = {:.2e} * year + {:.2e}'</span>.<span style="color: #c678dd;">format</span>(slope, intercept)
<span class="linenr">27: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#22238;&#27512;&#32080;&#26524;&#65306;'</span> + result)
<span class="linenr">28: </span>
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr">30: </span>  plt.scatter(years, populations, s=<span style="color: #da8548; font-weight: bold;">0.1</span>, label=<span style="color: #98be65;">'actual'</span>, color=<span style="color: #98be65;">'k'</span>)
<span class="linenr">31: </span>  plt.plot(years.tolist(), (slope * years + intercept).tolist(), label=result, color=<span style="color: #98be65;">'k'</span>)
<span class="linenr">32: </span>  plt.xlabel(<span style="color: #98be65;">'Year'</span>)
<span class="linenr">33: </span>  plt.ylabel(<span style="color: #98be65;">'Population'</span>)
<span class="linenr">34: </span>  plt.legend()
<span class="linenr">35: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show();</span>
<span class="linenr">36: </span>  plt.plot()
<span class="linenr">37: </span>  plt.savefig(<span style="color: #98be65;">"population.png"</span>)
<span class="linenr">38: </span>
<span class="linenr">39: </span>  <span style="color: #dcaeea;">x</span> = years.reshape(-<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">40: </span>  <span style="color: #dcaeea;">y</span> = populations
<span class="linenr">41: </span>
<span class="linenr">42: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21478;&#19968;&#31278;&#20316;&#27861;&#65292;&#27714;&#27161;&#28310;&#24046;&#65292;&#23565;y&#20570;&#27491;&#35215;&#21270;</span>
<span class="linenr">43: </span>  <span style="color: #dcaeea;">x_mean</span>, <span style="color: #dcaeea;">x_std</span> = torch.mean(x), torch.std(x)
<span class="linenr">44: </span>  <span style="color: #dcaeea;">x_norm</span> = (x - x_mean) / x_std
<span class="linenr">45: </span>  <span style="color: #dcaeea;">y_mean</span>, <span style="color: #dcaeea;">y_std</span> = torch.mean(y), torch.std(y)
<span class="linenr">46: </span>  <span style="color: #dcaeea;">y_norm</span> = (y - y_mean) / y_std
<span class="linenr">47: </span>
<span class="linenr">48: </span>  <span style="color: #dcaeea;">fc</span> = torch.nn.Linear(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">49: </span>  <span style="color: #dcaeea;">criterion</span> = torch.nn.MSELoss()
<span class="linenr">50: </span>  <span style="color: #dcaeea;">optimizer</span> = torch.optim.Adam(fc.parameters())
<span class="linenr">51: </span>  <span style="color: #dcaeea;">weight_norm</span>, <span style="color: #dcaeea;">bias_norm</span> = fc.parameters()
<span class="linenr">52: </span>
<span class="linenr">53: </span>  <span style="color: #51afef;">for</span> step <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">5001</span>):
<span class="linenr">54: </span>      <span style="color: #51afef;">if</span> <span style="color: #dcaeea;">step</span>:
<span class="linenr">55: </span>          fc.zero_grad()
<span class="linenr">56: </span>          loss_norm.backward()
<span class="linenr">57: </span>          optimizer.step() <span style="color: #5B6268;">#</span><span style="color: #5B6268;">adam</span>
<span class="linenr">58: </span>      output_norm = fc(x_norm)
<span class="linenr">59: </span>      <span style="color: #dcaeea;">pred_norm</span> = output_norm.squeeze()
<span class="linenr">60: </span>      <span style="color: #dcaeea;">loss_norm</span> = criterion(pred_norm, y_norm)
<span class="linenr">61: </span>      <span style="color: #dcaeea;">weight</span> = y_std / x_std * weight_norm
<span class="linenr">62: </span>      <span style="color: #dcaeea;">bias</span> = (weight_norm * (<span style="color: #da8548; font-weight: bold;">0</span> - x_mean) / x_std + bias_norm) * y_std + y_mean
<span class="linenr">63: </span>      <span style="color: #51afef;">if</span> step % <span style="color: #da8548; font-weight: bold;">1000</span> == <span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr">64: </span>          <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#31532;{}&#27493;&#65306;weight = {}, bias = {}'</span>.<span style="color: #c678dd;">format</span>(step, weight.item(), bias.item()))
<span class="linenr">65: </span>
<span class="linenr">66: </span>  <span style="color: #dcaeea;">result</span> = <span style="color: #98be65;">'population = {:.2e} * year + {:.2e}'</span>.<span style="color: #c678dd;">format</span>(weight.item(), bias.item())
<span class="linenr">67: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'&#22238;&#27512;&#32080;&#26524;&#65306;'</span> + result)
</pre>
</div>

<pre class="example">
回歸結果：population = 7.53e+07 * year + -1.45e+11
第 0 步：weight = 54825928.0, bias = -105385164800.0
第 1000 步：weight = 75289896.0, bias = -144881893376.0
第 2000 步：weight = 75291168.0, bias = -144550854656.0
第 3000 步：weight = 75291208.0, bias = -144524410880.0
第 4000 步：weight = 75291224.0, bias = -144524197888.0
第 5000 步：weight = 75291232.0, bias = -144524230656.0
回歸結果：population = 7.53e+07 * year + -1.45e+11
</pre>



<div id="orgdc340c2" class="figure">
<p><img src="images/population.png" alt="population.png" /><br />
</p>
<p><span class="figure-number">Figure 140: </span>線性迴歸分佈圖</p>
</div>
</div>
</li>
<li><a id="org1170adb"></a>Example #3: Cifar-10<br />
<div class="outline-text-4" id="text-20-4-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">ResNet-18&#23454;&#29616;Cifar-10&#22270;&#20687;&#20998;&#31867;</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> torch
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> torch.nn <span style="color: #51afef;">as</span> nn
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> torch.nn.functional <span style="color: #51afef;">as</span> F
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">ResidualBlock</span>(nn.Module):
<span class="linenr"> 7: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>, inchannel, outchannel, stride=<span style="color: #da8548; font-weight: bold;">1</span>):
<span class="linenr"> 8: </span>          <span style="color: #c678dd;">super</span>(ResidualBlock, <span style="color: #51afef;">self</span>).__init__()
<span class="linenr"> 9: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">left</span> = nn.Sequential(
<span class="linenr">10: </span>              nn.Conv2d(inchannel, outchannel, kernel_size=<span style="color: #da8548; font-weight: bold;">3</span>, stride=stride, padding=<span style="color: #da8548; font-weight: bold;">1</span>, bias=<span style="color: #a9a1e1;">False</span>),
<span class="linenr">11: </span>              nn.BatchNorm2d(outchannel),
<span class="linenr">12: </span>              nn.ReLU(inplace=<span style="color: #a9a1e1;">True</span>),
<span class="linenr">13: </span>              nn.Conv2d(outchannel, outchannel, kernel_size=<span style="color: #da8548; font-weight: bold;">3</span>, stride=<span style="color: #da8548; font-weight: bold;">1</span>, padding=<span style="color: #da8548; font-weight: bold;">1</span>, bias=<span style="color: #a9a1e1;">False</span>),
<span class="linenr">14: </span>              nn.BatchNorm2d(outchannel)
<span class="linenr">15: </span>          )
<span class="linenr">16: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">shortcut</span> = nn.Sequential() <span style="color: #5B6268;">#</span>
<span class="linenr">17: </span>          <span style="color: #51afef;">if</span> stride != <span style="color: #da8548; font-weight: bold;">1</span> <span style="color: #51afef;">or</span> inchannel != <span style="color: #dcaeea;">outchannel</span>:
<span class="linenr">18: </span>              <span style="color: #51afef;">self</span>.shortcut = nn.Sequential(
<span class="linenr">19: </span>                  nn.Conv2d(inchannel, outchannel, kernel_size=<span style="color: #da8548; font-weight: bold;">1</span>, stride=stride, bias=<span style="color: #a9a1e1;">False</span>),
<span class="linenr">20: </span>                  nn.BatchNorm2d(outchannel)
<span class="linenr">21: </span>              )
<span class="linenr">22: </span>
<span class="linenr">23: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">forward</span>(<span style="color: #51afef;">self</span>, x): <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36889;&#26159;&#19968;&#20491;block</span>
<span class="linenr">24: </span>          <span style="color: #dcaeea;">out</span> = <span style="color: #51afef;">self</span>.left(x) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32102;&#24038;&#37002;&#30340;layer&#36305;&#19968;&#27425;</span>
<span class="linenr">25: </span>          <span style="color: #dcaeea;">out</span> += <span style="color: #51afef;">self</span>.shortcut(x) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20877;&#32102;&#21491;&#37002;&#30340;&#25463;&#24465;layer&#36305;&#19968;&#27425;&#65292;&#20108;&#32773;&#30456;&#21152;</span>
<span class="linenr">26: </span>          <span style="color: #dcaeea;">out</span> = F.relu(out)
<span class="linenr">27: </span>          <span style="color: #51afef;">return</span> out
<span class="linenr">28: </span>
<span class="linenr">29: </span>  <span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">ResNet</span>(nn.Module):
<span class="linenr">30: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>, ResidualBlock, num_classes=<span style="color: #da8548; font-weight: bold;">10</span>):
<span class="linenr">31: </span>          <span style="color: #c678dd;">super</span>(ResNet, <span style="color: #51afef;">self</span>).__init__()
<span class="linenr">32: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">inchannel</span> = <span style="color: #da8548; font-weight: bold;">64</span>
<span class="linenr">33: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">conv1</span> = nn.Sequential(
<span class="linenr">34: </span>              nn.Conv2d(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">64</span>, kernel_size=<span style="color: #da8548; font-weight: bold;">3</span>, stride=<span style="color: #da8548; font-weight: bold;">1</span>, padding=<span style="color: #da8548; font-weight: bold;">1</span>, bias=<span style="color: #a9a1e1;">False</span>),
<span class="linenr">35: </span>              nn.BatchNorm2d(<span style="color: #da8548; font-weight: bold;">64</span>),
<span class="linenr">36: </span>              nn.ReLU(),
<span class="linenr">37: </span>          )
<span class="linenr">38: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">layer1</span> = <span style="color: #51afef;">self</span>.make_layer(ResidualBlock, <span style="color: #da8548; font-weight: bold;">64</span>,  <span style="color: #da8548; font-weight: bold;">2</span>, stride=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">39: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">layer2</span> = <span style="color: #51afef;">self</span>.make_layer(ResidualBlock, <span style="color: #da8548; font-weight: bold;">128</span>, <span style="color: #da8548; font-weight: bold;">2</span>, stride=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">40: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">layer3</span> = <span style="color: #51afef;">self</span>.make_layer(ResidualBlock, <span style="color: #da8548; font-weight: bold;">256</span>, <span style="color: #da8548; font-weight: bold;">2</span>, stride=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">41: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">layer4</span> = <span style="color: #51afef;">self</span>.make_layer(ResidualBlock, <span style="color: #da8548; font-weight: bold;">512</span>, <span style="color: #da8548; font-weight: bold;">2</span>, stride=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">42: </span>          <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">fc</span> = nn.Linear(<span style="color: #da8548; font-weight: bold;">512</span>, num_classes)
<span class="linenr">43: </span>
<span class="linenr">44: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">make_layer</span>(<span style="color: #51afef;">self</span>, block, channels, num_blocks, stride):
<span class="linenr">45: </span>          <span style="color: #dcaeea;">strides</span> = [stride] + [<span style="color: #da8548; font-weight: bold;">1</span>] * (num_blocks - <span style="color: #da8548; font-weight: bold;">1</span>)   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">strides=[1,1]</span>
<span class="linenr">46: </span>          <span style="color: #dcaeea;">layers</span> = []
<span class="linenr">47: </span>          <span style="color: #51afef;">for</span> stride <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">strides</span>:
<span class="linenr">48: </span>              layers.append(block(<span style="color: #51afef;">self</span>.inchannel, channels, stride))
<span class="linenr">49: </span>              <span style="color: #51afef;">self</span>.inchannel = channels
<span class="linenr">50: </span>          <span style="color: #51afef;">return</span> nn.Sequential(*layers)
<span class="linenr">51: </span>
<span class="linenr">52: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">forward</span>(<span style="color: #51afef;">self</span>, x):
<span class="linenr">53: </span>          <span style="color: #dcaeea;">out</span> = <span style="color: #51afef;">self</span>.conv1(x)
<span class="linenr">54: </span>          <span style="color: #dcaeea;">out</span> = <span style="color: #51afef;">self</span>.layer1(out)
<span class="linenr">55: </span>          <span style="color: #dcaeea;">out</span> = <span style="color: #51afef;">self</span>.layer2(out)
<span class="linenr">56: </span>          <span style="color: #dcaeea;">out</span> = <span style="color: #51afef;">self</span>.layer3(out)
<span class="linenr">57: </span>          <span style="color: #dcaeea;">out</span> = <span style="color: #51afef;">self</span>.layer4(out)
<span class="linenr">58: </span>          <span style="color: #dcaeea;">out</span> = F.avg_pool2d(out, <span style="color: #da8548; font-weight: bold;">4</span>)
<span class="linenr">59: </span>          <span style="color: #dcaeea;">out</span> = out.view(out.size(<span style="color: #da8548; font-weight: bold;">0</span>), -<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">60: </span>          <span style="color: #dcaeea;">out</span> = <span style="color: #51afef;">self</span>.fc(out)
<span class="linenr">61: </span>          <span style="color: #51afef;">return</span> out
<span class="linenr">62: </span>
<span class="linenr">63: </span>
<span class="linenr">64: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">ResNet18</span>():
<span class="linenr">65: </span>      <span style="color: #51afef;">return</span> ResNet(ResidualBlock)
<span class="linenr">66: </span>
</pre>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-orged720b2" class="outline-3">
<h3 id="orged720b2"><span class="section-number-3">20.5</span> PyTorch 基本運算</h3>
<div class="outline-text-3" id="text-20-5">
</div>
<ol class="org-ol">
<li><a id="orgef2925e"></a>使用 tensor 建構一個未初始化的矩陣<br />
<div class="outline-text-4" id="text-20-5-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> sys
<span class="linenr">2: </span><span style="color: #51afef;">print</span>(sys.version)
<span class="linenr">3: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">4: </span><span style="color: #dcaeea;">x</span> = torch.empty(<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">5: </span><span style="color: #51afef;">print</span>(x)
</pre>
</div>

<pre class="example">
3.7.4 (default, Sep  7 2019, 18:27:02) 
[Clang 10.0.1 (clang-1001.0.46.4)]
tensor([[4.6894e+27, 7.9463e+08, 3.2604e-12],
        [1.7743e+28, 2.0535e-19, 5.9682e-02],
        [7.0374e+22, 3.8946e+21, 4.4650e+30],
        [7.0975e+22, 7.9309e+34, 7.9439e+08],
        [3.2604e-12, 7.3113e+34, 2.0706e-19]])
</pre>
</div>
</li>

<li><a id="org60fa2b7"></a>建構一個亂數的矩陣<br />
<div class="outline-text-4" id="text-20-5-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> sys
<span class="linenr">2: </span><span style="color: #51afef;">print</span>(sys.version)
<span class="linenr">3: </span><span style="color: #51afef;">import</span> torch
<span class="linenr">4: </span><span style="color: #dcaeea;">x</span> = torch.empty(<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">5: </span><span style="color: #51afef;">print</span>(x)
</pre>
</div>

<pre class="example">
3.7.4 (default, Jul  9 2019, 18:13:23) 
[Clang 10.0.1 (clang-1001.0.46.4)]
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.1704e-41],
        [ 0.0000e+00,  2.2369e+08,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],
        [        nan,         nan, -7.0351e+02]])
</pre>


<div class="org-src-container">
<pre class="src src-latex"><span style="color: #ECBE7B;">\newpage</span>
</pre>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-org21a19be" class="outline-3">
<h3 id="org21a19be"><span class="section-number-3">20.6</span> PyTorch 自動求導(autograd)機制</h3>
<div class="outline-text-3" id="text-20-6">
</div>
<ol class="org-ol">
<li><a id="org9b3f747"></a>Autograd: 自動微分<br />
<div class="outline-text-4" id="text-20-6-1">
<p>
autograd 包是 PyTorch 所有神經網路的核心，為 Tensors 上的所有操作提供了自動區分。同時，它也是一個逐個執行的框架，意味著 backprop 由程式碼執行定義，每一次迭代都可以不同。<br />
</p>
</div>
</li>

<li><a id="org1ff3020"></a>autograd.Variable<br />
<div class="outline-text-4" id="text-20-6-2">
<p>
autograd.Variable 是 torch.autograd 中很重要的 class。它用來包裝 Tensor，將 Tensor 轉換為 Variable 之後，可以裝載梯度信息。autograd.Variable 包含一個張量，並支援幾乎所有定義的操作，在完成計算後，呼叫.backward()並自動計算所有梯度。可以通過.data 屬性訪問原始張量，而將此變數的梯度累加到.grad。<br />
如圖<a href="#org39580d8">141</a>，data 負責儲存 tensor 數據，grad 屬性儲存關於該變數的導數。<br />
</p>
<div class="org-src-container">
<pre class="src src-ditaa">         +---------------------+   
         |  autograd.Variable  |
         |  +------+  +------+ |
 tensor----&gt;| data |  | grad | |
         |  +------+  +------+ |
         |    +-----------+    |
         |    |  grad_fn  |    |
         |    +-----------+    |
         +---------------------+
</pre>
</div>

<div id="org39580d8" class="figure">
<p><img src="images/autogradVariable.png" alt="autogradVariable.png" width="300" /><br />
</p>
<p><span class="figure-number">Figure 141: </span>autograd.Variable</p>
</div>

<p>
pytorch 的一個重要特點就是動態計算圖（Dynamic Computational Graphs）。計算圖中每一個節點代表一個變量，變量間建立運算關係並且可以修改，而不像 Tensorflow 中的計算圖是固定不可變的。<br />
</p>

<p>
Variable 用來構建一個計算圖中的節點。將 Tensor 轉換為 Variabla 類型之後，該 Tensor 就成了計算圖中的一個節點。對於該節點，有兩個重要的特性：<br />
</p>

<ul class="org-ul">
<li>.data——獲得該節點的值，即 Tensor 類型的值<br /></li>
<li>.grad——獲得該節點處的梯度信息<br /></li>
</ul>

<p>
關於 Variable 的參數之一「requires_grad」和特性之一「grad_fn」有要注意的地方，都和該變量是否是人自己創建的有關：<br />
</p>

<ol class="org-ol">
<li>requires_grad 有兩個值：True 和 False，True 代表此變量處需要計算梯度，False 代表不需要。變量的「requires_grad」值是 Variable 的一個參數，在建立 Variable 的時候就已經設定好，默認是 False。<br /></li>
<li>grad_fn 的值可以得知該變量是否是一個計算結果，也就是說該變量是不是一個函數的輸出值。若是，則 grad_fn 返回一個與該函數相關的對象，否則是 None。<br /></li>
</ol>
</div>
</li>

<li><a id="orgd141973"></a>Example<br />
<div class="outline-text-4" id="text-20-6-3">
<p>
自動求導是 PyTorch 中非常重要的特性，可自動計算複雜的導數。以下式為例：<br />
</p>
\begin{equation}
\label{org6305017}
y = x + 2 \\
z = y^2 + 3 \\
\frac{\partial Z}{\partial X} = \frac{\partial Z}{\partial Y} \frac{\partial Y}{\partial X} 
\end{equation}

<p>
公式\eqref{org6305017}相當於\(z=(x+2)^2+3\)之\(x\)，也就是說，我們有一個向量 x，在經過一系列的運算後，會得到變量 z，現在，我們想要求 z 關於 x 的導數，此時可以透過 PyTorch 來算動求解。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> torch
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> torch.autograd <span style="color: #51afef;">import</span> Variable
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">x</span> = Variable(torch.Tensor([<span style="color: #da8548; font-weight: bold;">2</span>]), requires_grad=<span style="color: #a9a1e1;">True</span>)
<span class="linenr"> 5: </span><span style="color: #dcaeea;">y</span> = x + <span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">z</span> = y**<span style="color: #da8548; font-weight: bold;">2</span> + <span style="color: #da8548; font-weight: bold;">3</span>
<span class="linenr"> 7: </span><span style="color: #51afef;">print</span>(z)
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#33258;&#21205;&#27714;&#23566;</span>
<span class="linenr">10: </span>z.backward()
<span class="linenr">11: </span><span style="color: #51afef;">print</span>(x.grad)
</pre>
</div>

<pre class="example">
tensor([19.], grad_fn=&lt;AddBackward0&gt;)
tensor([8.])
</pre>


<p>
其中，x.grad 會傳回 z 關於 x 的向量梯度。計算結果與手動計算相同，即，\( \frac{\partial Z}{\partial X} = 2(x+2) = 2(2+2) = 8 \)<br />
</p>
</div>
</li>

<li><a id="org5009d70"></a>PyTorch 的梯度計算<sup><a id="fnr.23" class="footref" href="#fn.23">23</a></sup><br />
<div class="outline-text-4" id="text-20-6-4">
<p>
pytorch 的一個重要特點就是動態計算圖（Dynamic Computational Graphs）。計算圖中每一個節點代表一個變量，變量間建立運算關係並且可以修改，而不像 Tensorflow 中的計算圖是固定不可變的。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> torch
<span class="linenr">2: </span>  <span style="color: #51afef;">from</span> torch.autograd <span style="color: #51afef;">import</span> Variable
<span class="linenr">3: </span>  <span style="color: #dcaeea;">x</span>=Variable(torch.randn(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr">4: </span>  <span style="color: #dcaeea;">y</span>=Variable(torch.randn(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr">5: </span>  <span style="color: #dcaeea;">z</span>=Variable(torch.randn(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">2</span>),requires_grad=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">6: </span>  <span style="color: #dcaeea;">a</span>=x+y
<span class="linenr">7: </span>  <span style="color: #dcaeea;">b</span>=a+z
<span class="linenr">8: </span>  <span style="color: #51afef;">print</span>(b)
</pre>
</div>

<pre class="example">
tensor([[ 3.1240,  0.7868],
        [-0.9371, -2.5228]], grad_fn=&lt;AddBackward0&gt;)
</pre>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-org2776df8" class="outline-2">
<h2 id="org2776df8"><span class="section-number-2">21</span> Scikit-learn: 分類問題</h2>
<div class="outline-text-2" id="text-21">
</div>
<div id="outline-container-org4815b0c" class="outline-3">
<h3 id="org4815b0c"><span class="section-number-3">21.1</span> 簡介</h3>
<div class="outline-text-3" id="text-21-1">
<p>
scikit-learn，又寫作 sklearn，是一個開源的基於 python 語言的機器學習工具包。它通過 NumPy, SciPy 和 Matplotlib 等 python 數值計算的庫實現高效的算法應用，並且涵蓋了幾乎所有主流機器學習算法<sup><a id="fnr.24" class="footref" href="#fn.24">24</a></sup>。<br />
</p>

<p>
工程應用中，用 python 手寫代碼來從頭實現一個算法的可能性非常低，這樣不僅耗時耗力，還不一定能夠寫出構架清晰，穩定性強的模型。更多情況下，是分析採集到的數據，根據數據特徵選擇適合的算法，在工具包中調用算法，調整算法的參數，獲取需要的信息，從而實現算法效率和效果之間的平衡。而 sklearn，正是這樣一個可以幫助我們高效實現算法應用的工具包。<br />
</p>

<p>
sklearn 有一個完整而豐富的官網，裡面講解了基於 sklearn 對所有算法的實現和簡單應用。<br />
</p>
</div>
</div>

<div id="outline-container-org091ee63" class="outline-3">
<h3 id="org091ee63"><span class="section-number-3">21.2</span> 常用模組</h3>
<div class="outline-text-3" id="text-21-2">
<p>
sklearn 中常用的模塊有分類、回歸、聚類、降維、模型選擇、預處理。<br />
</p>
<ul class="org-ul">
<li>分類：識別某個對象屬於哪個類別，常用的算法有：SVM（支持向量機）、nearest neighbors（最近鄰）、random forest（隨機森林），常見的應用有：垃圾郵件識別、圖像識別。<br /></li>
<li>回歸：預測與對象相關聯的連續值屬性，常見的算法有：SVR（支持向量機）、 ridge regression（嶺回歸）、Lasso，常見的應用有：藥物反應，預測股價。<br /></li>
<li>聚類：將相似對象自動分組，常用的算法有：k-Means、 spectral clustering、mean-shift，常見的應用有：客戶細分，分組實驗結果。<br /></li>
<li>降維：減少要考慮的隨機變量的數量，常見的算法有：PCA（主成分分析）、feature selection（特徵選擇）、non-negative matrix factorization（非負矩陣分解），常見的應用有：可視化，提高效率。<br /></li>
<li>模型選擇：比較，驗證，選擇參數和模型，常用的模塊有：grid search（網格搜索）、cross validation（交叉驗證）、 metrics（度量）。它的目標是通過參數調整提高精度。<br /></li>
<li>預處理：特徵提取和歸一化，常用的模塊有：preprocessing，feature extraction，常見的應用有：把輸入數據（如文本）轉換為機器學習算法可用的數據。<br /></li>
</ul>
</div>
</div>

<div id="outline-container-org63829bb" class="outline-3">
<h3 id="org63829bb"><span class="section-number-3">21.3</span> 分類：識別某個對象屬於哪個類別</h3>
<div class="outline-text-3" id="text-21-3">
<p>
解決分類問題要先理解：每個演算法都是基於某些特定前提之下所開發，各有優缺點，故，沒有任何一個分類器可以面對所有情境均能取得最佳解，以下以一資料集(鳶尾花)為例，取出其中兩種屬性，利用不同的分類法進行分類，包括：perception, logistic, SVM, decision tree, k-nearest。<br />
</p>
</div>
</div>

<div id="outline-container-orgf41e2af" class="outline-3">
<h3 id="orgf41e2af"><span class="section-number-3">21.4</span> 使用 scikit-learn: 感知器演算法</h3>
<div class="outline-text-3" id="text-21-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr">  2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  3: </span>
<span class="linenr">  4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr">  5: </span>  <span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span class="linenr">  6: </span>  <span style="color: #dcaeea;">X</span> = iris.data[:, [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>]]
<span class="linenr">  7: </span>  <span style="color: #dcaeea;">y</span> = iris.target
<span class="linenr">  8: </span>
<span class="linenr">  9: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Class labels:'</span>, np.unique(y))
<span class="linenr"> 10: </span>
<span class="linenr"> 11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38928;&#34389;&#29702;&#65306;&#23559;&#36039;&#26009;&#20998;&#28858;&#35347;&#32244;&#21644;&#28204;&#35430;</span>
<span class="linenr"> 12: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">stratify=y, &#34920;&#31034;&#26371;&#23559;&#36039;&#26009;&#20381;test_size&#27604;&#20363;&#22238;&#20659;</span>
<span class="linenr"> 13: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr"> 14: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(
<span class="linenr"> 15: </span>      X, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>, stratify=y)
<span class="linenr"> 16: </span>
<span class="linenr"> 17: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y:'</span>, np.bincount(y))
<span class="linenr"> 18: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y_train:'</span>, np.bincount(y_train))
<span class="linenr"> 19: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y_test:'</span>, np.bincount(y_test))
<span class="linenr"> 20: </span>
<span class="linenr"> 21: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#36039;&#26009;&#27161;&#28310;&#21270;: &#21033;&#29992;preprocessing&#27169;&#32068;&#35041;&#30340;StandardScaler&#39006;&#21029;</span>
<span class="linenr"> 22: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr"> 23: </span>  sc = StandardScaler() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23526;&#20363;&#21270;&#19968;&#20491;StandardScaler&#29289;&#20214;</span>
<span class="linenr"> 24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span class="linenr"> 25: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span class="linenr"> 26: </span>  sc.fit(X_train) 
<span class="linenr"> 27: </span>  <span style="color: #dcaeea;">X_train_std</span> = sc.transform(X_train)
<span class="linenr"> 28: </span>  <span style="color: #dcaeea;">X_test_std</span> = sc.transform(X_test)
<span class="linenr"> 29: </span>
<span class="linenr"> 30: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr"> 31: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr"> 32: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 33: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, test_idx=<span style="color: #a9a1e1;">None</span>, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr"> 34: </span>
<span class="linenr"> 35: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr"> 36: </span>      <span style="color: #dcaeea;">markers</span> = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr"> 37: </span>      <span style="color: #dcaeea;">colors</span> = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr"> 38: </span>      <span style="color: #dcaeea;">cmap</span> = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr"> 39: </span>
<span class="linenr"> 40: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr"> 41: </span>      <span style="color: #dcaeea;">x1_min</span>, <span style="color: #dcaeea;">x1_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 42: </span>      <span style="color: #dcaeea;">x2_min</span>, <span style="color: #dcaeea;">x2_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 43: </span>      <span style="color: #dcaeea;">xx1</span>, <span style="color: #dcaeea;">xx2</span> = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr"> 44: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr"> 45: </span>      <span style="color: #dcaeea;">Z</span> = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr"> 46: </span>      <span style="color: #dcaeea;">Z</span> = Z.reshape(xx1.shape)
<span class="linenr"> 47: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr"> 48: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr"> 49: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr"> 50: </span>
<span class="linenr"> 51: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr"> 52: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>], 
<span class="linenr"> 53: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr"> 54: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>, 
<span class="linenr"> 55: </span>                      c=colors[idx],
<span class="linenr"> 56: </span>                      marker=markers[idx], 
<span class="linenr"> 57: </span>                      label=cl, 
<span class="linenr"> 58: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr"> 59: </span>
<span class="linenr"> 60: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">highlight test samples</span>
<span class="linenr"> 61: </span>      <span style="color: #51afef;">if</span> <span style="color: #dcaeea;">test_idx</span>:
<span class="linenr"> 62: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot all samples</span>
<span class="linenr"> 63: </span>          <span style="color: #dcaeea;">X_test</span>, y_test = X[test_idx, :], y[test_idx]
<span class="linenr"> 64: </span>
<span class="linenr"> 65: </span>          plt.scatter(X_test[:, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr"> 66: </span>                      X_test[:, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr"> 67: </span>                      c=<span style="color: #98be65;">''</span>,
<span class="linenr"> 68: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr"> 69: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr"> 70: </span>                      linewidth=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr"> 71: </span>                      marker=<span style="color: #98be65;">'o'</span>,
<span class="linenr"> 72: </span>                      s=<span style="color: #da8548; font-weight: bold;">100</span>, 
<span class="linenr"> 73: </span>                      label=<span style="color: #98be65;">'test set'</span>)
<span class="linenr"> 74: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr"> 75: </span>
<span class="linenr"> 76: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;perceptron model</span>
<span class="linenr"> 77: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24478;linear_model&#27169;&#32068;&#20013;&#36617;&#20837;Perceptron&#39006;&#21029;,&#20197;fit&#36914;&#34892;&#35347;&#32244;</span>
<span class="linenr"> 78: </span>  <span style="color: #51afef;">from</span> sklearn.linear_model <span style="color: #51afef;">import</span> Perceptron
<span class="linenr"> 79: </span>  <span style="color: #dcaeea;">ppn</span> = Perceptron(n_iter_no_change=<span style="color: #da8548; font-weight: bold;">40</span>, eta0=<span style="color: #da8548; font-weight: bold;">0.01</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 80: </span>  ppn.fit(X_train_std, y_train)
<span class="linenr"> 81: </span>
<span class="linenr"> 82: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36914;&#34892;&#38928;&#28204;</span>
<span class="linenr"> 83: </span>  <span style="color: #dcaeea;">y_pred</span> = ppn.predict(X_test_std)
<span class="linenr"> 84: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Misclassified samples: %d'</span> % (y_test != y_pred).<span style="color: #c678dd;">sum</span>())
<span class="linenr"> 85: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))</span>
<span class="linenr"> 86: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Accuracy: %.2f'</span> % ppn.score(X_test_std, y_test))
<span class="linenr"> 87: </span>
<span class="linenr"> 88: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Training a perceptron model using the standardized training data:</span>
<span class="linenr"> 89: </span>  X_combined_std = np.vstack((X_train_std, X_test_std))
<span class="linenr"> 90: </span>  <span style="color: #dcaeea;">y_combined</span> = np.hstack((y_train, y_test))
<span class="linenr"> 91: </span>
<span class="linenr"> 92: </span>  plot_decision_regions(X=X_combined_std, y=y_combined,
<span class="linenr"> 93: </span>                        classifier=ppn, test_idx=<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">105</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr"> 94: </span>  plt.xlabel(<span style="color: #98be65;">'petal length [standardized]'</span>)
<span class="linenr"> 95: </span>  plt.ylabel(<span style="color: #98be65;">'petal width [standardized]'</span>)
<span class="linenr"> 96: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr"> 97: </span>
<span class="linenr"> 98: </span>  plt.tight_layout()
<span class="linenr"> 99: </span>  plt.savefig(<span style="color: #98be65;">'03_01.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">100: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
</pre>
</div>

<pre class="example">
Class labels: [0 1 2]
Labels counts in y: [50 50 50]
Labels counts in y_train: [35 35 35]
Labels counts in y_test: [15 15 15]
Misclassified samples: 3
Accuracy: 0.93
</pre>


<p>
要找出一個合適的學習速率，需要一些實務經驗。如果學習速率過大，演算法會衝過「全域最小值」；如果學習速率過小，則演算法就要迭代很多很多次才會收歛。在上例中，學習速率(eta)在設為 0.1 時，分類準確率為 0.82；而當設為 0.01 時，分類準確率為 0.93。<br />
</p>

<p>
感知器演算法遇到無法完美「線性分類」的數據時，會導致無法收歛，如圖<a href="#org614d6d1">142</a>。<br />
</p>


<div id="org614d6d1" class="figure">
<p><img src="images/03_01.png" alt="03_01.png" /><br />
</p>
<p><span class="figure-number">Figure 142: </span>線性模式的限制：無法完美分類三種花</p>
</div>
</div>
</div>

<div id="outline-container-org67abd7f" class="outline-3">
<h3 id="org67abd7f"><span class="section-number-3">21.5</span> 使用 scikit-learn: 邏輯斯迴歸</h3>
<div class="outline-text-3" id="text-21-5">
<p>
Logistic regression 簡稱 logreg，雖然其中譯為邏輯斯迴歸，但與邏輯無關，也和迴歸沒關係，它本質上是一種分類演算法，常被視為現代機器學習的&ldquo;Hello world.&rdquo;。<br />
</p>

<p>
當要處理的數據並非「線性可分」，則在每一輪的迭代中，總會有一些樣本會被錯誤分類，於是加權就會不斷的被更新，對於這種數據，感知器絕對不會收歛。<br />
</p>

<p>
要理解 logistic regression, 需先理解「勝算比」(odds ratio)，勝算比可以\( \frac{p}{1-p} \)，其中\(p\)為「正事件」(positive event)的機率，此一詞並不一定意味著好，只是指我們想要預測的事件。例如，某個病患且具有某種疾病的機率；我們可以將「正事件」視為類別標籤 y=1，然後，我們可以進一步定義 logit 函數，稱為「對數勝算」(log-odds)：<br />
\[logit(p) = log\frac{p}{1-p}\] <br />
函數\(logit\)的輸入為 0 到 1 間的值，它會將其轉換為分佈於整個實數範圍內的值，我們可以用它來表達「特徵值」和「勝算比」間的線性關係：<br />
\[logit(p(y=1|x))=w_0x_0+w_1x_1+...+w_mx_m=\sum_{i=1}^{m}=w^tx\]<br />
在此\(p(y=1|x)\)的意思是：給定特徵值 x，當某特定樣本屬於類別 1 的條件機率。<br />
</p>

<p>
實際上，我們真正感興趣的是：預測某特定樣本屬於特定類別的機率，也就是 logit 函數的反函數，又稱為 logistic 函數。又因為其特殊的 S 形曲線(如圖<a href="#org1692dc0">14</a>)，故有時也被稱為 sigmoid 函數。<br />
</p>



<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr">  2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  3: </span>
<span class="linenr">  4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr">  5: </span>  <span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span class="linenr">  6: </span>  <span style="color: #dcaeea;">X</span> = iris.data[:, [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>]]
<span class="linenr">  7: </span>  <span style="color: #dcaeea;">y</span> = iris.target
<span class="linenr">  8: </span>
<span class="linenr">  9: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Class labels:'</span>, np.unique(y))
<span class="linenr"> 10: </span>
<span class="linenr"> 11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">stratify=y, &#34920;&#31034;&#26371;&#23559;&#36039;&#26009;&#20381;test_size&#27604;&#20363;&#22238;&#20659;</span>
<span class="linenr"> 12: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr"> 13: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(
<span class="linenr"> 14: </span>      X, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>, stratify=y)
<span class="linenr"> 15: </span>
<span class="linenr"> 16: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y:'</span>, np.bincount(y))
<span class="linenr"> 17: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y_train:'</span>, np.bincount(y_train))
<span class="linenr"> 18: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y_test:'</span>, np.bincount(y_test))
<span class="linenr"> 19: </span>
<span class="linenr"> 20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#36039;&#26009;&#27161;&#28310;&#21270;: &#21033;&#29992;preprocessing&#27169;&#32068;&#35041;&#30340;StandardScaler&#39006;&#21029;</span>
<span class="linenr"> 21: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr"> 22: </span>  sc = StandardScaler() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23526;&#20363;&#21270;&#19968;&#20491;StandardScaler&#29289;&#20214;</span>
<span class="linenr"> 23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span class="linenr"> 24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span class="linenr"> 25: </span>  sc.fit(X_train) 
<span class="linenr"> 26: </span>  <span style="color: #dcaeea;">X_train_std</span> = sc.transform(X_train)
<span class="linenr"> 27: </span>  <span style="color: #dcaeea;">X_test_std</span> = sc.transform(X_test)
<span class="linenr"> 28: </span>
<span class="linenr"> 29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr"> 30: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr"> 31: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 32: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, test_idx=<span style="color: #a9a1e1;">None</span>, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr"> 33: </span>
<span class="linenr"> 34: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr"> 35: </span>      <span style="color: #dcaeea;">markers</span> = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr"> 36: </span>      <span style="color: #dcaeea;">colors</span> = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr"> 37: </span>      <span style="color: #dcaeea;">cmap</span> = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr"> 38: </span>
<span class="linenr"> 39: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr"> 40: </span>      <span style="color: #dcaeea;">x1_min</span>, <span style="color: #dcaeea;">x1_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 41: </span>      <span style="color: #dcaeea;">x2_min</span>, <span style="color: #dcaeea;">x2_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 42: </span>      <span style="color: #dcaeea;">xx1</span>, <span style="color: #dcaeea;">xx2</span> = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr"> 43: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr"> 44: </span>      <span style="color: #dcaeea;">Z</span> = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr"> 45: </span>      <span style="color: #dcaeea;">Z</span> = Z.reshape(xx1.shape)
<span class="linenr"> 46: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr"> 47: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr"> 48: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr"> 49: </span>
<span class="linenr"> 50: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr"> 51: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>], 
<span class="linenr"> 52: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr"> 53: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>, 
<span class="linenr"> 54: </span>                      c=colors[idx],
<span class="linenr"> 55: </span>                      marker=markers[idx], 
<span class="linenr"> 56: </span>                      label=cl, 
<span class="linenr"> 57: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr"> 58: </span>
<span class="linenr"> 59: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">highlight test samples</span>
<span class="linenr"> 60: </span>      <span style="color: #51afef;">if</span> <span style="color: #dcaeea;">test_idx</span>:
<span class="linenr"> 61: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot all samples</span>
<span class="linenr"> 62: </span>          <span style="color: #dcaeea;">X_test</span>, y_test = X[test_idx, :], y[test_idx]
<span class="linenr"> 63: </span>
<span class="linenr"> 64: </span>          plt.scatter(X_test[:, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr"> 65: </span>                      X_test[:, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr"> 66: </span>                      c=<span style="color: #98be65;">''</span>,
<span class="linenr"> 67: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr"> 68: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr"> 69: </span>                      linewidth=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr"> 70: </span>                      marker=<span style="color: #98be65;">'o'</span>,
<span class="linenr"> 71: </span>                      s=<span style="color: #da8548; font-weight: bold;">100</span>, 
<span class="linenr"> 72: </span>                      label=<span style="color: #98be65;">'test set'</span>)
<span class="linenr"> 73: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr"> 74: </span>
<span class="linenr"> 75: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;scikit-learn&#35347;&#32244;&#19968;&#20491;logistic regression model</span>
<span class="linenr"> 76: </span>  <span style="color: #51afef;">from</span> sklearn.linear_model <span style="color: #51afef;">import</span> LogisticRegression
<span class="linenr"> 77: </span>  <span style="color: #dcaeea;">lr</span> = LogisticRegression(C=<span style="color: #da8548; font-weight: bold;">100.0</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 78: </span>  lr.fit(X_train_std, y_train)
<span class="linenr"> 79: </span>
<span class="linenr"> 80: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Training a perceptron model using the standardized training data:</span>
<span class="linenr"> 81: </span>  X_combined_std = np.vstack((X_train_std, X_test_std))
<span class="linenr"> 82: </span>  <span style="color: #dcaeea;">y_combined</span> = np.hstack((y_train, y_test))
<span class="linenr"> 83: </span>
<span class="linenr"> 84: </span>  plot_decision_regions(X_combined_std, y_combined,
<span class="linenr"> 85: </span>                        classifier=lr,
<span class="linenr"> 86: </span>                        test_idx=<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">105</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr"> 87: </span>
<span class="linenr"> 88: </span>  plt.xlabel(<span style="color: #98be65;">'petal length [standardized]'</span>)
<span class="linenr"> 89: </span>  plt.ylabel(<span style="color: #98be65;">'petal width [standardized]'</span>)
<span class="linenr"> 90: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr"> 91: </span>
<span class="linenr"> 92: </span>  plt.tight_layout()
<span class="linenr"> 93: </span>  plt.savefig(<span style="color: #98be65;">'03_06.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr"> 94: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr"> 95: </span>
<span class="linenr"> 96: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36914;&#34892;&#38928;&#28204;</span>
<span class="linenr"> 97: </span>  <span style="color: #51afef;">print</span>(lr.predict_proba(X_test_std[:<span style="color: #da8548; font-weight: bold;">3</span>, :]))
<span class="linenr"> 98: </span>  <span style="color: #51afef;">print</span>(lr.predict_proba(X_test_std[:<span style="color: #da8548; font-weight: bold;">3</span>, :]).<span style="color: #c678dd;">sum</span>(axis=<span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr"> 99: </span>  <span style="color: #51afef;">print</span>(lr.predict_proba(X_test_std[:<span style="color: #da8548; font-weight: bold;">3</span>, :]).argmax(axis=<span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr">100: </span>  <span style="color: #51afef;">print</span>(lr.predict(X_test_std[:<span style="color: #da8548; font-weight: bold;">3</span>, :]))
<span class="linenr">101: </span>  <span style="color: #51afef;">print</span>(lr.predict(X_test_std[<span style="color: #da8548; font-weight: bold;">0</span>, :].reshape(<span style="color: #da8548; font-weight: bold;">1</span>, -<span style="color: #da8548; font-weight: bold;">1</span>)))
<span class="linenr">102: </span>
</pre>
</div>

<pre class="example">
Class labels: [0 1 2]
Labels counts in y: [50 50 50]
Labels counts in y_train: [35 35 35]
Labels counts in y_test: [15 15 15]
[[3.17983737e-08 1.44886616e-01 8.55113353e-01]
 [8.33962295e-01 1.66037705e-01 4.55557009e-12]
 [8.48762934e-01 1.51237066e-01 4.63166788e-13]]
[1. 1. 1.]
[2 0 0]
[2 0 0]
[2]
</pre>


<div id="org592e87d" class="figure">
<p><img src="images/03_06.png" alt="03_06.png" /><br />
</p>
<p><span class="figure-number">Figure 143: </span>Logistic Regression: 鳶尾花分類</p>
</div>

<p>
lr.predict_proba(X_test_std[:3, :]))列出一個二維陣列，各列中的最高值均代表預測的答案，故，第一列預測答案為第一類花的機率最高(0.855)；也可以通過找出每列中最大的行值來取得預測的類別標籤：lr.predict_proba(X_test_std[:3, :]).argmax(axis=1))。<br />
</p>
</div>
</div>

<div id="outline-container-orga5d3ee0" class="outline-3">
<h3 id="orga5d3ee0"><span class="section-number-3">21.6</span> 使用 SVM</h3>
<div class="outline-text-3" id="text-21-6">
<p>
SVM 的目標是在兩種類別(class)的資料點(data points)間找到最佳決策邊界(decision boundaries)，SVM 透過以下兩個步驟來找到決策邊介：<br />
</p>
<ol class="org-ol">
<li>把資料映射到一個高維表示法，在此空間屬找最佳決策邊界，一般這個邊人田中中為一超曲面。至於為何要將資料映射到高維度空，可由圖<a href="#org017b1f5">144</a>看出其原理，圖左的二維資料點原本很難找到有效的區分策略，但若將其映射到 3 維空間，這些資料點很有可能會變成圖右的狀況，此時就能透過一個平面將兩類資料點做有效的分割。<br /></li>
</ol>


<div id="org017b1f5" class="figure">
<p><img src="images/DecisionSurface-1.jpg" alt="DecisionSurface-1.jpg" /><br />
</p>
<p><span class="figure-number">Figure 144: </span>SVM 映射高維空間</p>
</div>

<ol class="org-ol">
<li>在找超曲面時，超曲面要位於兩類倶料點之間的中線。這步驟稱為最大化邊界(maximizing the margin)，也就是讓曲面分別和兩類資料點保有最大距離，如圖<a href="#orgb33f111">145</a>。<br /></li>
</ol>


<div id="orgb33f111" class="figure">
<p><img src="images/svm-max.jpg" alt="svm-max.jpg" /><br />
</p>
<p><span class="figure-number">Figure 145: </span>SVM</p>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">X</span> = iris.data[:, [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>]]
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">y</span> = iris.target
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Class labels:'</span>, np.unique(y))
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">stratify=y, &#34920;&#31034;&#26371;&#23559;&#36039;&#26009;&#20381;test_size&#27604;&#20363;&#22238;&#20659;</span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">13: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(
<span class="linenr">14: </span>      X, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>, stratify=y)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y:'</span>, np.bincount(y))
<span class="linenr">17: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y_train:'</span>, np.bincount(y_train))
<span class="linenr">18: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y_test:'</span>, np.bincount(y_test))
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#36039;&#26009;&#27161;&#28310;&#21270;: &#21033;&#29992;preprocessing&#27169;&#32068;&#35041;&#30340;StandardScaler&#39006;&#21029;</span>
<span class="linenr">21: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr">22: </span>  sc = StandardScaler() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23526;&#20363;&#21270;&#19968;&#20491;StandardScaler&#29289;&#20214;</span>
<span class="linenr">23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span class="linenr">24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span class="linenr">25: </span>  sc.fit(X_train) 
<span class="linenr">26: </span>  <span style="color: #dcaeea;">X_train_std</span> = sc.transform(X_train)
<span class="linenr">27: </span>  <span style="color: #dcaeea;">X_test_std</span> = sc.transform(X_test)
<span class="linenr">28: </span>
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">30: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr">31: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">32: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, test_idx=<span style="color: #a9a1e1;">None</span>, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr">33: </span>
<span class="linenr">34: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr">35: </span>      <span style="color: #dcaeea;">markers</span> = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr">36: </span>      <span style="color: #dcaeea;">colors</span> = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr">37: </span>      <span style="color: #dcaeea;">cmap</span> = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr">38: </span>
<span class="linenr">39: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr">40: </span>      <span style="color: #dcaeea;">x1_min</span>, <span style="color: #dcaeea;">x1_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">41: </span>      <span style="color: #dcaeea;">x2_min</span>, <span style="color: #dcaeea;">x2_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">42: </span>      <span style="color: #dcaeea;">xx1</span>, <span style="color: #dcaeea;">xx2</span> = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr">43: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr">44: </span>      <span style="color: #dcaeea;">Z</span> = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr">45: </span>      <span style="color: #dcaeea;">Z</span> = Z.reshape(xx1.shape)
<span class="linenr">46: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr">47: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr">48: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr">49: </span>
<span class="linenr">50: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr">51: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>], 
<span class="linenr">52: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">53: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>, 
<span class="linenr">54: </span>                      c=colors[idx],
<span class="linenr">55: </span>                      marker=markers[idx], 
<span class="linenr">56: </span>                      label=cl, 
<span class="linenr">57: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr">58: </span>
<span class="linenr">59: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">highlight test samples</span>
<span class="linenr">60: </span>      <span style="color: #51afef;">if</span> <span style="color: #dcaeea;">test_idx</span>:
<span class="linenr">61: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot all samples</span>
<span class="linenr">62: </span>          <span style="color: #dcaeea;">X_test</span>, y_test = X[test_idx, :], y[test_idx]
<span class="linenr">63: </span>
<span class="linenr">64: </span>          plt.scatter(X_test[:, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">65: </span>                      X_test[:, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">66: </span>                      c=<span style="color: #98be65;">''</span>,
<span class="linenr">67: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr">68: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr">69: </span>                      linewidth=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">70: </span>                      marker=<span style="color: #98be65;">'o'</span>,
<span class="linenr">71: </span>                      s=<span style="color: #da8548; font-weight: bold;">100</span>, 
<span class="linenr">72: </span>                      label=<span style="color: #98be65;">'test set'</span>)
<span class="linenr">73: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">74: </span>
<span class="linenr">75: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Training a perceptron model using the standardized training data:</span>
<span class="linenr">76: </span>  X_combined_std = np.vstack((X_train_std, X_test_std))
<span class="linenr">77: </span>  <span style="color: #dcaeea;">y_combined</span> = np.hstack((y_train, y_test))
<span class="linenr">78: </span>
<span class="linenr">79: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;SVM&#20358;&#38928;&#28204;</span>
<span class="linenr">80: </span>  <span style="color: #51afef;">from</span> sklearn.svm <span style="color: #51afef;">import</span> SVC
<span class="linenr">81: </span>  <span style="color: #dcaeea;">svm</span> = SVC(kernel=<span style="color: #98be65;">'linear'</span>, C=<span style="color: #da8548; font-weight: bold;">1.0</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">82: </span>  svm.fit(X_train_std, y_train)
<span class="linenr">83: </span>
<span class="linenr">84: </span>  plot_decision_regions(X_combined_std, y_combined,
<span class="linenr">85: </span>                        classifier=svm,
<span class="linenr">86: </span>                        test_idx=<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">105</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr">87: </span>
<span class="linenr">88: </span>  plt.xlabel(<span style="color: #98be65;">'petal length [standardized]'</span>)
<span class="linenr">89: </span>  plt.ylabel(<span style="color: #98be65;">'petal width [standardized]'</span>)
<span class="linenr">90: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">91: </span>
<span class="linenr">92: </span>  plt.tight_layout()
<span class="linenr">93: </span>  plt.savefig(<span style="color: #98be65;">'03_11.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">94: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">95: </span>
</pre>
</div>

<pre class="example">
Class labels: [0 1 2]
Labels counts in y: [50 50 50]
Labels counts in y_train: [35 35 35]
Labels counts in y_test: [15 15 15]
</pre>



<div id="org8df58f1" class="figure">
<p><img src="images/03_11.png" alt="03_11.png" /><br />
</p>
<p><span class="figure-number">Figure 146: </span>SVM Regression: 鳶尾花分類</p>
</div>

<p>
Logistic regression(圖<a href="#org592e87d">143</a>)與 SVM(圖<a href="#org8df58f1">146</a>)常會產生相似的結果，但 logistic regression 試圖最大化「訓練數據集」的「條件概似」(conditional likelihood)，這會使 logistic regression 比 SVM 更容易傾向「離群值」(outlier)，SVM 主要在意的是那些非常接近「決策邊界」的那些點。Logistic regression 的優點是簡單。<br />
</p>
</div>
</div>

<div id="outline-container-org9e07415" class="outline-3">
<h3 id="org9e07415"><span class="section-number-3">21.7</span> 使用 SVM 解決非線性問題</h3>
<div class="outline-text-3" id="text-21-7">
<p>
SVM 受到愛用的一個主要原因是它可用「核心化」(kernelized)來解決非線性分類問題(如 XOR)。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">X_xor</span> = np.random.randn(<span style="color: #da8548; font-weight: bold;">200</span>, <span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">y_xor</span> = np.logical_xor(X_xor[:, <span style="color: #da8548; font-weight: bold;">0</span>] &gt; <span style="color: #da8548; font-weight: bold;">0</span>,
<span class="linenr"> 5: </span>                         X_xor[:, <span style="color: #da8548; font-weight: bold;">1</span>] &gt; <span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">y_xor</span> = np.where(y_xor, <span style="color: #da8548; font-weight: bold;">1</span>, -<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr">10: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">11: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">polot_decision_regions</span>(X, y, classifier, test_idx=<span style="color: #a9a1e1;">None</span>, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr">12: </span>
<span class="linenr">13: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr">14: </span>      <span style="color: #dcaeea;">markers</span> = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr">15: </span>      <span style="color: #dcaeea;">colors</span> = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr">16: </span>      <span style="color: #dcaeea;">cmap</span> = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr">17: </span>
<span class="linenr">18: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr">19: </span>      <span style="color: #dcaeea;">x1_min</span>, <span style="color: #dcaeea;">x1_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">20: </span>      <span style="color: #dcaeea;">x2_min</span>, <span style="color: #dcaeea;">x2_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">21: </span>      <span style="color: #dcaeea;">xx1</span>, <span style="color: #dcaeea;">xx2</span> = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr">22: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr">23: </span>      <span style="color: #dcaeea;">Z</span> = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr">24: </span>      <span style="color: #dcaeea;">Z</span> = Z.reshape(xx1.shape)
<span class="linenr">25: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr">26: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr">27: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr">28: </span>
<span class="linenr">29: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr">30: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>], 
<span class="linenr">31: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">32: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>, 
<span class="linenr">33: </span>                      c=colors[idx],
<span class="linenr">34: </span>                      marker=markers[idx], 
<span class="linenr">35: </span>                      label=cl, 
<span class="linenr">36: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr">37: </span>
<span class="linenr">38: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">highlight test samples</span>
<span class="linenr">39: </span>      <span style="color: #51afef;">if</span> <span style="color: #dcaeea;">test_idx</span>:
<span class="linenr">40: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot all samples</span>
<span class="linenr">41: </span>          <span style="color: #dcaeea;">X_test</span>, y_test = X[test_idx, :], y[test_idx]
<span class="linenr">42: </span>
<span class="linenr">43: </span>          plt.scatter(X_test[:, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">44: </span>                      X_test[:, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">45: </span>                      c=<span style="color: #98be65;">''</span>,
<span class="linenr">46: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr">47: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr">48: </span>                      linewidth=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">49: </span>                      marker=<span style="color: #98be65;">'o'</span>,
<span class="linenr">50: </span>                      s=<span style="color: #da8548; font-weight: bold;">100</span>, 
<span class="linenr">51: </span>                      label=<span style="color: #98be65;">'test set'</span>)
<span class="linenr">52: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">53: </span>
<span class="linenr">54: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Using the kernel trick to find separating hyperplanes in higher dimensional space</span>
<span class="linenr">55: </span>  <span style="color: #51afef;">from</span> sklearn.svm <span style="color: #51afef;">import</span> SVC
<span class="linenr">56: </span>  <span style="color: #dcaeea;">svm</span> = SVC(kernel=<span style="color: #98be65;">'rbf'</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>, gamma=<span style="color: #da8548; font-weight: bold;">0.10</span>, C=<span style="color: #da8548; font-weight: bold;">10.0</span>)
<span class="linenr">57: </span>  svm.fit(X_xor, y_xor)
<span class="linenr">58: </span>  plot_decision_regions(X_xor, y_xor,
<span class="linenr">59: </span>                        classifier=svm)
<span class="linenr">60: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">61: </span>  plt.tight_layout()
<span class="linenr">62: </span>  plt.savefig(<span style="color: #98be65;">'03_14.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">63: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">64: </span>
</pre>
</div>


<div id="org321889d" class="figure">
<p><img src="images/03_14.png" alt="03_14.png" /><br />
</p>
<p><span class="figure-number">Figure 147: </span>SVM 解決非線性問題: XOR</p>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">X</span> = iris.data[:, [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>]]
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">y</span> = iris.target
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Class labels:'</span>, np.unique(y))
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">stratify=y, &#34920;&#31034;&#26371;&#23559;&#36039;&#26009;&#20381;test_size&#27604;&#20363;&#22238;&#20659;</span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">13: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(
<span class="linenr">14: </span>      X, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>, stratify=y)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y:'</span>, np.bincount(y))
<span class="linenr">17: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y_train:'</span>, np.bincount(y_train))
<span class="linenr">18: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y_test:'</span>, np.bincount(y_test))
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#36039;&#26009;&#27161;&#28310;&#21270;: &#21033;&#29992;preprocessing&#27169;&#32068;&#35041;&#30340;StandardScaler&#39006;&#21029;</span>
<span class="linenr">21: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr">22: </span>  sc = StandardScaler() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23526;&#20363;&#21270;&#19968;&#20491;StandardScaler&#29289;&#20214;</span>
<span class="linenr">23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span class="linenr">24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span class="linenr">25: </span>  sc.fit(X_train) 
<span class="linenr">26: </span>  <span style="color: #dcaeea;">X_train_std</span> = sc.transform(X_train)
<span class="linenr">27: </span>  <span style="color: #dcaeea;">X_test_std</span> = sc.transform(X_test)
<span class="linenr">28: </span>
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">30: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr">31: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">32: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, test_idx=<span style="color: #a9a1e1;">None</span>, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr">33: </span>
<span class="linenr">34: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr">35: </span>      <span style="color: #dcaeea;">markers</span> = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr">36: </span>      <span style="color: #dcaeea;">colors</span> = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr">37: </span>      <span style="color: #dcaeea;">cmap</span> = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr">38: </span>
<span class="linenr">39: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr">40: </span>      <span style="color: #dcaeea;">x1_min</span>, <span style="color: #dcaeea;">x1_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">41: </span>      <span style="color: #dcaeea;">x2_min</span>, <span style="color: #dcaeea;">x2_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">42: </span>      <span style="color: #dcaeea;">xx1</span>, <span style="color: #dcaeea;">xx2</span> = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr">43: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr">44: </span>      <span style="color: #dcaeea;">Z</span> = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr">45: </span>      <span style="color: #dcaeea;">Z</span> = Z.reshape(xx1.shape)
<span class="linenr">46: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr">47: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr">48: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr">49: </span>
<span class="linenr">50: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr">51: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>], 
<span class="linenr">52: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">53: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>, 
<span class="linenr">54: </span>                      c=colors[idx],
<span class="linenr">55: </span>                      marker=markers[idx], 
<span class="linenr">56: </span>                      label=cl, 
<span class="linenr">57: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr">58: </span>
<span class="linenr">59: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">highlight test samples</span>
<span class="linenr">60: </span>      <span style="color: #51afef;">if</span> <span style="color: #dcaeea;">test_idx</span>:
<span class="linenr">61: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot all samples</span>
<span class="linenr">62: </span>          <span style="color: #dcaeea;">X_test</span>, y_test = X[test_idx, :], y[test_idx]
<span class="linenr">63: </span>
<span class="linenr">64: </span>          plt.scatter(X_test[:, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">65: </span>                      X_test[:, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">66: </span>                      c=<span style="color: #98be65;">''</span>,
<span class="linenr">67: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr">68: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr">69: </span>                      linewidth=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">70: </span>                      marker=<span style="color: #98be65;">'o'</span>,
<span class="linenr">71: </span>                      s=<span style="color: #da8548; font-weight: bold;">100</span>, 
<span class="linenr">72: </span>                      label=<span style="color: #98be65;">'test set'</span>)
<span class="linenr">73: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">74: </span>
<span class="linenr">75: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Training a perceptron model using the standardized training data:</span>
<span class="linenr">76: </span>  X_combined_std = np.vstack((X_train_std, X_test_std))
<span class="linenr">77: </span>  <span style="color: #dcaeea;">y_combined</span> = np.hstack((y_train, y_test))
<span class="linenr">78: </span>
<span class="linenr">79: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;SVM&#20358;&#38928;&#28204;&#40182;&#23614;&#33457;</span>
<span class="linenr">80: </span>  <span style="color: #51afef;">from</span> sklearn.svm <span style="color: #51afef;">import</span> SVC
<span class="linenr">81: </span>  <span style="color: #dcaeea;">svm</span> = SVC(kernel=<span style="color: #98be65;">'rbf'</span>, random_state=<span style="color: #da8548; font-weight: bold;">0</span>, gamma=<span style="color: #da8548; font-weight: bold;">0.10</span>, C=<span style="color: #da8548; font-weight: bold;">10.0</span>)
<span class="linenr">82: </span>  svm.fit(X_train_std, y_train)
<span class="linenr">83: </span>
<span class="linenr">84: </span>  plot_decision_regions(X_combined_std, y_combined,
<span class="linenr">85: </span>                        classifier=svm,
<span class="linenr">86: </span>                        test_idx=<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">105</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr">87: </span>
<span class="linenr">88: </span>  plt.xlabel(<span style="color: #98be65;">'petal length [standardized]'</span>)
<span class="linenr">89: </span>  plt.ylabel(<span style="color: #98be65;">'petal width [standardized]'</span>)
<span class="linenr">90: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">91: </span>
<span class="linenr">92: </span>  plt.tight_layout()
<span class="linenr">93: </span>  plt.savefig(<span style="color: #98be65;">'03_15.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">94: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">95: </span>
</pre>
</div>

<pre class="example">
Class labels: [0 1 2]
Labels counts in y: [50 50 50]
Labels counts in y_train: [35 35 35]
Labels counts in y_test: [15 15 15]
</pre>



<div id="org5731d5c" class="figure">
<p><img src="images/03_15.png" alt="03_15.png" /><br />
</p>
<p><span class="figure-number">Figure 148: </span>SVM: 鳶尾花分類</p>
</div>
</div>
</div>

<div id="outline-container-org6a72c2b" class="outline-3">
<h3 id="org6a72c2b"><span class="section-number-3">21.8</span> 使用決策樹</h3>
<div class="outline-text-3" id="text-21-8">
<p>
「決策樹」可透過劃分特徵空間來建構複雜的矩形「決策邊界」。然而，越深的「決策樹」便會產生越複雜的「決策邊界」，因而導致過度擬知。以下透過 scikit-learn，以「熵」作為「不純度」的標準，訓練一個深度最多為 3 的「決策樹」。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">X</span> = iris.data[:, [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>]]
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">y</span> = iris.target
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Class labels:'</span>, np.unique(y))
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">stratify=y, &#34920;&#31034;&#26371;&#23559;&#36039;&#26009;&#20381;test_size&#27604;&#20363;&#22238;&#20659;</span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">13: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(
<span class="linenr">14: </span>      X, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>, stratify=y)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y:'</span>, np.bincount(y))
<span class="linenr">17: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y_train:'</span>, np.bincount(y_train))
<span class="linenr">18: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y_test:'</span>, np.bincount(y_test))
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#36039;&#26009;&#27161;&#28310;&#21270;: &#21033;&#29992;preprocessing&#27169;&#32068;&#35041;&#30340;StandardScaler&#39006;&#21029;</span>
<span class="linenr">21: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr">22: </span>  sc = StandardScaler() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23526;&#20363;&#21270;&#19968;&#20491;StandardScaler&#29289;&#20214;</span>
<span class="linenr">23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span class="linenr">24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span class="linenr">25: </span>  sc.fit(X_train) 
<span class="linenr">26: </span>  <span style="color: #dcaeea;">X_train_std</span> = sc.transform(X_train)
<span class="linenr">27: </span>  <span style="color: #dcaeea;">X_test_std</span> = sc.transform(X_test)
<span class="linenr">28: </span>
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">30: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr">31: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">32: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, test_idx=<span style="color: #a9a1e1;">None</span>, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr">33: </span>
<span class="linenr">34: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr">35: </span>      <span style="color: #dcaeea;">markers</span> = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr">36: </span>      <span style="color: #dcaeea;">colors</span> = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr">37: </span>      <span style="color: #dcaeea;">cmap</span> = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr">38: </span>
<span class="linenr">39: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr">40: </span>      <span style="color: #dcaeea;">x1_min</span>, <span style="color: #dcaeea;">x1_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">41: </span>      <span style="color: #dcaeea;">x2_min</span>, <span style="color: #dcaeea;">x2_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">42: </span>      <span style="color: #dcaeea;">xx1</span>, <span style="color: #dcaeea;">xx2</span> = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr">43: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr">44: </span>      <span style="color: #dcaeea;">Z</span> = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr">45: </span>      <span style="color: #dcaeea;">Z</span> = Z.reshape(xx1.shape)
<span class="linenr">46: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr">47: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr">48: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr">49: </span>
<span class="linenr">50: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr">51: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>], 
<span class="linenr">52: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">53: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>, 
<span class="linenr">54: </span>                      c=colors[idx],
<span class="linenr">55: </span>                      marker=markers[idx], 
<span class="linenr">56: </span>                      label=cl, 
<span class="linenr">57: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr">58: </span>
<span class="linenr">59: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">highlight test samples</span>
<span class="linenr">60: </span>      <span style="color: #51afef;">if</span> <span style="color: #dcaeea;">test_idx</span>:
<span class="linenr">61: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot all samples</span>
<span class="linenr">62: </span>          <span style="color: #dcaeea;">X_test</span>, y_test = X[test_idx, :], y[test_idx]
<span class="linenr">63: </span>
<span class="linenr">64: </span>          plt.scatter(X_test[:, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">65: </span>                      X_test[:, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">66: </span>                      c=<span style="color: #98be65;">''</span>,
<span class="linenr">67: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr">68: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr">69: </span>                      linewidth=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">70: </span>                      marker=<span style="color: #98be65;">'o'</span>,
<span class="linenr">71: </span>                      s=<span style="color: #da8548; font-weight: bold;">100</span>, 
<span class="linenr">72: </span>                      label=<span style="color: #98be65;">'test set'</span>)
<span class="linenr">73: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">74: </span>
<span class="linenr">75: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Training a perceptron model using the standardized training data:</span>
<span class="linenr">76: </span>  X_combined = np.vstack((X_train, X_test))
<span class="linenr">77: </span>  <span style="color: #dcaeea;">y_combined</span> = np.hstack((y_train, y_test))
<span class="linenr">78: </span>
<span class="linenr">79: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;&#27770;&#31574;&#27193;&#20358;&#38928;&#28204;&#40182;&#23614;&#33457;</span>
<span class="linenr">80: </span>  <span style="color: #51afef;">from</span> sklearn.tree <span style="color: #51afef;">import</span> DecisionTreeClassifier
<span class="linenr">81: </span>  <span style="color: #dcaeea;">tree</span> = DecisionTreeClassifier(criterion=<span style="color: #98be65;">'gini'</span>,
<span class="linenr">82: </span>                                max_depth=<span style="color: #da8548; font-weight: bold;">4</span>,
<span class="linenr">83: </span>                                random_state=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">84: </span>  tree.fit(X_train, y_train)
<span class="linenr">85: </span>
<span class="linenr">86: </span>  plot_decision_regions(X_combined, y_combined,
<span class="linenr">87: </span>                        classifier=tree,
<span class="linenr">88: </span>                        test_idx=<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">105</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr">89: </span>
<span class="linenr">90: </span>  plt.xlabel(<span style="color: #98be65;">'petal length [standardized]'</span>)
<span class="linenr">91: </span>  plt.ylabel(<span style="color: #98be65;">'petal width [standardized]'</span>)
<span class="linenr">92: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">93: </span>
<span class="linenr">94: </span>  plt.tight_layout()
<span class="linenr">95: </span>  plt.savefig(<span style="color: #98be65;">'03_17.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">96: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">97: </span>
</pre>
</div>

<pre class="example">
Class labels: [0 1 2]
Labels counts in y: [50 50 50]
Labels counts in y_train: [35 35 35]
Labels counts in y_test: [15 15 15]
</pre>



<div id="org9f46319" class="figure">
<p><img src="images/03_17.png" alt="03_17.png" /><br />
</p>
<p><span class="figure-number">Figure 149: </span>Decision Tree: 鳶尾花分類</p>
</div>
</div>
</div>

<div id="outline-container-org7200218" class="outline-3">
<h3 id="org7200218"><span class="section-number-3">21.9</span> 使用隨機森林結合決策樹</h3>
<div class="outline-text-3" id="text-21-9">
<p>
「隨機森林」(random forest)可被視為多個「決策樹」結合成的一個整體(ensemble)，隨機森林的想法是結合多個具有高變異的深度決策樹，將它們的結果平均，來建構一個更強固的模型，這種模型的「一般化誤差」較低，也較不會發生過度擬合的問題。<br />
</p>

<p>
隨機森林演算法可以簡單歸納為下列四個步驟：<br />
</p>
<ol class="org-ol">
<li>定義大小為 n 的隨機「自助」(bootstrap)樣本（從「訓練樣本集」中隨機選擇 n 個樣本，採用「取出後放回」方式）。<br /></li>
<li>從 bootstrap 樣本中導出決策樹：<br />
<ul class="org-ul">
<li>隨機選擇 d 個特徵<br /></li>
<li>使用特徵分割該節點，依「目標函數」找出最佳方式<br /></li>
</ul></li>
<li>重複 k 次步驟 1 與 2<br /></li>
<li>匯總所有決策樹的預測，以「多數決」(majority voting)的方式來指定類別標籤。<br /></li>
</ol>

<p>
隨機森林的一大優勢是我們不必擔心如何選擇一個好的「超參數」值，一般而言，我們不需修剪「隨機森林」，因為「整體學習模型」是相當強固的，個別「決策樹」的雜訊不會影響整體結果。我們唯一真正需要關心的參數是上述步驟中的數量 k。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">X</span> = iris.data[:, [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>]]
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">y</span> = iris.target
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Class labels:'</span>, np.unique(y))
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">stratify=y, &#34920;&#31034;&#26371;&#23559;&#36039;&#26009;&#20381;test_size&#27604;&#20363;&#22238;&#20659;</span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">13: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(
<span class="linenr">14: </span>      X, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>, stratify=y)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y:'</span>, np.bincount(y))
<span class="linenr">17: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y_train:'</span>, np.bincount(y_train))
<span class="linenr">18: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y_test:'</span>, np.bincount(y_test))
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#36039;&#26009;&#27161;&#28310;&#21270;: &#21033;&#29992;preprocessing&#27169;&#32068;&#35041;&#30340;StandardScaler&#39006;&#21029;</span>
<span class="linenr">21: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr">22: </span>  sc = StandardScaler() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23526;&#20363;&#21270;&#19968;&#20491;StandardScaler&#29289;&#20214;</span>
<span class="linenr">23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span class="linenr">24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span class="linenr">25: </span>  sc.fit(X_train) 
<span class="linenr">26: </span>  <span style="color: #dcaeea;">X_train_std</span> = sc.transform(X_train)
<span class="linenr">27: </span>  <span style="color: #dcaeea;">X_test_std</span> = sc.transform(X_test)
<span class="linenr">28: </span>
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">30: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr">31: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">32: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, test_idx=<span style="color: #a9a1e1;">None</span>, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr">33: </span>
<span class="linenr">34: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr">35: </span>      <span style="color: #dcaeea;">markers</span> = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr">36: </span>      <span style="color: #dcaeea;">colors</span> = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr">37: </span>      <span style="color: #dcaeea;">cmap</span> = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr">38: </span>
<span class="linenr">39: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr">40: </span>      <span style="color: #dcaeea;">x1_min</span>, <span style="color: #dcaeea;">x1_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">41: </span>      <span style="color: #dcaeea;">x2_min</span>, <span style="color: #dcaeea;">x2_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">42: </span>      <span style="color: #dcaeea;">xx1</span>, <span style="color: #dcaeea;">xx2</span> = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr">43: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr">44: </span>      <span style="color: #dcaeea;">Z</span> = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr">45: </span>      <span style="color: #dcaeea;">Z</span> = Z.reshape(xx1.shape)
<span class="linenr">46: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr">47: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr">48: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr">49: </span>
<span class="linenr">50: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr">51: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>], 
<span class="linenr">52: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">53: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>, 
<span class="linenr">54: </span>                      c=colors[idx],
<span class="linenr">55: </span>                      marker=markers[idx], 
<span class="linenr">56: </span>                      label=cl, 
<span class="linenr">57: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr">58: </span>
<span class="linenr">59: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">highlight test samples</span>
<span class="linenr">60: </span>      <span style="color: #51afef;">if</span> <span style="color: #dcaeea;">test_idx</span>:
<span class="linenr">61: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot all samples</span>
<span class="linenr">62: </span>          <span style="color: #dcaeea;">X_test</span>, y_test = X[test_idx, :], y[test_idx]
<span class="linenr">63: </span>
<span class="linenr">64: </span>          plt.scatter(X_test[:, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">65: </span>                      X_test[:, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">66: </span>                      c=<span style="color: #98be65;">''</span>,
<span class="linenr">67: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr">68: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr">69: </span>                      linewidth=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">70: </span>                      marker=<span style="color: #98be65;">'o'</span>,
<span class="linenr">71: </span>                      s=<span style="color: #da8548; font-weight: bold;">100</span>, 
<span class="linenr">72: </span>                      label=<span style="color: #98be65;">'test set'</span>)
<span class="linenr">73: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">74: </span>
<span class="linenr">75: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Training a perceptron model using the standardized training data:</span>
<span class="linenr">76: </span>  X_combined = np.vstack((X_train, X_test))
<span class="linenr">77: </span>  <span style="color: #dcaeea;">y_combined</span> = np.hstack((y_train, y_test))
<span class="linenr">78: </span>
<span class="linenr">79: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;&#38568;&#27231;&#26862;&#26519;&#27770;&#31574;&#27193;&#20358;&#38928;&#28204;&#40182;&#23614;&#33457;</span>
<span class="linenr">80: </span>  <span style="color: #51afef;">from</span> sklearn.ensemble <span style="color: #51afef;">import</span> RandomForestClassifier
<span class="linenr">81: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">n_estimators: &#29986;&#29983;&#30340;&#27770;&#31574;&#27193;&#25976;&#37327;</span>
<span class="linenr">82: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;model&#26178;&#20351;&#29992;&#30340;&#38651;&#33126;&#26680;&#24515;&#25976;&#37327;</span>
<span class="linenr">83: </span>  forest = RandomForestClassifier(criterion=<span style="color: #98be65;">'gini'</span>,
<span class="linenr">84: </span>                                  n_estimators=<span style="color: #da8548; font-weight: bold;">25</span>,
<span class="linenr">85: </span>                                  random_state=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">86: </span>                                  n_jobs=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">87: </span>  forest.fit(X_train, y_train)
<span class="linenr">88: </span>  plot_decision_regions(X_combined, y_combined,
<span class="linenr">89: </span>                        classifier=forest,
<span class="linenr">90: </span>                        test_idx=<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">105</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr">91: </span>
<span class="linenr">92: </span>  plt.xlabel(<span style="color: #98be65;">'petal length [standardized]'</span>)
<span class="linenr">93: </span>  plt.ylabel(<span style="color: #98be65;">'petal width [standardized]'</span>)
<span class="linenr">94: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">95: </span>
<span class="linenr">96: </span>  plt.tight_layout()
<span class="linenr">97: </span>  plt.savefig(<span style="color: #98be65;">'03_18.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">98: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
</pre>
</div>

<pre class="example">
Class labels: [0 1 2]
Labels counts in y: [50 50 50]
Labels counts in y_train: [35 35 35]
Labels counts in y_test: [15 15 15]
</pre>


<p>
+ATTR_HTML: :width 500<br />
<img src="images/03_18.png" alt="03_18.png" /><br />
</p>
</div>
</div>

<div id="outline-container-org44ee840" class="outline-3">
<h3 id="org44ee840"><span class="section-number-3">21.10</span> KNN</h3>
<div class="outline-text-3" id="text-21-10">
<p>
KNN (k-nearest neighbor classifier)為 lazy learner(惰性學習器)的典型例子，所謂惰性是指它不會從「訓練數據集」中學習出「判別函數」(discriminative function)，它的作法是把「訓練數據集」記憶起來。其步驟如下：<br />
</p>
<ol class="org-ol">
<li>選定 k 的值和一個「距離度量」(distance metric)。<br /></li>
<li>找出 k 個想要分類的、最相近的鄰近樣本。<br /></li>
<li>以多數決的方式指定類別標籤。<br /></li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">X</span> = iris.data[:, [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>]]
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">y</span> = iris.target
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Class labels:'</span>, np.unique(y))
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">stratify=y, &#34920;&#31034;&#26371;&#23559;&#36039;&#26009;&#20381;test_size&#27604;&#20363;&#22238;&#20659;</span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">13: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(
<span class="linenr">14: </span>      X, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>, stratify=y)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y:'</span>, np.bincount(y))
<span class="linenr">17: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y_train:'</span>, np.bincount(y_train))
<span class="linenr">18: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Labels counts in y_test:'</span>, np.bincount(y_test))
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#36039;&#26009;&#27161;&#28310;&#21270;: &#21033;&#29992;preprocessing&#27169;&#32068;&#35041;&#30340;StandardScaler&#39006;&#21029;</span>
<span class="linenr">21: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr">22: </span>  sc = StandardScaler() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23526;&#20363;&#21270;&#19968;&#20491;StandardScaler&#29289;&#20214;</span>
<span class="linenr">23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span class="linenr">24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span class="linenr">25: </span>  sc.fit(X_train) 
<span class="linenr">26: </span>  <span style="color: #dcaeea;">X_train_std</span> = sc.transform(X_train)
<span class="linenr">27: </span>  <span style="color: #dcaeea;">X_test_std</span> = sc.transform(X_test)
<span class="linenr">28: </span>
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">30: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr">31: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">32: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, test_idx=<span style="color: #a9a1e1;">None</span>, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr">33: </span>
<span class="linenr">34: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr">35: </span>      <span style="color: #dcaeea;">markers</span> = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr">36: </span>      <span style="color: #dcaeea;">colors</span> = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr">37: </span>      <span style="color: #dcaeea;">cmap</span> = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr">38: </span>
<span class="linenr">39: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr">40: </span>      <span style="color: #dcaeea;">x1_min</span>, <span style="color: #dcaeea;">x1_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">41: </span>      <span style="color: #dcaeea;">x2_min</span>, <span style="color: #dcaeea;">x2_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">42: </span>      <span style="color: #dcaeea;">xx1</span>, <span style="color: #dcaeea;">xx2</span> = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr">43: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr">44: </span>      <span style="color: #dcaeea;">Z</span> = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr">45: </span>      <span style="color: #dcaeea;">Z</span> = Z.reshape(xx1.shape)
<span class="linenr">46: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr">47: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr">48: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr">49: </span>
<span class="linenr">50: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr">51: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>], 
<span class="linenr">52: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">53: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>, 
<span class="linenr">54: </span>                      c=colors[idx],
<span class="linenr">55: </span>                      marker=markers[idx], 
<span class="linenr">56: </span>                      label=cl, 
<span class="linenr">57: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr">58: </span>
<span class="linenr">59: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">highlight test samples</span>
<span class="linenr">60: </span>      <span style="color: #51afef;">if</span> <span style="color: #dcaeea;">test_idx</span>:
<span class="linenr">61: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot all samples</span>
<span class="linenr">62: </span>          <span style="color: #dcaeea;">X_test</span>, y_test = X[test_idx, :], y[test_idx]
<span class="linenr">63: </span>
<span class="linenr">64: </span>          plt.scatter(X_test[:, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">65: </span>                      X_test[:, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">66: </span>                      c=<span style="color: #98be65;">''</span>,
<span class="linenr">67: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr">68: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr">69: </span>                      linewidth=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">70: </span>                      marker=<span style="color: #98be65;">'o'</span>,
<span class="linenr">71: </span>                      s=<span style="color: #da8548; font-weight: bold;">100</span>, 
<span class="linenr">72: </span>                      label=<span style="color: #98be65;">'test set'</span>)
<span class="linenr">73: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">74: </span>
<span class="linenr">75: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Training a perceptron model using the standardized training data:</span>
<span class="linenr">76: </span>  X_combined_std = np.vstack((X_train_std, X_test))
<span class="linenr">77: </span>  <span style="color: #dcaeea;">y_combined</span> = np.hstack((y_train, y_test))
<span class="linenr">78: </span>
<span class="linenr">79: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;KNN&#20358;&#38928;&#28204;&#40182;&#23614;&#33457;</span>
<span class="linenr">80: </span>  <span style="color: #51afef;">from</span> sklearn.neighbors <span style="color: #51afef;">import</span> KNeighborsClassifier
<span class="linenr">81: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">n_neighbors: &#35201;&#25214;&#20986;&#30340;&#26368;&#36817;&#30456;&#37168;&#27171;&#26412;&#25976;</span>
<span class="linenr">82: </span>  knn = KNeighborsClassifier(n_neighbors=<span style="color: #da8548; font-weight: bold;">5</span>, p=<span style="color: #da8548; font-weight: bold;">2</span>,
<span class="linenr">83: </span>                             metric=<span style="color: #98be65;">'minkowski'</span>)
<span class="linenr">84: </span>  knn.fit(X_train_std, y_train)
<span class="linenr">85: </span>  plot_decision_regions(X_combined_std, y_combined,
<span class="linenr">86: </span>                        classifier=knn,
<span class="linenr">87: </span>                        test_idx=<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">105</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr">88: </span>
<span class="linenr">89: </span>  plt.xlabel(<span style="color: #98be65;">'petal length [standardized]'</span>)
<span class="linenr">90: </span>  plt.ylabel(<span style="color: #98be65;">'petal width [standardized]'</span>)
<span class="linenr">91: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">92: </span>
<span class="linenr">93: </span>  plt.tight_layout()
<span class="linenr">94: </span>  plt.savefig(<span style="color: #98be65;">'03_19.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">95: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">96: </span>
</pre>
</div>

<pre class="example">
Class labels: [0 1 2]
Labels counts in y: [50 50 50]
Labels counts in y_train: [35 35 35]
Labels counts in y_test: [15 15 15]
</pre>



<div id="orgf05f719" class="figure">
<p><img src="images/03_19.png" alt="03_19.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 150: </span>KNN: 鳶尾花分類</p>
</div>

<p>
想要在「過度擬合」與「擬合不足」之間取得一個良好的平衡，其中一個關鍵因素是選擇一個「好」的 k 值。我們選擇的「距離度量」必須是對該「訓練數據集」的特徵是有意義的。通常會以簡單的「歐氏距離」來度量實數值的樣本。<br />
</p>
</div>
</div>

<div id="outline-container-org5ad8146" class="outline-3">
<h3 id="org5ad8146"><span class="section-number-3">21.11</span> 有母數模型與無母數模型</h3>
<div class="outline-text-3" id="text-21-11">
<p>
「機器學習演算法」可分為「有母數」(parametric)和「無母數」(nonparametric)模型。使用有母數模型，我們從「訓練數據集」估計參數值，並學習出一個函數，他可以不需要原始的「訓練數據集」而對新數據集進行分類。「感知器模型」、「logistic regression」與「線性 SVM」都是典型的例子。另一方面，「無母數模型」無法用一組固定的特徵來描述，此外，特徵的個數還會因為「訓練數據集」的增大而變多 「決策樹」/「隨機森林」與「核支援向量機」則是目前我們看到的「無母數模型「的例子。KNN 也被歸類為「無母數模型」中的一種，稱為「基於實例學習」(instance-based learning)。「基於實例學習」會把「訓練數據集」記憶起來。而「惰性學習器」則是「基於實例學習」中的一個特例。「惰性學習器」在學習的過程中是沒有成本的。<br />
</p>
</div>
</div>
</div>

<div id="outline-container-org2cf41e9" class="outline-2">
<h2 id="org2cf41e9"><span class="section-number-2">22</span> NumPy</h2>
<div class="outline-text-2" id="text-22">
</div>
<div id="outline-container-orge4d1012" class="outline-3">
<h3 id="orge4d1012"><span class="section-number-3">22.1</span> 簡介</h3>
<div class="outline-text-3" id="text-22-1">
<p>
NumPy 是 Python 語言的一個擴充程式庫。支援高階大量的維度陣列與矩陣運算，此外也針對陣列運算提供大量的數學函式函式庫。NumPy 的前身 Numeric 最早是由 Jim Hugunin 與其它協作者共同開發，2005 年，Travis Oliphant 在 Numeric 中結合了另一個同性質的程式庫 Numarray 的特色，並加入了其它擴充功能而開發了 NumPy。NumPy 為開放原始碼並且由許多協作者共同維護開發。<sup><a id="fnr.25" class="footref" href="#fn.25">25</a></sup><br />
</p>
</div>
</div>

<div id="outline-container-org5180a71" class="outline-3">
<h3 id="org5180a71"><span class="section-number-3">22.2</span> NumPy 陣列</h3>
<div class="outline-text-3" id="text-22-2">
</div>
<ol class="org-ol">
<li><a id="orgebbab67"></a>create<br />
<div class="outline-text-4" id="text-22-2-1">
<p>
Numpy 的重點在於陣列的操作，其所有功能特色都建築在同質且多重維度的 ndarray（N-dimensional array）上。ndarray 的關鍵屬性是維度（ndim）、形狀（shape）和數值類型（dtype）。 一般我們稱一維陣列為 vector 而二維陣列為 matrix。一開始我們會引入 numpy 模組，透過傳入 list 到 numpy.array() 創建陣列。<br />
</p>

<ul class="org-ul">
<li>with data<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24341;&#20837; numpy &#27169;&#32068;</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">np1</span> = np.array([<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>])
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">np2</span> = np.array([<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">5</span>])
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38499;&#21015;&#30456;&#21152;</span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">print</span>(np1 + np2) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">[4 6 8]</span>
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#30456;&#38364;&#36039;&#35338;</span>
<span class="linenr">10: </span>  <span style="color: #51afef;">print</span>(np1.ndim, np1.shape, np1.dtype) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1 (3,) int64 =&gt; &#19968;&#32173;&#38499;&#21015;, &#19977;&#20491;&#20803;&#32032;, &#36039;&#26009;&#22411;&#21029;</span>
<span class="linenr">11: </span>
<span class="linenr">12: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">create identity matrix</span>
<span class="linenr">13: </span>  <span style="color: #dcaeea;">ary1</span> = np.eye(<span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">14: </span>  <span style="color: #51afef;">print</span>(ary1)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">create diagonal array</span>
<span class="linenr">17: </span>  <span style="color: #dcaeea;">ary2</span> = np.diag((<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">4</span>,<span style="color: #da8548; font-weight: bold;">6</span>))
<span class="linenr">18: </span>  <span style="color: #51afef;">print</span>(ary2)
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #5B6268;">#</span>
<span class="linenr">21: </span>  <span style="color: #dcaeea;">ary3</span> = np.array([<span style="color: #c678dd;">range</span>(i, i+<span style="color: #da8548; font-weight: bold;">3</span>) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> [<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">4</span>,<span style="color: #da8548; font-weight: bold;">6</span>]])
<span class="linenr">22: </span>  <span style="color: #51afef;">print</span>(ary3)
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">tile</span>
<span class="linenr">25: </span>  <span style="color: #dcaeea;">ary4</span> = np.array([<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">2</span>])
<span class="linenr">26: </span>  <span style="color: #51afef;">print</span>(np.tile(ary4,<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr">27: </span>  <span style="color: #51afef;">print</span>(np.tile(ary4,(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr">28: </span>
<span class="linenr">29: </span>  <span style="color: #dcaeea;">ary5</span> = np.array([[<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">2</span>],[<span style="color: #da8548; font-weight: bold;">6</span>,<span style="color: #da8548; font-weight: bold;">7</span>]])
<span class="linenr">30: </span>  <span style="color: #51afef;">print</span>(np.tile(ary5,<span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr">31: </span>  <span style="color: #51afef;">print</span>(np.tile(ary5,(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">2</span>)))
</pre>
</div>

<pre class="example">
[4 6 8]
1 (3,) int64
[[1. 0. 0.]
 [0. 1. 0.]
 [0. 0. 1.]]
[[2 0 0 0]
 [0 1 0 0]
 [0 0 4 0]
 [0 0 0 6]]
[[2 3 4]
 [4 5 6]
 [6 7 8]]
[0 1 2 0 1 2]
[[0 1 2 0 1 2]
 [0 1 2 0 1 2]]
[[1 2 1 2 1 2]
 [6 7 6 7 6 7]]
[[1 2 1 2]
 [6 7 6 7]
 [1 2 1 2]
 [6 7 6 7]]
</pre>

<ul class="org-ul">
<li>numpy.random.randint()<br /></li>
</ul>
<p>
語法：numpy.random.randint(low, high=None, size=None, dtype=&rsquo;l&rsquo;)<br />
</p>

<p>
函式的作用是，返回一個隨機整型數，範圍從低（包括）到高（不包括），即[low, high)。<br />
如果沒有寫引數 high 的值，則返回[0,low)的值。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">2: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">3: </span>
<span class="linenr">4: </span>  <span style="color: #dcaeea;">x1</span> = np.random.randint(<span style="color: #da8548; font-weight: bold;">10</span>, size=<span style="color: #da8548; font-weight: bold;">6</span>)
<span class="linenr">5: </span>  <span style="color: #dcaeea;">x2</span> = np.random.randint(<span style="color: #da8548; font-weight: bold;">10</span>, size=(<span style="color: #da8548; font-weight: bold;">3</span>,<span style="color: #da8548; font-weight: bold;">4</span>))
<span class="linenr">6: </span>  <span style="color: #dcaeea;">x3</span> = np.random.randint(<span style="color: #da8548; font-weight: bold;">10</span>, size=(<span style="color: #da8548; font-weight: bold;">3</span>,<span style="color: #da8548; font-weight: bold;">4</span>,<span style="color: #da8548; font-weight: bold;">5</span>))
<span class="linenr">7: </span>  <span style="color: #51afef;">print</span>(x1)
<span class="linenr">8: </span>  <span style="color: #51afef;">print</span>(x2)
<span class="linenr">9: </span>  <span style="color: #51afef;">print</span>(x3)
</pre>
</div>

<pre class="example">
[5 0 3 3 7 9]
[[3 5 2 4]
 [7 6 8 8]
 [1 6 7 7]]
[[[8 1 5 9 8]
  [9 4 3 0 3]
  [5 0 2 3 8]
  [1 3 3 3 7]]

 [[0 1 9 9 0]
  [4 7 3 2 7]
  [2 0 0 4 5]
  [5 6 8 4 1]]

 [[4 9 8 1 1]
  [7 9 9 3 6]
  [7 2 0 3 5]
  [9 4 4 6 4]]]
</pre>

<ul class="org-ul">
<li>numpy.random.rand()<br /></li>
</ul>
<p>
根據給定維度生成(0,1)間的資料，包含 0，不包含 1<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">2: </span>
<span class="linenr">3: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">9627</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35373;&#32622;&#30456;&#21516;&#35722;&#25976;&#65292;&#27599;&#27425;&#29983;&#25104;&#30456;&#21516;&#20098;&#25976;</span>
<span class="linenr">4: </span>  <span style="color: #dcaeea;">ar</span> = np.random.rand(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">4</span>)
<span class="linenr">5: </span>  <span style="color: #51afef;">print</span>(ar)
</pre>
</div>

<pre class="example">
[[0.28012059 0.19216219 0.63985614 0.48842053]
 [0.9441813  0.88992099 0.17534833 0.29543319]]
</pre>
</div>
</li>

<li><a id="org173f61a"></a>numpy 矩陣間的運算<br />
<div class="outline-text-4" id="text-22-2-2">
<ul class="org-ul">
<li>element-wise<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">2: </span>
<span class="linenr">3: </span>  <span style="color: #dcaeea;">ar</span> = np.array([[<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">3</span>],[<span style="color: #da8548; font-weight: bold;">4</span>,<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">6</span>],[<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">3</span>,<span style="color: #da8548; font-weight: bold;">4</span>]])
<span class="linenr">4: </span>  <span style="color: #51afef;">print</span>(ar)
<span class="linenr">5: </span>  <span style="color: #51afef;">print</span>(ar+ar)
<span class="linenr">6: </span>  <span style="color: #51afef;">print</span>(ar**.<span style="color: #da8548; font-weight: bold;">5</span>)
<span class="linenr">7: </span>
<span class="linenr">8: </span>  <span style="color: #dcaeea;">ar1</span> = np.array([[<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">2</span>],[<span style="color: #da8548; font-weight: bold;">3</span>,<span style="color: #da8548; font-weight: bold;">3</span>],[<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">1</span>]])
<span class="linenr">9: </span>  <span style="color: #51afef;">print</span>(ar.dot(ar1))  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#30697;&#38499;dot</span>
</pre>
</div>

<pre class="example">
[[1 2 3]
 [4 5 6]
 [2 3 4]]
[[ 2  4  6]
 [ 8 10 12]
 [ 4  6  8]]
[[1.         1.41421356 1.73205081]
 [2.         2.23606798 2.44948974]
 [1.41421356 1.73205081 2.        ]]
[[11 11]
 [29 29]
 [17 17]]
</pre>

<ul class="org-ul">
<li>Expressing Conditional Logic as Array Operations<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">xr</span> = np.array([<span style="color: #da8548; font-weight: bold;">1.1</span>, <span style="color: #da8548; font-weight: bold;">1.2</span>, <span style="color: #da8548; font-weight: bold;">1.3</span>, <span style="color: #da8548; font-weight: bold;">1.4</span>, <span style="color: #da8548; font-weight: bold;">1.5</span>])
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">yr</span> = np.array([<span style="color: #da8548; font-weight: bold;">2.1</span>, <span style="color: #da8548; font-weight: bold;">2.2</span>, <span style="color: #da8548; font-weight: bold;">2.3</span>, <span style="color: #da8548; font-weight: bold;">2.4</span>, <span style="color: #da8548; font-weight: bold;">2.5</span>])
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">cond</span> = np.array([<span style="color: #a9a1e1;">True</span>, <span style="color: #a9a1e1;">False</span>, <span style="color: #a9a1e1;">True</span>, <span style="color: #a9a1e1;">True</span>, <span style="color: #a9a1e1;">False</span>])
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">result</span> = [(x <span style="color: #51afef;">if</span> c <span style="color: #51afef;">else</span> y)
<span class="linenr"> 8: </span>            <span style="color: #51afef;">for</span> x, y, c <span style="color: #51afef;">in</span> <span style="color: #c678dd;">zip</span>(xr, yr, cond)]
<span class="linenr"> 9: </span>  <span style="color: #51afef;">print</span>(result)
<span class="linenr">10: </span>  <span style="color: #51afef;">print</span>(<span style="color: #c678dd;">type</span>(result))
<span class="linenr">11: </span>  <span style="color: #51afef;">print</span>(np.where(cond, xr, yr))
<span class="linenr">12: </span>  <span style="color: #51afef;">print</span>(<span style="color: #c678dd;">type</span>(result))
<span class="linenr">13: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#32080;&#26524;&#37117;&#20659;&#22238;list,&#28858;&#20309;&#26377;,&#30340;&#24046;&#30064;</span>
</pre>
</div>

<pre class="example">
[1.1, 2.2, 1.3, 1.4, 2.5]
&lt;class 'list'&gt;
[1.1 2.2 1.3 1.4 2.5]
&lt;class 'list'&gt;
</pre>


<ul class="org-ul">
<li>Braodcasting<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">2: </span>
<span class="linenr">3: </span>  <span style="color: #dcaeea;">x1</span> = np.arange(<span style="color: #da8548; font-weight: bold;">9.0</span>).reshape((<span style="color: #da8548; font-weight: bold;">3</span>,<span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr">4: </span>  <span style="color: #51afef;">print</span>(x1)
<span class="linenr">5: </span>  <span style="color: #dcaeea;">x2</span> = np.arange(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">4</span>)
<span class="linenr">6: </span>  <span style="color: #51afef;">print</span>(x2)
<span class="linenr">7: </span>  <span style="color: #51afef;">print</span>(np.multiply(x1,x2))
</pre>
</div>

<pre class="example">
[[0. 1. 2.]
 [3. 4. 5.]
 [6. 7. 8.]]
[1 2 3]
[[ 0.  2.  6.]
 [ 3.  8. 15.]
 [ 6. 14. 24.]]
</pre>


<ul class="org-ul">
<li>Array sorting<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">2: </span>
<span class="linenr">3: </span>  <span style="color: #dcaeea;">ar</span> = np.array([[<span style="color: #da8548; font-weight: bold;">3</span>,<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">5</span>],[<span style="color: #da8548; font-weight: bold;">10</span>,-<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">9</span>],[<span style="color: #da8548; font-weight: bold;">4</span>,<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">12</span>]])
<span class="linenr">4: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"origin:\n"</span>,ar)
<span class="linenr">5: </span>  ar.sort(axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">6: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"axis=0:\n"</span>,ar)
<span class="linenr">7: </span>  ar.sort(axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">8: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"axis=1:\n"</span>,ar)
</pre>
</div>

<pre class="example">
origin:
 [[ 3  2  5]
 [10 -1  9]
 [ 4  1 12]]
axis=0:
 [[ 3 -1  5]
 [ 4  1  9]
 [10  2 12]]
axis=1:
 [[-1  3  5]
 [ 1  4  9]
 [ 2 10 12]]
</pre>

<ul class="org-ul">
<li>Array reverse<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">2: </span>
<span class="linenr">3: </span>  <span style="color: #dcaeea;">ar</span> = np.arange(<span style="color: #da8548; font-weight: bold;">5</span>)
<span class="linenr">4: </span>  <span style="color: #51afef;">print</span>(ar[::-<span style="color: #da8548; font-weight: bold;">1</span>])
</pre>
</div>

<pre class="example">
[4 3 2 1 0]
</pre>
</div>
</li>

<li><a id="orga4bf5ce"></a>矩陣間的 convolute 運算<br />
<div class="outline-text-4" id="text-22-2-3">
<ul class="org-ul">
<li>numpy.convolve(a, v, mode=&rsquo;full&rsquo;)，這是 numpy 函數中的卷積函數庫<br /></li>
</ul>
<p>
參數：<br />
</p>
<ul class="org-ul">
<li>a:(N,)輸入的一維數組<br /></li>
<li>b:(M,)輸入的第二個一維數組<br /></li>
<li>mode:{&rsquo;full&rsquo;, &rsquo;valid&rsquo;, &rsquo;same&rsquo;}參數可選<br />
<ul class="org-ul">
<li>full　預設值，返回每一個卷積值，長度是 N+M-1,在卷積的邊緣處，信號不重疊，存在邊際效應。<br /></li>
</ul></li>
</ul>
<p>
　- same　返回的數組長度為 max(M, N),邊際效應依舊存在。<br />
　- valid 　返回的數組長度為 max(M,N)-min(M,N)+1,此時返回的是完全重疊的點。邊緣的點無效。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">-*- coding: utf-8 -*-</span>
<span class="linenr"> 2: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35299;&#27770;&#22294;&#24418;&#20013;&#25991;&#21839;&#38988;</span>
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> pylab <span style="color: #51afef;">import</span> *
<span class="linenr"> 4: </span>  mpl.<span style="color: #dcaeea;">rcParams</span>[<span style="color: #98be65;">'font.sans-serif'</span>] = [<span style="color: #98be65;">'SimHei'</span>] 
<span class="linenr"> 5: </span>  plt.<span style="color: #dcaeea;">rcParams</span>[<span style="color: #98be65;">'axes.unicode_minus'</span>]=<span style="color: #a9a1e1;">False</span>
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 8: </span>  <span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">a</span> = np.array([<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>])
<span class="linenr">11: </span>  <span style="color: #dcaeea;">b</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>])
<span class="linenr">12: </span>  <span style="color: #dcaeea;">y</span> = np.arange(<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">13: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"a: "</span>,a)
<span class="linenr">14: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"b: "</span>,b)
<span class="linenr">15: </span>  
<span class="linenr">16: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1.</span>
<span class="linenr">17: </span>  <span style="color: #51afef;">print</span>(a*b[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">18: </span>  <span style="color: #51afef;">print</span>(a*b[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">19: </span>  <span style="color: #51afef;">print</span>(a*b[<span style="color: #da8548; font-weight: bold;">2</span>])
<span class="linenr">20: </span>  plt.clf()
<span class="linenr">21: </span>  plt.xlim((-<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">5</span>))
<span class="linenr">22: </span>  plt.ylim((<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">5</span>))
<span class="linenr">23: </span>  plt.bar(y, a*b[<span style="color: #da8548; font-weight: bold;">0</span>], .<span style="color: #da8548; font-weight: bold;">5</span>, color=<span style="color: #98be65;">'blue'</span>)
<span class="linenr">24: </span>  plt.xlabel(<span style="color: #98be65;">'a[n]&#20056;&#20197;b[0]&#24460;&#24179;&#31227;&#33267;&#20301;&#32622;0'</span>);
<span class="linenr">25: </span>  plt.savefig(<span style="color: #98be65;">"anb0.png"</span>)
<span class="linenr">26: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">2.</span>
<span class="linenr">27: </span>  plt.clf() 
<span class="linenr">28: </span>  plt.xlim((-<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">5</span>))
<span class="linenr">29: </span>  plt.ylim((<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">5</span>))
<span class="linenr">30: </span>  plt.bar(y+<span style="color: #da8548; font-weight: bold;">1</span>, a*b[<span style="color: #da8548; font-weight: bold;">1</span>], .<span style="color: #da8548; font-weight: bold;">5</span>, color=<span style="color: #98be65;">'green'</span>)
<span class="linenr">31: </span>  plt.xlabel(<span style="color: #98be65;">'a[n]&#20056;&#20197;b[1]&#24460;&#24179;&#31227;&#33267;&#20301;&#32622;1'</span>);
<span class="linenr">32: </span>  plt.savefig(<span style="color: #98be65;">"anb1.png"</span>)
<span class="linenr">33: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">3. </span>
<span class="linenr">34: </span>  plt.clf()
<span class="linenr">35: </span>  plt.xlim((-<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">5</span>))
<span class="linenr">36: </span>  plt.ylim((<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">5</span>))
<span class="linenr">37: </span>  plt.bar(y+<span style="color: #da8548; font-weight: bold;">2</span>, a*b[<span style="color: #da8548; font-weight: bold;">2</span>], .<span style="color: #da8548; font-weight: bold;">5</span>, color=<span style="color: #98be65;">'orange'</span>)
<span class="linenr">38: </span>  plt.xlabel(<span style="color: #98be65;">'a[n]&#20056;&#20197;b[2]&#24460;&#24179;&#31227;&#33267;&#20301;&#32622;2'</span>);
<span class="linenr">39: </span>  plt.savefig(<span style="color: #98be65;">"anb2.png"</span>)
<span class="linenr">40: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Stack</span>
<span class="linenr">41: </span>  <span style="color: #dcaeea;">anb0</span> = [<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.2</span>, <span style="color: #da8548; font-weight: bold;">0.3</span>, <span style="color: #da8548; font-weight: bold;">0</span>,   <span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr">42: </span>  <span style="color: #dcaeea;">anb1</span> = [  <span style="color: #da8548; font-weight: bold;">0</span>,   <span style="color: #da8548; font-weight: bold;">1</span>,   <span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>,   <span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr">43: </span>  <span style="color: #dcaeea;">anb2</span> = [  <span style="color: #da8548; font-weight: bold;">0</span>,   <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1.5</span>]
<span class="linenr">44: </span>  plt.clf()
<span class="linenr">45: </span>  plt.xlim((-<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">5</span>))
<span class="linenr">46: </span>  plt.ylim((<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">5</span>))
<span class="linenr">47: </span>  plt.bar(np.arange(<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">5</span>), anb0, .<span style="color: #da8548; font-weight: bold;">5</span>, color=<span style="color: #98be65;">'blue'</span>)
<span class="linenr">48: </span>  plt.bar(np.arange(<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">5</span>), anb1, .<span style="color: #da8548; font-weight: bold;">5</span>, color=<span style="color: #98be65;">'green'</span>, bottom=anb0)
<span class="linenr">49: </span>  plt.bar(np.arange(<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">5</span>), anb2, .<span style="color: #da8548; font-weight: bold;">5</span>, color=<span style="color: #98be65;">'orange'</span>, bottom=np.add(anb0,anb1))
<span class="linenr">50: </span>  plt.xlabel(<span style="color: #98be65;">'&#23559;&#19977;&#22294;&#30090;&#21152;&#36215;&#20358;'</span>);
<span class="linenr">51: </span>
<span class="linenr">52: </span>  plt.savefig(<span style="color: #98be65;">"abStack.png"</span>)
<span class="linenr">53: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"Full:"</span>,np.convolve(a, b))
<span class="linenr">54: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"same:"</span>,np.convolve(a, b, <span style="color: #98be65;">'same'</span>))
<span class="linenr">55: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"valid:"</span>,np.convolve(a, b, <span style="color: #98be65;">'valid'</span>))
<span class="linenr">56: </span>
</pre>
</div>

<pre class="example">
a:  [1 2 3]
b:  [0.1 1.  0.5]
[0.1 0.2 0.3]
[1. 2. 3.]
[0.5 1.  1.5]
Full: [0.1 1.2 2.8 4.  1.5]
same: [1.2 2.8 4. ]
valid: [2.8]
</pre>


<p>
以矩陣 a = np.array([1, 2, 3]), b = np.array([0.1, 1, 0.5])為例，numpy.convolve 的計算過程如下：<br />
</p>
<ol class="org-ol">
<li>求出 a*b[0]，平移至位置 0，如圖[[fig:convolve-1]<br /></li>
</ol>

<div id="org353e106" class="figure">
<p><img src="images/anb0.png" alt="anb0.png" /><br />
</p>
<p><span class="figure-number">Figure 151: </span>convolve 的計算過程-1</p>
</div>
<ol class="org-ol">
<li>求出 a*b[1]，平移至位置 1，如圖<a href="#org6159f77">152</a><br /></li>
</ol>

<div id="org6159f77" class="figure">
<p><img src="images/anb1.png" alt="anb1.png" /><br />
</p>
<p><span class="figure-number">Figure 152: </span>convolve 的計算過程-2</p>
</div>
<ol class="org-ol">
<li>求出 a*b[2]，平移至位置 2，如圖<a href="#org760a198">153</a><br /></li>
</ol>

<div id="org760a198" class="figure">
<p><img src="images/anb2.png" alt="anb2.png" /><br />
</p>
<p><span class="figure-number">Figure 153: </span>convolve 的計算過程-1</p>
</div>
<ol class="org-ol">
<li>將圖<a href="#org353e106">151</a>，圖<a href="#org6159f77">152</a>，圖<a href="#org760a198">153</a>疊加起來<br /></li>
</ol>

<div id="org4524ab3" class="figure">
<p><img src="images/abStack.png" alt="abStack.png" /><br />
</p>
<p><span class="figure-number">Figure 154: </span>convolve 的計算過程-1</p>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-org7770535" class="outline-3">
<h3 id="org7770535"><span class="section-number-3">22.3</span> Numpy 檔案輸出輸入</h3>
<div class="outline-text-3" id="text-22-3">
</div>
<ol class="org-ol">
<li><a id="org17acd01"></a>Binary Format  <sup><a id="fnr.26" class="footref" href="#fn.26">26</a></sup><br />
<div class="outline-text-4" id="text-22-3-1">
<ul class="org-ul">
<li>save() / load()<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">2: </span><span style="color: #51afef;">import</span> subprocess
<span class="linenr">3: </span>
<span class="linenr">4: </span><span style="color: #dcaeea;">x</span> = np.arange(<span style="color: #da8548; font-weight: bold;">20</span>)
<span class="linenr">5: </span><span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#21407;&#22987;&#20839;&#23481;:"</span>, x)
<span class="linenr">6: </span>np.save(<span style="color: #98be65;">"test_array.npy"</span>, x) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">.npy</span>
<span class="linenr">7: </span>y = np.load(<span style="color: #98be65;">"test_array.npy"</span>)
<span class="linenr">8: </span><span style="color: #51afef;">print</span>(<span style="color: #98be65;">"&#35712;&#22238;&#20358;:"</span>, y)
</pre>
</div>

<pre class="example">
原始內容: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
讀回來: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
</pre>


<ul class="org-ul">
<li>savez(): 儲存多個陣列在一個 zip 的檔案中<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">2: </span><span style="color: #dcaeea;">aData</span> = [<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">3</span>,<span style="color: #da8548; font-weight: bold;">4</span>,<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">6</span>]
<span class="linenr">3: </span><span style="color: #dcaeea;">bData</span> = [<span style="color: #da8548; font-weight: bold;">7</span>,<span style="color: #da8548; font-weight: bold;">8</span>,<span style="color: #da8548; font-weight: bold;">9</span>,<span style="color: #da8548; font-weight: bold;">10</span>,<span style="color: #da8548; font-weight: bold;">11</span>,<span style="color: #da8548; font-weight: bold;">12</span>]
<span class="linenr">4: </span>np.savez(<span style="color: #98be65;">'my_archive.npz'</span>, a=aData, b=bData)
<span class="linenr">5: </span><span style="color: #dcaeea;">myArch</span> = np.load(<span style="color: #98be65;">'my_archive.npz'</span>)
<span class="linenr">6: </span><span style="color: #51afef;">print</span>(myArch[<span style="color: #98be65;">'a'</span>])
</pre>
</div>

<pre class="example">
[1 2 3 4 5 6]
</pre>
</div>
</li>

<li><a id="orgc73990d"></a>Text Files<br />
<div class="outline-text-4" id="text-22-3-2">
<ul class="org-ul">
<li>savetxt<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">2: </span>
<span class="linenr">3: </span>  <span style="color: #dcaeea;">ac</span> = np.array([np.arange(<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">6</span>),np.arange(<span style="color: #da8548; font-weight: bold;">10</span>,<span style="color: #da8548; font-weight: bold;">15</span>)])
<span class="linenr">4: </span>  np.savetxt(<span style="color: #98be65;">'txtfile.txt'</span>, ac)
<span class="linenr">5: </span>
<span class="linenr">6: </span>  <span style="color: #dcaeea;">myArr</span> = np.loadtxt(<span style="color: #98be65;">'txtfile.txt'</span>, delimiter=<span style="color: #98be65;">' '</span>)
<span class="linenr">7: </span>  <span style="color: #51afef;">print</span>(myArr)
</pre>
</div>

<pre class="example">
[[ 1.  2.  3.  4.  5.]
 [10. 11. 12. 13. 14.]]
</pre>


<ul class="org-ul">
<li>savez(): 儲存多個陣列在一個 zip 的檔案中<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">2: </span><span style="color: #dcaeea;">aData</span> = [<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">3</span>,<span style="color: #da8548; font-weight: bold;">4</span>,<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">6</span>]
<span class="linenr">3: </span><span style="color: #dcaeea;">bData</span> = [<span style="color: #da8548; font-weight: bold;">7</span>,<span style="color: #da8548; font-weight: bold;">8</span>,<span style="color: #da8548; font-weight: bold;">9</span>,<span style="color: #da8548; font-weight: bold;">10</span>,<span style="color: #da8548; font-weight: bold;">11</span>,<span style="color: #da8548; font-weight: bold;">12</span>]
<span class="linenr">4: </span>np.savez(<span style="color: #98be65;">'my_archive.npz'</span>, a=aData, b=bData)
<span class="linenr">5: </span><span style="color: #dcaeea;">myArch</span> = np.load(<span style="color: #98be65;">'my_archive.npz'</span>)
<span class="linenr">6: </span><span style="color: #51afef;">print</span>(myArch[<span style="color: #98be65;">'a'</span>])
</pre>
</div>

<pre class="example">
[1 2 3 4 5 6]
</pre>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgc7e07b5" class="outline-3">
<h3 id="orgc7e07b5"><span class="section-number-3">22.4</span> Data Visualization</h3>
<div class="outline-text-3" id="text-22-4">
</div>
<ol class="org-ol">
<li><a id="orga58fe1f"></a>pylab<br />
<div class="outline-text-4" id="text-22-4-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> pylab <span style="color: #51afef;">as</span> pl
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">x</span> = np.arange(<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">2.0</span>*np.pi, <span style="color: #da8548; font-weight: bold;">0.01</span>)
<span class="linenr"> 5: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"x:"</span>, x[:<span style="color: #da8548; font-weight: bold;">5</span>])
<span class="linenr"> 6: </span>  y = np.sin(x)
<span class="linenr"> 7: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"y:"</span>, y[:<span style="color: #da8548; font-weight: bold;">5</span>])
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  pl.plot(x,y)
<span class="linenr">10: </span>  pl.xlabel(<span style="color: #98be65;">'x'</span>)
<span class="linenr">11: </span>  pl.ylabel(<span style="color: #98be65;">'y'</span>)
<span class="linenr">12: </span>  pl.title(<span style="color: #98be65;">'sin'</span>)
<span class="linenr">13: </span>  pl.savefig(<span style="color: #98be65;">"sinPlot.png"</span>)
</pre>
</div>

<pre class="example">
x: [0.   0.01 0.02 0.03 0.04]
y: [0.         0.00999983 0.01999867 0.0299955  0.03998933]
</pre>


<div id="org8e97328" class="figure">
<p><img src="images/sinPlot.png" alt="sinPlot.png" /><br />
</p>
<p><span class="figure-number">Figure 155: </span>sinPlot</p>
</div>
</div>
</li>

<li><a id="org1e2dc76"></a>Bokeh<br />
<div class="outline-text-4" id="text-22-4-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> bokeh.plotting <span style="color: #51afef;">import</span> figure, show
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> bokeh.io <span style="color: #51afef;">import</span> output_notebook, export_png, output_file, save
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr"> 5: </span>  
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">N</span> = <span style="color: #da8548; font-weight: bold;">4000</span>
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">x</span> = np.random.random(size=N)*<span style="color: #da8548; font-weight: bold;">100</span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">y</span> = np.random.random(size=N)*<span style="color: #da8548; font-weight: bold;">100</span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">radii</span> = np.random.random(size=N)*<span style="color: #da8548; font-weight: bold;">1.5</span>
<span class="linenr">11: </span>  <span style="color: #dcaeea;">colors</span> = [<span style="color: #98be65;">"#%02x%02x%02x"</span> % (r, g, <span style="color: #da8548; font-weight: bold;">150</span>) <span style="color: #51afef;">for</span> r, g <span style="color: #51afef;">in</span> <span style="color: #c678dd;">zip</span>(np.floor(<span style="color: #da8548; font-weight: bold;">50</span>+<span style="color: #da8548; font-weight: bold;">2</span>*x).astype(<span style="color: #c678dd;">int</span>), np.floor(<span style="color: #da8548; font-weight: bold;">30</span>+<span style="color: #da8548; font-weight: bold;">2</span>*y).astype(<span style="color: #c678dd;">int</span>))]
<span class="linenr">12: </span>
<span class="linenr">13: </span>  output_notebook()
<span class="linenr">14: </span>  <span style="color: #dcaeea;">p</span> = figure(title=<span style="color: #98be65;">"Basic Title"</span>, plot_width=<span style="color: #da8548; font-weight: bold;">300</span>, plot_height=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">15: </span>  p.circle(x, y, radius=radii, fill_color=colors, fill_alpha=<span style="color: #da8548; font-weight: bold;">0.6</span>, line_color=<span style="color: #a9a1e1;">None</span>)
<span class="linenr">16: </span>  output_file(<span style="color: #98be65;">'bokehPlot.html'</span>, mode=<span style="color: #98be65;">'inline'</span>)
<span class="linenr">17: </span>  save(p)
</pre>
</div>

<p>
<a href="bokehPlot.htm">bokehPlot.htm</a><br />
</p>
</div>
</li>

<li><a id="org2754676"></a>seaborn<br />
<div class="outline-text-4" id="text-22-4-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> seaborn <span style="color: #51afef;">as</span> sns
<span class="linenr"> 2: </span>  sns.<span style="color: #c678dd;">set</span>(style=<span style="color: #98be65;">"ticks"</span>)
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">Load the example dataset for Anscombe's quartet</span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">df</span> = sns.load_dataset(<span style="color: #98be65;">"anscombe"</span>)
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Show the results of a linear regression within each dataset</span>
<span class="linenr"> 8: </span>  sns.lmplot(x=<span style="color: #98be65;">"x"</span>, y=<span style="color: #98be65;">"y"</span>, col=<span style="color: #98be65;">"dataset"</span>, hue=<span style="color: #98be65;">"dataset"</span>, data=df, col_wrap=<span style="color: #da8548; font-weight: bold;">2</span>, ci=<span style="color: #a9a1e1;">None</span>, palette=<span style="color: #98be65;">"muted"</span>, height=<span style="color: #da8548; font-weight: bold;">4</span>, scatter_kws={<span style="color: #98be65;">'s'</span> :<span style="color: #da8548; font-weight: bold;">50</span>,<span style="color: #98be65;">'alpha'</span>:<span style="color: #da8548; font-weight: bold;">1</span>})
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">11: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">sns.sns_plot.savefig("seabornPlot.png")</span>
<span class="linenr">12: </span>  plt.savefig(<span style="color: #98be65;">"seabornPlot.png"</span>)
</pre>
</div>


<div id="org4d93f4f" class="figure">
<p><img src="images/seabornPlot.png" alt="seabornPlot.png" /><br />
</p>
<p><span class="figure-number">Figure 156: </span>seabornPlot</p>
</div>
</div>
</li>

<li><a id="org1d92de8"></a>Altair<br />
<div class="outline-text-4" id="text-22-4-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> altair <span style="color: #51afef;">as</span> alt
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> vega_datasets <span style="color: #51afef;">import</span> data
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">cars</span> = data.cars()
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">chart</span> = alt.Chart(cars).mark_point().encode( x=<span style="color: #98be65;">'Horsepower'</span>, y=<span style="color: #98be65;">'Miles_per_Gallon'</span>, color=<span style="color: #98be65;">'Origin'</span>).interactive()
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">import pylab as pl</span>
<span class="linenr"> 9: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">pl.savefig("altairPlot.png")</span>
<span class="linenr">10: </span>  chart.save(<span style="color: #98be65;">'altairPlot.png'</span>, webdriver=<span style="color: #98be65;">'firefox'</span>)
<span class="linenr">11: </span>  chart.savechart(<span style="color: #98be65;">'altairPlot.html'</span>)
</pre>
</div>


<div id="orgc9e714d" class="figure">
<p><img src="images/altairPlot.png" alt="altairPlot.png" /><br />
</p>
<p><span class="figure-number">Figure 157: </span>altairPlot</p>
</div>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-orgcfb423d" class="outline-2">
<h2 id="orgcfb423d"><span class="section-number-2">23</span> Pandas</h2>
<div class="outline-text-2" id="text-23">
</div>
<div id="outline-container-orgeaee77a" class="outline-3">
<h3 id="orgeaee77a"><span class="section-number-3">23.1</span> Pandas 簡介<sup><a id="fnr.27" class="footref" href="#fn.27">27</a></sup></h3>
<div class="outline-text-3" id="text-23-1">
<p>
Pandas 是 python 的一個數據分析 lib，2009 年底開源出來，提供高效能、簡易使用的資料格式(Data Frame)讓使用者可以快速操作及分析資料，主要特色描述如下：<br />
</p>
<ol class="org-ol">
<li>在異質數據的讀取、轉換和處理上，都讓分析人員更容易處理，例如：從列欄試算表中找到想要的值。<br /></li>
<li>Pandas 提供兩種主要的資料結構，Series 與 DataFrame。Series 顧名思義就是用來處理時間序列相關的資料(如感測器資料等)，主要為建立索引的一維陣列。DataFrame 則是用來處理結構化(Table like)的資料，有列索引與欄標籤的二維資料集，例如關聯式資料庫、CSV 等等。<br /></li>
<li>透過載入至 Pandas 的資料結構物件後，可以透過結構化物件所提供的方法，來快速地進行資料的前處理，如資料補值，空值去除或取代等。<br /></li>
<li>更多的輸入來源及輸出整合性，例如：可以從資料庫讀取資料進入 Dataframe，也可將處理完的資料存回資料庫。<br /></li>
</ol>
</div>
</div>

<div id="outline-container-orgddc2121" class="outline-3">
<h3 id="orgddc2121"><span class="section-number-3">23.2</span> Pandas 提供的資料結構</h3>
<div class="outline-text-3" id="text-23-2">
<ol class="org-ol">
<li>Series：用來處理時間序列相關的資料(如感測器資料等)，主要為建立索引的一維陣列。<br /></li>
<li>DataFrame：用來處理結構化(Table like)的資料，有列索引與欄標籤的二維資料集，例如關聯式資料庫、CSV 等等。<br /></li>
<li>Panel：用來處理有資料及索引、列索引與欄標籤的三維資料集。<br /></li>
</ol>
</div>
</div>

<div id="outline-container-org7c26c8b" class="outline-3">
<h3 id="org7c26c8b"><span class="section-number-3">23.3</span> Pandas 實作</h3>
<div class="outline-text-3" id="text-23-3">
</div>
<ol class="org-ol">
<li><a id="orgb40c5ae"></a>檔案讀寫<br />
<div class="outline-text-4" id="text-23-3-1">
<ul class="org-ul">
<li>讀入<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">2: </span>
<span class="linenr">3: </span>  <span style="color: #dcaeea;">csv_file</span> = <span style="color: #98be65;">"https://storage.googleapis.com/learn_pd_like_tidyverse/gapminder.csv"</span>
<span class="linenr">4: </span>  <span style="color: #dcaeea;">gapminder</span> = pd.read_csv(csv_file)
<span class="linenr">5: </span>  <span style="color: #51afef;">print</span>(<span style="color: #c678dd;">type</span>(gapminder))
<span class="linenr">6: </span>  <span style="color: #51afef;">print</span>(gapminder.head())
</pre>
</div>

<pre class="example">
&lt;class 'pandas.core.frame.DataFrame'&gt;
       country continent  year  lifeExp       pop   gdpPercap
0  Afghanistan      Asia  1952   28.801   8425333  779.445314
1  Afghanistan      Asia  1957   30.332   9240934  820.853030
2  Afghanistan      Asia  1962   31.997  10267083  853.100710
3  Afghanistan      Asia  1967   34.020  11537966  836.197138
4  Afghanistan      Asia  1972   36.088  13079460  739.981106
</pre>


<ul class="org-ul">
<li>get remote csv to Google Colab<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">run on Google Colab</span>
<span class="linenr">2: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">3: </span>
<span class="linenr">4: </span>  <span style="color: #51afef;">from</span> google.colab <span style="color: #51afef;">import</span> files
<span class="linenr">5: </span>  <span style="color: #dcaeea;">uploaded</span> = files.upload()
<span class="linenr">6: </span>
<span class="linenr">7: </span>  <span style="color: #dcaeea;">stockIndexDataDf</span> = pd.read_csv(<span style="color: #98be65;">'./stock_index_data.csv'</span>)
<span class="linenr">8: </span>  <span style="color: #51afef;">print</span>(stockIndexDataDf)
</pre>
</div>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-org94df761" class="outline-2">
<h2 id="org94df761"><span class="section-number-2">24</span> AI v.s. security</h2>
<div class="outline-text-2" id="text-24">
</div>
<div id="outline-container-org402e0c1" class="outline-3">
<h3 id="org402e0c1"><span class="section-number-3">24.1</span> 釣魚網站偵測實戰</h3>
<div class="outline-text-3" id="text-24-1">
<p>
<a href="https://github.com/PacktPublishing/Mastering-Machine-Learning-for-Penetration-Testing/tree/master/Chapter02">github data</a><br />
</p>
</div>

<ol class="org-ol">
<li><a id="org0ba7d79"></a>資料集<br />
<div class="outline-text-4" id="text-24-1-1">
<p>
UCI Machine Learning Repository (Phishing Websites Data Set).<br />
<a href="https://archive.ics.uci.edu/ml/datasets/Phishing+Websites">https://archive.ics.uci.edu/ml/datasets/Phishing+Websites</a><br />
</p>

<p>
The dataset is provided as an arff file<br />
</p>

<p>
處理過的資料集:<br />
{30 Attributes (having_IP_Address URL_Length, abnormal_URL and so on)}+ {1 Attribute (Result)}<br />
</p>

<p>
-1,1,1,1,-1,-1,-1,-1,-1,1,1,-1,1,-1,1,-1,-1,-1,0,1,1,1,1,-1,-1,-1,-1,1,1,-1,-1<br />
</p>

<p>
真正要能上線跑的演算法不多，因為會面臨資料量太大(流量)的問題，會導致記憶體不足&#x2026;.<br />
</p>
</div>
</li>

<li><a id="org05a9340"></a>papers<br />
<div class="outline-text-4" id="text-24-1-2">
<ul class="org-ul">
<li>Mohammad, Rami, McCluskey, T.L. and Thabtah, Fadi (2012). An Assessment of Features Related to Phishing Websites using an Automated Technique. In: International Conferece For Internet Technology And Secured Transactions. ICITST 2012 . IEEE, London, UK, pp. 492-497. ISBN 978-1-4673-5325-0<br /></li>
<li>Mohammad, Rami, Thabtah, Fadi Abdeljaber and McCluskey, T.L. (2014). Predicting phishing websites based on self-structuring neural network. Neural Computing and Applications, 25 (2). pp. 443-458. ISSN 0941-0643<br /></li>
<li>Mohammad, Rami, McCluskey, T.L. and Thabtah, Fadi Abdeljaber (2014). Intelligent Rule based Phishing Websites Classification. IET Information Security, 8 (3). pp. 153-160. ISSN 1751-8709<br /></li>
</ul>
</div>
</li>

<li><a id="orga44cc58"></a>使用 LogisticRegression<br />
<div class="outline-text-4" id="text-24-1-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> *
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> sklearn.linear_model <span style="color: #51afef;">import</span> LogisticRegression
<span class="linenr"> 4: </span>  <span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> accuracy_score
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">training_data</span> = np.genfromtxt(<span style="color: #98be65;">'dataset.csv'</span>, delimiter=<span style="color: #98be65;">','</span>, dtype=np.int32)
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">inputs</span> = training_data[:,:-<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">outputs</span> = training_data[:, -<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">training_inputs</span> = inputs[:<span style="color: #da8548; font-weight: bold;">2000</span>] 
<span class="linenr">11: </span>  <span style="color: #dcaeea;">training_outputs</span> = outputs[:<span style="color: #da8548; font-weight: bold;">2000</span>] 
<span class="linenr">12: </span>  <span style="color: #dcaeea;">testing_inputs</span> = inputs[<span style="color: #da8548; font-weight: bold;">2000</span>:] 
<span class="linenr">13: </span>  <span style="color: #dcaeea;">testing_outputs</span> = outputs[<span style="color: #da8548; font-weight: bold;">2000</span>:]
<span class="linenr">14: </span>
<span class="linenr">15: </span>  <span style="color: #dcaeea;">classifier</span> = LogisticRegression()
<span class="linenr">16: </span>  classifier.fit(training_inputs, training_outputs)
<span class="linenr">17: </span>  <span style="color: #dcaeea;">predictions</span> = classifier.predict(testing_inputs)
<span class="linenr">18: </span>  <span style="color: #dcaeea;">accuracy</span> = <span style="color: #da8548; font-weight: bold;">100.0</span> * accuracy_score(testing_outputs, predictions)
<span class="linenr">19: </span>  <span style="color: #51afef;">print</span> (<span style="color: #98be65;">"The accuracy of your Logistic Regression on testing data is: "</span> + <span style="color: #c678dd;">str</span>(accuracy))
</pre>
</div>
</div>
</li>

<li><a id="org850e9f6"></a>使用 DecisionTreeClassifier<br />
<div class="outline-text-4" id="text-24-1-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> tree 
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> accuracy_score 
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">training_data</span> = np.genfromtxt(<span style="color: #98be65;">'dataset.csv'</span>, delimiter=<span style="color: #98be65;">','</span>, dtype=np.int32)
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">inputs</span> = training_data[:,:-<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">outputs</span> = training_data[:, -<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">training_inputs</span> = inputs[:<span style="color: #da8548; font-weight: bold;">2000</span>] 
<span class="linenr">10: </span>  <span style="color: #dcaeea;">training_outputs</span> = outputs[:<span style="color: #da8548; font-weight: bold;">2000</span>] 
<span class="linenr">11: </span>  <span style="color: #dcaeea;">testing_inputs</span> = inputs[<span style="color: #da8548; font-weight: bold;">2000</span>:] 
<span class="linenr">12: </span>  <span style="color: #dcaeea;">testing_outputs</span> = outputs[<span style="color: #da8548; font-weight: bold;">2000</span>:]
<span class="linenr">13: </span>
<span class="linenr">14: </span>  <span style="color: #dcaeea;">classifier</span> = tree.DecisionTreeClassifier()
<span class="linenr">15: </span>  classifier.fit(training_inputs, training_outputs)
<span class="linenr">16: </span>  <span style="color: #dcaeea;">predictions</span> = classifier.predict(testing_inputs)
<span class="linenr">17: </span>  <span style="color: #dcaeea;">accuracy</span> = <span style="color: #da8548; font-weight: bold;">100.0</span> * accuracy_score(testing_outputs, predictions)
<span class="linenr">18: </span>  <span style="color: #51afef;">print</span> (<span style="color: #98be65;">"The accuracy of your decision tree on testing data is: "</span> + <span style="color: #c678dd;">str</span>(accuracy))
</pre>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-orge122155" class="outline-3">
<h3 id="orge122155"><span class="section-number-3">24.2</span> Text Classification</h3>
<div class="outline-text-3" id="text-24-2">
<p>
<a href="https://github.com/MyDearGreatTeacher/TensorSecurity/blob/master/code/AI_security/3_TextClassification%E8%88%87%E5%9E%83%E5%9C%BE%E7%9F%AD%E4%BF%A1%E9%A0%90%E6%B8%AC.md">Text classification github</a><br />
</p>

<p>
二元分類: binary classification<br />
</p>

<p>
spam detection[email, SMS]<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org5226e27"></a>papers<br />
<div class="outline-text-4" id="text-24-2-1">
<ul class="org-ul">
<li>MS Spam Collection Dataset, Collection of SMS messages tagged as spam or legitimate, <a href="https://www.kaggle.com/uciml/sms-spam-collection-dataset/data">https://www.kaggle.com/uciml/sms-spam-collection-dataset/data</a><br /></li>
<li>The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.<br /></li>
<li>2009 年博士論文, A CORPUS LINGUISTICS STUDY OF SMS TEXT MESSAGING, CAROLINE TAGG, <a href="https://etheses.bham.ac.uk/id/eprint/253/1/Tagg09PhD.pdf">https://etheses.bham.ac.uk/id/eprint/253/1/Tagg09PhD.pdf</a><br /></li>
</ul>
</div>
</li>

<li><a id="orgdcf8745"></a>資料集<br />
<div class="outline-text-4" id="text-24-2-2">
<p>
<a href="https://github.com/CorkyMaigre/sms-spam-ml/blob/master/dataset/SMSSpamCollection">https://github.com/CorkyMaigre/sms-spam-ml/blob/master/dataset/SMSSpamCollection</a><br />
</p>

<div class="org-src-container">
<pre class="src src-sh">  ham   Go until jurong point, crazy.. Available only<span style="color: #51afef;"> in</span> bugis n great world la e buffet... Cine there got amore wat...
  ham   Ok lar... Joking wif u oni...
  spam  Free entry<span style="color: #51afef;"> in</span> <span style="color: #da8548; font-weight: bold;">2</span> a wkly comp to win FA Cup final tkts 21st May <span style="color: #da8548; font-weight: bold;">2005.</span> Text FA to <span style="color: #da8548; font-weight: bold;">87121</span> to receive entry question<span style="color: #51afef;">(</span>std txt rate<span style="color: #51afef;">)</span>T&amp;C<span style="color: #98be65;">'s apply 08452810075over18'</span>s
  ham   U dun say so early hor... U c already then say...
  ham   Nah I don<span style="color: #98be65;">'t think he goes to usf, he lives around here though</span>
<span style="color: #98be65;">  spam  FreeMsg Hey there darling it'</span>s been <span style="color: #da8548; font-weight: bold;">3</span> week<span style="color: #98be65;">'s now and no word back! I'</span>d like some fun you up for it still? Tb ok! XxX std chgs to send, &#163;1.50 to rcv
  ham   Even my brother is not like to speak with me. They treat me like aids patent.
  ham   As per your request <span style="color: #98be65;">'Melle Melle (Oru Minnaminunginte Nurungu Vettam)'</span> has been set as your callertune for all Callers. Press *9 to copy your friends Callertune
  spam  WINNER!! As a valued network customer you have been selected to receivea &#163;900 prize reward! To claim call <span style="color: #da8548; font-weight: bold;">09061701461.</span> Claim code KL341. Valid <span style="color: #da8548; font-weight: bold;">12</span> hours only.
  spam  Had your mobile <span style="color: #da8548; font-weight: bold;">11</span> months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on <span style="color: #da8548; font-weight: bold;">08002986030</span>
  ham   I<span style="color: #98be65;">'m gonna be home soon and i don'</span>t want to talk about this stuff anymore tonight, k? I<span style="color: #98be65;">'ve cried enough today.</span>
<span style="color: #98be65;">  spam  SIX chances to win CASH! From 100 to 20,000 pounds txt&gt; CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info</span>
<span style="color: #98be65;">  spam  URGENT! You have won a 1 week FREE membership in our &#163;100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&amp;C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18</span>
<span style="color: #98be65;">  ham   I'</span>ve been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.
  ham   I HAVE A DATE ON SUNDAY WITH WILL!!
  spam  XXXMobileMovieClub: To use your credit, click the WAP link<span style="color: #51afef;"> in</span> the next txt message or click here&gt;&gt; http://wap. xxxmobilemovieclub.com?<span style="color: #dcaeea;">n</span>=QJKGIGHJJGCBL
  ham   Oh k...i<span style="color: #98be65;">'m watching here:)</span>
<span style="color: #98be65;">  ham   Eh u remember how 2 spell his name... Yes i did. He v naughty </span><span style="color: #98be65;">make</span><span style="color: #98be65;"> until i v wet.</span>
<span style="color: #98be65;">  ham   Fine if that&#65533;s the way u feel. That&#65533;s the way its gota b</span>
<span style="color: #98be65;">  spam  England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/&#250;1.20 POBOXox36504W45WQ 16+</span>
<span style="color: #98be65;">  ham   Is that seriously how you spell his name?</span>
<span style="color: #98be65;">  ham   I&#8216;m going to try for 2 months ha ha only joking</span>
<span style="color: #98be65;">  ham   So &#252; pay first lar... Then when is da stock comin...</span>
<span style="color: #98be65;">  ham   Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?</span>
<span style="color: #98be65;">  ham   Ffffffffff. Alright no way I can meet up with you sooner?</span>
<span style="color: #98be65;">  ham   Just forced myself to eat a slice. I'</span>m really not hungry tho. This sucks. Mark is getting worried. He knows I<span style="color: #98be65;">'m sick when I turn down pizza. Lol</span>
<span style="color: #98be65;">  ham   Lol your always so convincing.</span>
<span style="color: #98be65;">  ham   Did you catch the bus ? Are you frying an egg ? Did you </span><span style="color: #98be65;">make</span><span style="color: #98be65;"> a tea? Are you eating your mom'</span>s left over dinner ? Do you feel my Love ?
  ham   I<span style="color: #98be65;">'m back &amp;amp; we'</span>re packing the car now, I<span style="color: #98be65;">'ll let you know if there'</span>s room
  ham   Ahhh. Work. I vaguely remember that! What does it feel like? Lol
  ham   Wait that<span style="color: #98be65;">'s still not all that clear, were you not sure about me being sarcastic or that that'</span>s why x doesn<span style="color: #98be65;">'t want to live with us</span>
<span style="color: #98be65;">  ham   Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child and he got caught up in that. Till 2! But we won'</span>t go there! Not doing too badly cheers. You? 

</pre>
</div>
</div>
</li>

<li><a id="orgffd99e0"></a>使用 LogisticRegression<br />
<div class="outline-text-4" id="text-24-2-3">
<ul class="org-ul">
<li>Hands-on-Machine-Learning-for-Cyber-Security/Chapter05/sms_spam.py /<br /></li>
<li><a href="https://github.com/PacktPublishing/Hands-on-Machine-Learning-for-Cyber-Security/blob/master/Chapter05/sms_spam.py">https://github.com/PacktPublishing/Hands-on-Machine-Learning-for-Cyber-Security/blob/master/Chapter05/sms_spam.py</a><br /></li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> sklearn.feature_extraction.text <span style="color: #51afef;">import</span> TfidfVectorizer
<span class="linenr"> 4: </span>  <span style="color: #51afef;">from</span> sklearn.linear_model.logistic <span style="color: #51afef;">import</span> LogisticRegression
<span class="linenr"> 5: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split, cross_val_score
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">dataframe</span> = pd.read_csv(<span style="color: #98be65;">'SMSSpamCollectionDataSet'</span>, delimiter=<span style="color: #98be65;">'\t'</span>,header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">X_train_dataset</span>, <span style="color: #dcaeea;">X_test_dataset</span>, <span style="color: #dcaeea;">y_train_dataset</span>, <span style="color: #dcaeea;">y_test_dataset</span> = train_test_split(dataframe[<span style="color: #da8548; font-weight: bold;">1</span>],dataframe[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #dcaeea;">vectorizer</span> = TfidfVectorizer()
<span class="linenr">12: </span>  <span style="color: #dcaeea;">X_train_dataset</span> = vectorizer.fit_transform(X_train_dataset)
<span class="linenr">13: </span>
<span class="linenr">14: </span>  <span style="color: #dcaeea;">classifier_log</span> = LogisticRegression()
<span class="linenr">15: </span>  classifier_log.fit(X_train_dataset, y_train_dataset)
<span class="linenr">16: </span>
<span class="linenr">17: </span>  <span style="color: #dcaeea;">X_test_dataset</span> = vectorizer.transform( [<span style="color: #98be65;">'URGENT! Your Mobile No 1234 was awarded a Prize'</span>, <span style="color: #98be65;">'Hey honey, whats up?'</span>] )
<span class="linenr">18: </span>
<span class="linenr">19: </span>  <span style="color: #dcaeea;">predictions_logistic</span> = classifier.predict(X_test_dataset)
<span class="linenr">20: </span>  <span style="color: #51afef;">print</span>(predictions)
</pre>
</div>
</div>
</li>

<li><a id="org775925b"></a>TensorFlow_RNN for 垃圾短信預測<br />
<div class="outline-text-4" id="text-24-2-4">
<p>
TensorFlow 機器學習實戰指南 (美)尼克‧麥克盧爾<br />
 9.2 用 TensorFlow 實現 RNN 模型進行垃圾短信預測<br />
 <a href="https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook-Second-Edition">https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook-Second-Edition</a><br />
</p>


<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #51afef;">import</span> os
<span class="linenr">  2: </span>  <span style="color: #51afef;">import</span> re
<span class="linenr">  3: </span>  <span style="color: #51afef;">import</span> io
<span class="linenr">  4: </span>  <span style="color: #51afef;">import</span> requests
<span class="linenr">  5: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  6: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">  7: </span>  <span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr">  8: </span>  <span style="color: #51afef;">from</span> zipfile <span style="color: #51afef;">import</span> ZipFile
<span class="linenr">  9: </span>  <span style="color: #51afef;">from</span> tensorflow.python.framework <span style="color: #51afef;">import</span> ops
<span class="linenr"> 10: </span>  ops.reset_default_graph()
<span class="linenr"> 11: </span>
<span class="linenr"> 12: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Start a graph</span>
<span class="linenr"> 13: </span>  <span style="color: #dcaeea;">sess</span> = tf.Session()
<span class="linenr"> 14: </span>
<span class="linenr"> 15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Set RNN parameters</span>
<span class="linenr"> 16: </span>  <span style="color: #dcaeea;">epochs</span> = <span style="color: #da8548; font-weight: bold;">20</span>
<span class="linenr"> 17: </span>  <span style="color: #dcaeea;">batch_size</span> = <span style="color: #da8548; font-weight: bold;">250</span>
<span class="linenr"> 18: </span>  <span style="color: #dcaeea;">max_sequence_length</span> = <span style="color: #da8548; font-weight: bold;">25</span>
<span class="linenr"> 19: </span>  <span style="color: #dcaeea;">rnn_size</span> = <span style="color: #da8548; font-weight: bold;">10</span>
<span class="linenr"> 20: </span>  <span style="color: #dcaeea;">embedding_size</span> = <span style="color: #da8548; font-weight: bold;">50</span>
<span class="linenr"> 21: </span>  <span style="color: #dcaeea;">min_word_frequency</span> = <span style="color: #da8548; font-weight: bold;">10</span>
<span class="linenr"> 22: </span>  <span style="color: #dcaeea;">learning_rate</span> = <span style="color: #da8548; font-weight: bold;">0.0005</span>
<span class="linenr"> 23: </span>  <span style="color: #dcaeea;">dropout_keep_prob</span> = tf.placeholder(tf.float32)
<span class="linenr"> 24: </span>
<span class="linenr"> 25: </span>
<span class="linenr"> 26: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Download or open data</span>
<span class="linenr"> 27: </span>  <span style="color: #dcaeea;">data_dir</span> = <span style="color: #98be65;">'temp'</span>
<span class="linenr"> 28: </span>  <span style="color: #dcaeea;">data_file</span> = <span style="color: #98be65;">'text_data.txt'</span>
<span class="linenr"> 29: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.exists(data_dir):
<span class="linenr"> 30: </span>      os.makedirs(data_dir)
<span class="linenr"> 31: </span>
<span class="linenr"> 32: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isfile(os.path.join(data_dir, data_file)):
<span class="linenr"> 33: </span>      <span style="color: #dcaeea;">zip_url</span> = <span style="color: #98be65;">'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'</span>
<span class="linenr"> 34: </span>      <span style="color: #dcaeea;">r</span> = requests.get(zip_url)
<span class="linenr"> 35: </span>      <span style="color: #dcaeea;">z</span> = ZipFile(io.BytesIO(r.content))
<span class="linenr"> 36: </span>      <span style="color: #c678dd;">file</span> = z.read(<span style="color: #98be65;">'SMSSpamCollection'</span>)
<span class="linenr"> 37: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Format Data</span>
<span class="linenr"> 38: </span>      <span style="color: #dcaeea;">text_data</span> = <span style="color: #c678dd;">file</span>.decode()
<span class="linenr"> 39: </span>      <span style="color: #dcaeea;">text_data</span> = text_data.encode(<span style="color: #98be65;">'ascii'</span>, errors=<span style="color: #98be65;">'ignore'</span>)
<span class="linenr"> 40: </span>      <span style="color: #dcaeea;">text_data</span> = text_data.decode().split(<span style="color: #98be65;">'\n'</span>)
<span class="linenr"> 41: </span>
<span class="linenr"> 42: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Save data to text file</span>
<span class="linenr"> 43: </span>      <span style="color: #51afef;">with</span> <span style="color: #c678dd;">open</span>(os.path.join(data_dir, data_file), <span style="color: #98be65;">'w'</span>) <span style="color: #51afef;">as</span> <span style="color: #dcaeea;">file_conn</span>:
<span class="linenr"> 44: </span>          <span style="color: #51afef;">for</span> text <span style="color: #51afef;">in</span> text_data:
<span class="linenr"> 45: </span>              file_conn.write(<span style="color: #98be65;">"{}\n"</span>.<span style="color: #c678dd;">format</span>(text))
<span class="linenr"> 46: </span>  <span style="color: #51afef;">else</span>:
<span class="linenr"> 47: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Open data from text file</span>
<span class="linenr"> 48: </span>      text_data = []
<span class="linenr"> 49: </span>      <span style="color: #51afef;">with</span> <span style="color: #c678dd;">open</span>(os.path.join(data_dir, data_file), <span style="color: #98be65;">'r'</span>) <span style="color: #51afef;">as</span> <span style="color: #dcaeea;">file_conn</span>:
<span class="linenr"> 50: </span>          <span style="color: #51afef;">for</span> row <span style="color: #51afef;">in</span> file_conn:
<span class="linenr"> 51: </span>              text_data.append(row)
<span class="linenr"> 52: </span>      text_data = text_data[:-<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr"> 53: </span>
<span class="linenr"> 54: </span>  <span style="color: #dcaeea;">text_data</span> = [x.split(<span style="color: #98be65;">'\t'</span>) <span style="color: #51afef;">for</span> x <span style="color: #51afef;">in</span> text_data <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(x) &gt;= <span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr"> 55: </span>  [text_data_target, text_data_train] = [<span style="color: #c678dd;">list</span>(x) <span style="color: #51afef;">for</span> x <span style="color: #51afef;">in</span> <span style="color: #c678dd;">zip</span>(*text_data)]
<span class="linenr"> 56: </span>
<span class="linenr"> 57: </span>
<span class="linenr"> 58: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Create a text cleaning function</span>
<span class="linenr"> 59: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">clean_text</span>(text_string):
<span class="linenr"> 60: </span>      <span style="color: #dcaeea;">text_string</span> = re.sub(r<span style="color: #98be65;">'([^\s\w]|_|[0-9])+'</span>, <span style="color: #98be65;">''</span>, text_string)
<span class="linenr"> 61: </span>      <span style="color: #dcaeea;">text_string</span> = <span style="color: #98be65;">" "</span>.join(text_string.split())
<span class="linenr"> 62: </span>      <span style="color: #dcaeea;">text_string</span> = text_string.lower()
<span class="linenr"> 63: </span>      <span style="color: #51afef;">return</span> text_string
<span class="linenr"> 64: </span>
<span class="linenr"> 65: </span>
<span class="linenr"> 66: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Clean texts</span>
<span class="linenr"> 67: </span>  <span style="color: #dcaeea;">text_data_train</span> = [clean_text(x) <span style="color: #51afef;">for</span> x <span style="color: #51afef;">in</span> text_data_train]
<span class="linenr"> 68: </span>
<span class="linenr"> 69: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Change texts into numeric vectors</span>
<span class="linenr"> 70: </span>  <span style="color: #dcaeea;">vocab_processor</span> = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sequence_length,
<span class="linenr"> 71: </span>                                                                       min_frequency=min_word_frequency)
<span class="linenr"> 72: </span>  <span style="color: #dcaeea;">text_processed</span> = np.array(<span style="color: #c678dd;">list</span>(vocab_processor.fit_transform(text_data_train)))
<span class="linenr"> 73: </span>
<span class="linenr"> 74: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Shuffle and split data</span>
<span class="linenr"> 75: </span>  <span style="color: #dcaeea;">text_processed</span> = np.array(text_processed)
<span class="linenr"> 76: </span>  <span style="color: #dcaeea;">text_data_target</span> = np.array([<span style="color: #da8548; font-weight: bold;">1</span> <span style="color: #51afef;">if</span> x == <span style="color: #98be65;">'ham'</span> <span style="color: #51afef;">else</span> <span style="color: #da8548; font-weight: bold;">0</span> <span style="color: #51afef;">for</span> x <span style="color: #51afef;">in</span> text_data_target])
<span class="linenr"> 77: </span>  <span style="color: #dcaeea;">shuffled_ix</span> = np.random.permutation(np.arange(<span style="color: #c678dd;">len</span>(text_data_target)))
<span class="linenr"> 78: </span>  <span style="color: #dcaeea;">x_shuffled</span> = text_processed[shuffled_ix]
<span class="linenr"> 79: </span>  <span style="color: #dcaeea;">y_shuffled</span> = text_data_target[shuffled_ix]
<span class="linenr"> 80: </span>
<span class="linenr"> 81: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Split train/test set</span>
<span class="linenr"> 82: </span>  <span style="color: #dcaeea;">ix_cutoff</span> = <span style="color: #c678dd;">int</span>(<span style="color: #c678dd;">len</span>(y_shuffled)*<span style="color: #da8548; font-weight: bold;">0.80</span>)
<span class="linenr"> 83: </span>  <span style="color: #dcaeea;">x_train</span>, <span style="color: #dcaeea;">x_test</span> = x_shuffled[:ix_cutoff], x_shuffled[<span style="color: #dcaeea;">ix_cutoff</span>:]
<span class="linenr"> 84: </span>  <span style="color: #dcaeea;">y_train</span>, y_test = y_shuffled[:ix_cutoff], y_shuffled[<span style="color: #dcaeea;">ix_cutoff</span>:]
<span class="linenr"> 85: </span>  vocab_size = <span style="color: #c678dd;">len</span>(vocab_processor.vocabulary_)
<span class="linenr"> 86: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"Vocabulary Size: {:d}"</span>.<span style="color: #c678dd;">format</span>(vocab_size))
<span class="linenr"> 87: </span>  <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"80-20 Train Test split: {:d} -- {:d}"</span>.<span style="color: #c678dd;">format</span>(<span style="color: #c678dd;">len</span>(y_train), <span style="color: #c678dd;">len</span>(y_test)))
<span class="linenr"> 88: </span>
<span class="linenr"> 89: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Create placeholders</span>
<span class="linenr"> 90: </span>  <span style="color: #dcaeea;">x_data</span> = tf.placeholder(tf.int32, [<span style="color: #a9a1e1;">None</span>, max_sequence_length])
<span class="linenr"> 91: </span>  <span style="color: #dcaeea;">y_output</span> = tf.placeholder(tf.int32, [<span style="color: #a9a1e1;">None</span>])
<span class="linenr"> 92: </span>
<span class="linenr"> 93: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Create embedding</span>
<span class="linenr"> 94: </span>  <span style="color: #dcaeea;">embedding_mat</span> = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -<span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>))
<span class="linenr"> 95: </span>  <span style="color: #dcaeea;">embedding_output</span> = tf.nn.embedding_lookup(embedding_mat, x_data)
<span class="linenr"> 96: </span>
<span class="linenr"> 97: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Define the RNN cell</span>
<span class="linenr"> 98: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">tensorflow change &gt;= 1.0, rnn is put into tensorflow.contrib directory. Prior version not test.</span>
<span class="linenr"> 99: </span>  <span style="color: #51afef;">if</span> tf.__version__[<span style="color: #da8548; font-weight: bold;">0</span>] &gt;= <span style="color: #98be65;">'1'</span>:
<span class="linenr">100: </span>      <span style="color: #dcaeea;">cell</span> = tf.contrib.rnn.BasicRNNCell(num_units=rnn_size)
<span class="linenr">101: </span>  <span style="color: #51afef;">else</span>:
<span class="linenr">102: </span>      cell = tf.nn.rnn_cell.BasicRNNCell(num_units=rnn_size)
<span class="linenr">103: </span>
<span class="linenr">104: </span>  <span style="color: #dcaeea;">output</span>, <span style="color: #dcaeea;">state</span> = tf.nn.dynamic_rnn(cell, embedding_output, dtype=tf.float32)
<span class="linenr">105: </span>  <span style="color: #dcaeea;">output</span> = tf.nn.dropout(output, dropout_keep_prob)
<span class="linenr">106: </span>
<span class="linenr">107: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Get output of RNN sequence</span>
<span class="linenr">108: </span>  <span style="color: #dcaeea;">output</span> = tf.transpose(output, [<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">2</span>])
<span class="linenr">109: </span>  <span style="color: #dcaeea;">last</span> = tf.gather(output, <span style="color: #c678dd;">int</span>(output.get_shape()[<span style="color: #da8548; font-weight: bold;">0</span>]) - <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">110: </span>
<span class="linenr">111: </span>  <span style="color: #dcaeea;">weight</span> = tf.Variable(tf.truncated_normal([rnn_size, <span style="color: #da8548; font-weight: bold;">2</span>], stddev=<span style="color: #da8548; font-weight: bold;">0.1</span>))
<span class="linenr">112: </span>  <span style="color: #dcaeea;">bias</span> = tf.Variable(tf.constant(<span style="color: #da8548; font-weight: bold;">0.1</span>, shape=[<span style="color: #da8548; font-weight: bold;">2</span>]))
<span class="linenr">113: </span>  <span style="color: #dcaeea;">logits_out</span> = tf.matmul(last, weight) + bias
<span class="linenr">114: </span>
<span class="linenr">115: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Loss function</span>
<span class="linenr">116: </span>  <span style="color: #dcaeea;">losses</span> = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_out, labels=y_output)
<span class="linenr">117: </span>  <span style="color: #dcaeea;">loss</span> = tf.reduce_mean(losses)
<span class="linenr">118: </span>
<span class="linenr">119: </span>  <span style="color: #dcaeea;">accuracy</span> = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits_out, <span style="color: #da8548; font-weight: bold;">1</span>), tf.cast(y_output, tf.int64)), tf.float32))
<span class="linenr">120: </span>
<span class="linenr">121: </span>  <span style="color: #dcaeea;">optimizer</span> = tf.train.RMSPropOptimizer(learning_rate)
<span class="linenr">122: </span>  <span style="color: #dcaeea;">train_step</span> = optimizer.minimize(loss)
<span class="linenr">123: </span>
<span class="linenr">124: </span>  <span style="color: #dcaeea;">init</span> = tf.global_variables_initializer()
<span class="linenr">125: </span>  sess.run(init)
<span class="linenr">126: </span>
<span class="linenr">127: </span>  <span style="color: #dcaeea;">train_loss</span> = []
<span class="linenr">128: </span>  <span style="color: #dcaeea;">test_loss</span> = []
<span class="linenr">129: </span>  <span style="color: #dcaeea;">train_accuracy</span> = []
<span class="linenr">130: </span>  <span style="color: #dcaeea;">test_accuracy</span> = []
<span class="linenr">131: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Start training</span>
<span class="linenr">132: </span>  <span style="color: #51afef;">for</span> epoch <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(epochs):
<span class="linenr">133: </span>
<span class="linenr">134: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Shuffle training data</span>
<span class="linenr">135: </span>      <span style="color: #dcaeea;">shuffled_ix</span> = np.random.permutation(np.arange(<span style="color: #c678dd;">len</span>(x_train)))
<span class="linenr">136: </span>      <span style="color: #dcaeea;">x_train</span> = x_train[shuffled_ix]
<span class="linenr">137: </span>      <span style="color: #dcaeea;">y_train</span> = y_train[shuffled_ix]
<span class="linenr">138: </span>      <span style="color: #dcaeea;">num_batches</span> = <span style="color: #c678dd;">int</span>(<span style="color: #c678dd;">len</span>(x_train)/batch_size) + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">139: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">TO DO CALCULATE GENERATIONS ExACTLY</span>
<span class="linenr">140: </span>      <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(num_batches):
<span class="linenr">141: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Select train data</span>
<span class="linenr">142: </span>          <span style="color: #dcaeea;">min_ix</span> = i * batch_size
<span class="linenr">143: </span>          <span style="color: #dcaeea;">max_ix</span> = np.<span style="color: #c678dd;">min</span>([<span style="color: #c678dd;">len</span>(x_train), ((i+<span style="color: #da8548; font-weight: bold;">1</span>) * batch_size)])
<span class="linenr">144: </span>          <span style="color: #dcaeea;">x_train_batch</span> = x_train[<span style="color: #dcaeea;">min_ix</span>:max_ix]
<span class="linenr">145: </span>          y_train_batch = y_train[<span style="color: #dcaeea;">min_ix</span>:max_ix]
<span class="linenr">146: </span>        
<span class="linenr">147: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Run train step</span>
<span class="linenr">148: </span>          train_dict = {x_data: x_train_batch, y_output: y_train_batch, dropout_keep_prob:<span style="color: #da8548; font-weight: bold;">0.5</span>}
<span class="linenr">149: </span>          sess.run(train_step, feed_dict=train_dict)
<span class="linenr">150: </span>        
<span class="linenr">151: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Run loss and accuracy for training</span>
<span class="linenr">152: </span>      <span style="color: #dcaeea;">temp_train_loss</span>, <span style="color: #dcaeea;">temp_train_acc</span> = sess.run([loss, accuracy], feed_dict=train_dict)
<span class="linenr">153: </span>      train_loss.append(temp_train_loss)
<span class="linenr">154: </span>      train_accuracy.append(temp_train_acc)
<span class="linenr">155: </span>    
<span class="linenr">156: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Run Eval Step</span>
<span class="linenr">157: </span>      <span style="color: #dcaeea;">test_dict</span> = {<span style="color: #dcaeea;">x_data</span>: x_test, y_output: y_test, dropout_keep_prob:<span style="color: #da8548; font-weight: bold;">1.0</span>}
<span class="linenr">158: </span>      <span style="color: #dcaeea;">temp_test_loss</span>, temp_test_acc = sess.run([loss, accuracy], feed_dict=test_dict)
<span class="linenr">159: </span>      test_loss.append(temp_test_loss)
<span class="linenr">160: </span>      test_accuracy.append(temp_test_acc)
<span class="linenr">161: </span>      <span style="color: #51afef;">print</span>(<span style="color: #98be65;">'Epoch: {}, Test Loss: {:.2}, Test Acc: {:.2}'</span>.<span style="color: #c678dd;">format</span>(epoch+<span style="color: #da8548; font-weight: bold;">1</span>, temp_test_loss, temp_test_acc))
<span class="linenr">162: </span>    
<span class="linenr">163: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Plot loss over time</span>
<span class="linenr">164: </span>  <span style="color: #dcaeea;">epoch_seq</span> = np.arange(<span style="color: #da8548; font-weight: bold;">1</span>, epochs+<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">165: </span>  plt.plot(epoch_seq, train_loss, <span style="color: #98be65;">'k--'</span>, label=<span style="color: #98be65;">'Train Set'</span>)
<span class="linenr">166: </span>  plt.plot(epoch_seq, test_loss, <span style="color: #98be65;">'r-'</span>, label=<span style="color: #98be65;">'Test Set'</span>)
<span class="linenr">167: </span>  plt.title(<span style="color: #98be65;">'Softmax Loss'</span>)
<span class="linenr">168: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">169: </span>  plt.ylabel(<span style="color: #98be65;">'Softmax Loss'</span>)
<span class="linenr">170: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">171: </span>  plt.show()
<span class="linenr">172: </span>
<span class="linenr">173: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Plot accuracy over time</span>
<span class="linenr">174: </span>  plt.plot(epoch_seq, train_accuracy, <span style="color: #98be65;">'k--'</span>, label=<span style="color: #98be65;">'Train Set'</span>)
<span class="linenr">175: </span>  plt.plot(epoch_seq, test_accuracy, <span style="color: #98be65;">'r-'</span>, label=<span style="color: #98be65;">'Test Set'</span>)
<span class="linenr">176: </span>  plt.title(<span style="color: #98be65;">'Test Accuracy'</span>)
<span class="linenr">177: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">178: </span>  plt.ylabel(<span style="color: #98be65;">'Accuracy'</span>)
<span class="linenr">179: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">180: </span>  plt.show()
</pre>
</div>
</div>
</li>
</ol>
</div>


<div id="outline-container-orge1012d0" class="outline-3">
<h3 id="orge1012d0"><span class="section-number-3">24.3</span> AI and Botnet Detection</h3>
<div class="outline-text-3" id="text-24-3">
<p>
<a href="https://github.com/MyDearGreatTeacher/TensorSecurity/tree/master/code/AI_security/%E7%99%BC%E5%B1%95%E8%B6%A8%E5%8B%A2/Botnet">Botnet github</a><br />
IOT honey pot<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org69fe80b"></a>案例分析<br />
<div class="outline-text-4" id="text-24-3-1">
<p>
Hands-On Artificial Intelligence for Cybersecurity<br />
Alessandro Parisi<br />
</p>
</div>
</li>

<li><a id="orge89457b"></a>資料集<br />
<div class="outline-text-4" id="text-24-3-2">
<p>
<a href="https://github.com/MyDearGreatTeacher/AI201909/blob/master/data/network-logs.csv">https://github.com/MyDearGreatTeacher/AI201909/blob/master/data/network-logs.csv</a><br />
</p>

<p>
!wget <a href="https://raw.githubusercontent.com/MyDearGreatTeacher/AI201909/master/data/network-logs.csv">https://raw.githubusercontent.com/MyDearGreatTeacher/AI201909/master/data/network-logs.csv</a><br />
</p>



<div class="org-src-container">
<pre class="src src-csv">  REMOTE_PORT<span style="color: #00ffff;">   </span>LATENCY<span style="color: #00ffff;"> </span>THROUGHPUT<span style="color: #00ffff;">  </span>ANOMALY
  21<span style="color: #00ffff;">    </span>15.94287532<span style="color: #00ffff;"> </span>16.20299807<span style="color: #00ffff;"> </span>0
  20<span style="color: #00ffff;">    </span>12.66645095<span style="color: #00ffff;"> </span>15.89908374<span style="color: #00ffff;"> </span>1
  80<span style="color: #00ffff;">    </span>13.89454962<span style="color: #00ffff;"> </span>12.95800822<span style="color: #00ffff;"> </span>0
  21<span style="color: #00ffff;">    </span>13.62081292<span style="color: #00ffff;"> </span>15.45947525<span style="color: #00ffff;"> </span>0
  21<span style="color: #00ffff;">    </span>15.70548485<span style="color: #00ffff;"> </span>15.33956527<span style="color: #00ffff;"> </span>0
  23<span style="color: #00ffff;">    </span>15.59318973<span style="color: #00ffff;"> </span>15.61238106<span style="color: #00ffff;"> </span>0
  21<span style="color: #00ffff;">    </span>15.48906755<span style="color: #00ffff;"> </span>15.64087368<span style="color: #00ffff;"> </span>0
  80<span style="color: #00ffff;">    </span>15.52704801<span style="color: #00ffff;"> </span>15.63568031<span style="color: #00ffff;"> </span>0
  21<span style="color: #00ffff;">    </span>14.07506707<span style="color: #00ffff;"> </span>15.76531533<span style="color: #00ffff;"> </span>0
  ......
</pre>
</div>

<div class="org-src-container">
<pre class="src src-csv">  &#24310;&#36978;&#65288;Latency&#65289;&#65306;&#19968;&#20491;&#23553;&#21253;&#24478;&#20358;&#28304;&#31471;&#36865;&#20986;&#24460;&#65292;&#21040;&#30446;&#30340;&#31471;&#25509;&#25910;&#21040;&#36889;&#20491;&#23553;&#21253;&#65292;&#20013;&#38291;&#25152;&#33457;&#30340;&#26178;&#38291;&#12290;
  &#38971;&#23532;&#65288;Bandwidth&#65289;&#65306;&#20659;&#36664;&#23186;&#20171;&#30340;&#26368;&#22823;&#21534;&#21520;&#37327;&#65288;throughput&#65289;&#12290;

  https://blog.gtwang.org/web-development/network-lantency-and-bandwidth/
</pre>
</div>
</div>
</li>

<li><a id="orga4cadb0"></a>基本統計分析<br />
<div class="outline-text-4" id="text-24-3-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  !wget https://raw.githubusercontent.com/MyDearGreatTeacher/AI201909/master/data/network-logs.csv
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 5: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 6: </span>  %matplotlib inline
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">dataset</span> = pd.read_csv(<span style="color: #98be65;">'network-logs.csv'</span>)
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">hist_dist</span> = dataset[[<span style="color: #98be65;">'LATENCY'</span>, <span style="color: #98be65;">'THROUGHPUT'</span>]].hist(grid=<span style="color: #a9a1e1;">False</span>, figsize=(<span style="color: #da8548; font-weight: bold;">10</span>,<span style="color: #da8548; font-weight: bold;">4</span>))
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #dcaeea;">data</span> = dataset[[<span style="color: #98be65;">'LATENCY'</span>, <span style="color: #98be65;">'THROUGHPUT'</span>]].values
<span class="linenr">12: </span>
<span class="linenr">13: </span>  plt.scatter(data[:, <span style="color: #da8548; font-weight: bold;">0</span>], data[:, <span style="color: #da8548; font-weight: bold;">1</span>], alpha=<span style="color: #da8548; font-weight: bold;">0.6</span>)
<span class="linenr">14: </span>  plt.xlabel(<span style="color: #98be65;">'LATENCY'</span>)
<span class="linenr">15: </span>  plt.ylabel(<span style="color: #98be65;">'THROUGHPUT'</span>)
<span class="linenr">16: </span>  plt.title(<span style="color: #98be65;">'DATA FLOW'</span>)
<span class="linenr">17: </span>  plt.show()
</pre>
</div>
</div>
</li>

<li><a id="org3df7a15"></a>機器學習<br />
<div class="outline-text-4" id="text-24-3-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #51afef;">from</span> sklearn.linear_model <span style="color: #51afef;">import</span> *
<span class="linenr"> 5: </span>  <span style="color: #51afef;">from</span> sklearn.tree <span style="color: #51afef;">import</span> *
<span class="linenr"> 6: </span>  <span style="color: #51afef;">from</span> sklearn.naive_bayes <span style="color: #51afef;">import</span> *
<span class="linenr"> 7: </span>  <span style="color: #51afef;">from</span> sklearn.neighbors <span style="color: #51afef;">import</span> *
<span class="linenr"> 8: </span>  <span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> accuracy_score
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">11: </span>
<span class="linenr">12: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">13: </span>  %matplotlib inline
<span class="linenr">14: </span>
<span class="linenr">15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Load the data.</span>
<span class="linenr">16: </span>  <span style="color: #dcaeea;">dataset</span> = pd.read_csv(<span style="color: #98be65;">'network-logs.csv'</span>)
<span class="linenr">17: </span>
<span class="linenr">18: </span>
<span class="linenr">19: </span>  <span style="color: #dcaeea;">samples</span> = dataset.iloc[:, [<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>]].values <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#21482;&#21462;&#31532;1&#12289;2&#27396;&#30340;&#36039;&#26009;&#30070;features</span>
<span class="linenr">20: </span>  <span style="color: #dcaeea;">targets</span> = dataset[<span style="color: #98be65;">'ANOMALY'</span>].values
<span class="linenr">21: </span>
<span class="linenr">22: </span>  <span style="color: #dcaeea;">training_samples</span>, <span style="color: #dcaeea;">testing_samples</span>, <span style="color: #dcaeea;">training_targets</span>, <span style="color: #dcaeea;">testing_targets</span> = train_test_split(
<span class="linenr">23: </span>           samples, targets, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>, random_state=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">24: </span>
</pre>
</div>

<p>
接下來就可以套用各種分類演算法<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org5de06b7"></a>使用 k-Nearest Neighbors model<br />
<div class="outline-text-5" id="text-24-3-4-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">knc</span> = KNeighborsClassifier(n_neighbors=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">2: </span>  knc.fit(training_samples,training_targets)
<span class="linenr">3: </span>  <span style="color: #dcaeea;">knc_prediction</span> = knc.predict(testing_samples)
<span class="linenr">4: </span>  <span style="color: #dcaeea;">knc_accuracy</span> = <span style="color: #da8548; font-weight: bold;">100.0</span> * accuracy_score(testing_targets, knc_prediction)
<span class="linenr">5: </span>  <span style="color: #51afef;">print</span> (<span style="color: #98be65;">"K-Nearest Neighbours accuracy: "</span> + <span style="color: #c678dd;">str</span>(knc_accuracy))
<span class="linenr">6: </span>
<span class="linenr">7: </span>
</pre>
</div>
</div>
</li>

<li><a id="org9987572"></a>使用 Decision tree model<br />
<div class="outline-text-5" id="text-24-3-4-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">dtc</span> = DecisionTreeClassifier(random_state=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">2: </span>  dtc.fit(training_samples,training_targets)
<span class="linenr">3: </span>  <span style="color: #dcaeea;">dtc_prediction</span> = dtc.predict(testing_samples)
<span class="linenr">4: </span>  <span style="color: #dcaeea;">dtc_accuracy</span> = <span style="color: #da8548; font-weight: bold;">100.0</span> * accuracy_score(testing_targets, dtc_prediction)
<span class="linenr">5: </span>  <span style="color: #51afef;">print</span> (<span style="color: #98be65;">"Decision Tree accuracy: "</span> + <span style="color: #c678dd;">str</span>(dtc_accuracy))
</pre>
</div>
</div>
</li>

<li><a id="org56ffd26"></a>使用 Gaussian Naive Bayes model<br />
<div class="outline-text-5" id="text-24-3-4-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">gnb</span> = GaussianNB()
<span class="linenr">2: </span>  gnb.fit(training_samples,training_targets)
<span class="linenr">3: </span>  <span style="color: #dcaeea;">gnb_prediction</span> = gnb.predict(testing_samples)
<span class="linenr">4: </span>  <span style="color: #dcaeea;">gnb_accuracy</span> = <span style="color: #da8548; font-weight: bold;">100.0</span> * accuracy_score(testing_targets, gnb_prediction)
<span class="linenr">5: </span>  <span style="color: #51afef;">print</span> (<span style="color: #98be65;">"Gaussian Naive Bayes accuracy: "</span> + <span style="color: #c678dd;">str</span>(gnb_accuracy))
<span class="linenr">6: </span>
</pre>
</div>
</div>
</li>
</ol>
</li>

<li><a id="org96c5d46"></a>結果<br />
<div class="outline-text-4" id="text-24-3-5">
<ul class="org-ul">
<li>K-Nearest Neighbours accuracy: 95.90163934426229<br /></li>
<li>Decision Tree accuracy: 96.72131147540983<br /></li>
<li>Gaussian Naive Bayes accuracy: 98.36065573770492<br /></li>
</ul>



<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>
</pre>
</div>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-orgb2b2490" class="outline-2">
<h2 id="orgb2b2490"><span class="section-number-2">25</span> Emacs 常用快速鍵</h2>
<div class="outline-text-2" id="text-25">
<ul class="org-ul">
<li><a href="https://orgmode.org/manual/Footnotes.html">Footnotes</a>: C-c C-x f<br /></li>
<li>sortFootnote: C-c C-x f S<br /></li>
<li><a href="https://stackoverflow.com/questions/17621495/emacs-org-display-inline-images">Emacs org-display-inline-images</a>: C-c C-x C-v<br /></li>
<li><a href="https://stackoverflow.com/questions/31614316/how-can-i-reload-spacemacs-file-after-editing-without-restart-emacs">reload .spacemaces</a>: esc m f e R<br /></li>
<li><a href="https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols">List of Latex mathematical symbols</a><br /></li>
<li><a href="https://emacs.stackexchange.com/questions/30837/org-tables-wrap-all-fields-in-column-to-a-given-size/47236">wrap in table column</a>: M-x org-table-wrap-to-width<br /></li>
<li><a href="https://emacs.stackexchange.com/questions/14681/org-mode-table-row-number-grow-automatically?rq=1">org mode table row number grow automatically</a>: Shift-Enter<br /></li>
<li>scroll other windows: Fn-option-Down<br /></li>
<li>delete n rows: c-a c-u n c-k<br /></li>
</ul>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
<a href="https://kknews.cc/code/o8m4gpq.html">五分鐘理解深度學習中激活函數以及不同激活函數的使用場景</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><p class="footpara">
<a href="https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53">What the Hell is Perceptron?</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup> <div class="footpara"><p class="footpara">
<a href="https://kknews.cc/code/o8qaazo.html">機器學習：Python測試線性可分性的方法</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4">4</a></sup> <div class="footpara"><p class="footpara">
<a href="https://s-top.github.io/blog/2018-04-19-svm">Support Vector Machine（全面推导）</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5">5</a></sup> <div class="footpara"><p class="footpara">
<a href="https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF-%E5%A4%9A%E5%B1%A4%E6%84%9F%E7%9F%A5%E6%A9%9F-multilayer-perceptron-mlp-%E9%81%8B%E4%BD%9C%E6%96%B9%E5%BC%8F-f0e108e8b9af">機器學習- 神經網路(多層感知機 Multilayer perceptron, MLP)運作方式</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.6" class="footnum" href="#fnr.6">6</a></sup> <div class="footpara"><p class="footpara">
<a href="https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF-%E5%A4%9A%E5%B1%A4%E6%84%9F%E7%9F%A5%E6%A9%9F-multilayer-perceptron-mlp-%E5%90%AB%E8%A9%B3%E7%B4%B0%E6%8E%A8%E5%B0%8E-ee4f3d5d1b41">機器學習- 神經網路(多層感知機 Multilayer perceptron, MLP) 含倒傳遞( Backward propagation)詳細推導</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.7" class="footnum" href="#fnr.7">7</a></sup> <div class="footpara"><p class="footpara">
<a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-09-l1l2regularization/">L1 / L2 正規化</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.8" class="footnum" href="#fnr.8">8</a></sup> <div class="footpara"><p class="footpara">
<a href="https://ithelp.ithome.com.tw/articles/10219648?sc=rss.iron">Google ML課程筆記 - Overfitting 與 L1 /L2 Regularization </a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.9" class="footnum" href="#fnr.9">9</a></sup> <div class="footpara"><p class="footpara">
<a href="https://www.itread01.com/content/1549579879.html">機器學習十大演算法&#x2014;8. 隨機森林演算法</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.10" class="footnum" href="#fnr.10">10</a></sup> <div class="footpara"><p class="footpara">
<a href="https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8-%E7%B5%B1%E8%A8%88%E5%AD%B8%E7%BF%92-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-principle-component-analysis-pca-58229cd26e71">機器/統計學習:主成分分析(Principal Component Analysis, PCA)</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.11" class="footnum" href="#fnr.11">11</a></sup> <div class="footpara"><p class="footpara">
<a href="https://blog.csdn.net/dongyanwen6036/article/details/78311071">LDA與PCA都是常用的降維方法，二者的區別</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.12" class="footnum" href="#fnr.12">12</a></sup> <div class="footpara"><p class="footpara">
<a href="https://medium.com/yiyi-network/transfer-learning-1f87d4f1886f">Kaggle Learn | Deep Learning 深度學習 | 學習資源介紹 (Part 2)</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.13" class="footnum" href="#fnr.13">13</a></sup> <div class="footpara"><p class="footpara">
<a href="https://keras.io/zh/getting-started/sequential-model-guide/">Sequential 順序模型指引</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.14" class="footnum" href="#fnr.14">14</a></sup> <div class="footpara"><p class="footpara">
<a href="https://keras-cn.readthedocs.io/en/latest/models/model/">函數式模型接口</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.15" class="footnum" href="#fnr.15">15</a></sup> <div class="footpara"><p class="footpara">
<a href="https://ithelp.ithome.com.tw/articles/10191820">Day 06：處理影像的利器 &#x2013; 卷積神經網路(Convolutional Neural Network) </a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.16" class="footnum" href="#fnr.16">16</a></sup> <div class="footpara"><p class="footpara">
<a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/">An Intuitive Explanation of Convolutional Neural Networks</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.17" class="footnum" href="#fnr.17">17</a></sup> <div class="footpara"><p class="footpara">
<a href="https://medium.com/@syshen/%E5%85%A5%E9%96%80%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-2-d694cad7d1e5">入門深度學習 — 2</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.18" class="footnum" href="#fnr.18">18</a></sup> <div class="footpara"><p class="footpara">
<a href="https://www.books.com.tw/products/0010822932">Deep learning 深度學習必讀：Keras 大神帶你用 Python 實作</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.19" class="footnum" href="#fnr.19">19</a></sup> <div class="footpara"><p class="footpara">
Goodfello, Ian J., Oriol Vinyals &amp; Andrew M. Saxe, Qualitatively characterizing neural ntwork optimization problems, arXiv preprint arXiv: 1412.6544 (2014).<br />
</p></div></div>

<div class="footdef"><sup><a id="fn.20" class="footnum" href="#fnr.20">20</a></sup> <div class="footpara"><p class="footpara">
<a href="https://medium.com/pyladies-taiwan/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E6%96%B0%E6%89%8B%E6%9D%91-pytorch%E5%85%A5%E9%96%80-511df3c1c025">深度學習新手村：PyTorch入門</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.21" class="footnum" href="#fnr.21">21</a></sup> <div class="footpara"><p class="footpara">
<a href="https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305d9cd231015d9d0992ef0030">https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305d9cd231015d9d0992ef0030</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.22" class="footnum" href="#fnr.22">22</a></sup> <div class="footpara"><p class="footpara">
<a href="https://www.twblogs.net/a/5bd3c1902b717778ac20ccb6">PyTorch 常用方法總結</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.23" class="footnum" href="#fnr.23">23</a></sup> <div class="footpara"><p class="footpara">
<a href="https://zhuanlan.zhihu.com/p/29904755">Autograd:PyTorch中的梯度計算</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.24" class="footnum" href="#fnr.24">24</a></sup> <div class="footpara"><p class="footpara">
<a href="https://www.itread01.com/content/1551725786.html">機器學習入門之sklearn介紹</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.25" class="footnum" href="#fnr.25">25</a></sup> <div class="footpara"><p class="footpara">
<a href="https://zh.wikipedia.org/wiki/NumPy">NumPy</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.26" class="footnum" href="#fnr.26">26</a></sup> <div class="footpara"><p class="footpara">
<a href="https://ithelp.ithome.com.tw/articles/10196167">Day18-Numpy檔案輸入與輸出!</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.27" class="footnum" href="#fnr.27">27</a></sup> <div class="footpara"><p class="footpara">
<a href="https://oranwind.org/python-pandas-ji-chu-jiao-xue/">Pandas 基礎教學</a><br />
</p>

<div class="org-src-container">
<pre class="src src-emacs-lisp"><span style="color: #51afef;">(</span><span style="color: #c678dd;">org-element-map</span> <span style="color: #c678dd;">(</span><span style="color: #c678dd;">org-element-parse-buffer</span><span style="color: #c678dd;">)</span> <span style="color: #51afef;">'</span><span style="color: #ECBE7B;">link</span>
  <span style="color: #c678dd;">(</span><span style="color: #51afef;">lambda</span> <span style="color: #98be65;">(</span>link<span style="color: #98be65;">)</span>
    <span style="color: #98be65;">(</span><span style="color: #51afef;">when</span> <span style="color: #51afef;">(</span><span style="color: #a9a1e1;">string=</span> <span style="color: #c678dd;">(</span><span style="color: #c678dd;">org-element-property</span> <span style="color: #c678dd;">:type</span> link<span style="color: #c678dd;">)</span> <span style="color: #98be65;">"file"</span><span style="color: #51afef;">)</span>
      <span style="color: #51afef;">(</span><span style="color: #c678dd;">org-element-property</span> <span style="color: #c678dd;">:path</span> link<span style="color: #51afef;">)</span><span style="color: #98be65;">)</span><span style="color: #c678dd;">)</span><span style="color: #51afef;">)</span>

</pre>
</div></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Yung Chin, Yen</p>
<p class="date">Created: 2021-06-01 Tue 22:17</p>
</div>
</body>
</html>
